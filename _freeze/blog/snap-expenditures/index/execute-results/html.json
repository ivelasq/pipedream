{
  "hash": "4cd63099e9ae52a87b964b497ac98ba1",
  "result": {
    "markdown": "---\ntitle: \"Transforming PDFs into useful tables\"\ndate: \"2018-12-16\"\ncategories: [\"import\", \"tidy\", \"visualize\"]\ndescription: \"Most of the time, data doesn't come in tidy spreadsheets. With R, though, you can pull data from PDFs to use in analyses.\"\nimage: thumbnail.jpg\n---\n\n![Joseph Decker, Green Plums (1885)](thumbnail-wide.jpg){fig-alt=\"Several green plumbs on a brown table\"}\n\n\n\nWay back in 2016, the USDA released a [study](https://www.fns.usda.gov/snap/foods-typically-purchased-supplemental-nutrition-assistance-program-snap-households) entitled \"Foods Typically Purchased by Supplemental Nutrition Assistance Program (SNAP) Households\" that included a summary, final report, and appendices. Per the USDA's description:\n\n*\"This study uses calendar year 2011 point-of-sale transaction data from a leading grocery retailer to examine the food choices of SNAP and non-SNAP households.\"*\n\nAt the time though, I was most interested in looking at the [appendices data](https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf) - _263_ pages full of tables detailing the commodities and categories of food bought by both families served and not served by SNAP. Unfortunately, these wonderful data are in PDF format, with 'fancy' Excel formatting (merged cells, unnecessary column names), where the formatting varies depending on which appendix you are looking at.\n\nI [emailed](mailto:SNAPHQ-WEB@fns.usda.gov) SNAP HQ to ask if they had the raw data available in CSVs and was told simply:\n\n*\"Thank you for your inquiry. Unfortunately we do not have the data tables in a CSV file.\"*\n\nAt the time, my R skills were pretty rudimentary and I couldn't figure out how to easily and efficiently pull the data into usable tables. Two years later and with a little more experience with R and scraping and cleaning ugly files, I decided to try again using the {tidyverse} and {tabulizer} packages.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tabulizer)\n```\n:::\n\nWe use `tabulizer::extract_tables()` to extract the data from the [appendices PDF](https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf). Once we extract the tables, we are left with a list (which is slightly more manageable than the original PDFs).\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_3cf540c1e755ec4dea2e1ab8f0340109'}\n\n```{.r .cell-code}\nsnap_pdf <-\n  extract_tables(\n    \"https://fns-prod.azureedge.net/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf\"\n  )\n```\n:::\n\nUsing the {purrr} package, we create a data frame from the lists while simultaneously removing the unnecessary rows.\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_df <-\n  snap_pdf %>%\n  map(as_tibble) %>%\n  map_df( ~ slice(.,-2)) # slicing because of the unnecessary rows\n\nhead(snap_df)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 9\n  V1                    V2    V3             V4    V5    V6    V7    V8    V9   \n  <chr>                 <chr> <chr>          <chr> <chr> <chr> <chr> <chr> <chr>\n1 \"\"                    \"\"    SNAP Householâ€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n2 \"Soft drinks\"         \"\"    1 $357.7 5.44â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n3 \"Fluid milk products\" \"\"    2 $253.7 3.85â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n4 \"Beef:grinds\"         \"\"    3 $201.0 3.05â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n5 \"Bag snacks\"          \"\"    4 $199.3 3.03â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n6 \"Cheese\"              \"\"    5 $186.4 2.83â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n```\n:::\n:::\n\nDue to the original formatting of the PDFs, we need to do a lot of cleaning to make the list into a usable table. Using `slice()`, we isolate only the rows from Appendix 1.\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_appendix1 <-\n  snap_df %>%\n  slice(1:244)\n```\n:::\n\nNow comes the fun part (yay, data cleaning!). When we look at the data frame, the data for each commodity are in two separate columns (`V2` and `V3`), but only one column or the other. There are several empty columns (`V4` through `V9`), probably created due to the funky original formatting.\n\nFirst things first: let's put all the data in a single column called `col_dat`. Then, we will remove all the empty rows in the newly created `col_dat` column.\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_appendix1_pt1 <-\n  snap_appendix1 %>%\n  mutate(col_dat = case_when(grepl(\"[0-9]\", V2) ~ V2, # create a column that contains all the data\n                             grepl(\"[0-9]\", V3) ~ V3,\n                             TRUE ~ \"\")) %>%\n  filter(col_dat != \"\") # some rows are empty\n\nhead(snap_appendix1_pt1)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 10\n  V1                  V2    V3       V4    V5    V6    V7    V8    V9    col_dat\n  <chr>               <chr> <chr>    <chr> <chr> <chr> <chr> <chr> <chr> <chr>  \n1 Soft drinks         \"\"    1 $357.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  1 $357â€¦\n2 Fluid milk products \"\"    2 $253.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2 $253â€¦\n3 Beef:grinds         \"\"    3 $201.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  3 $201â€¦\n4 Bag snacks          \"\"    4 $199.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  4 $199â€¦\n5 Cheese              \"\"    5 $186.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  5 $186â€¦\n6 Baked breads        \"\"    6 $163.â€¦ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  6 $163â€¦\n```\n:::\n:::\n\nNow the numeric data we want is in a single column, we can select the columns `V1` and `col_dat`.\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_appendix1_pt2 <-\n  snap_appendix1_pt1 %>%\n  select(V1, col_dat)\n\nhead(snap_appendix1_pt2)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 2\n  V1                  col_dat                        \n  <chr>               <chr>                          \n1 Soft drinks         1 $357.7 5.44% 2 $1,263.3 4.01%\n2 Fluid milk products 2 $253.7 3.85% 1 $1,270.3 4.03%\n3 Beef:grinds         3 $201.0 3.05% 6 $621.1 1.97%  \n4 Bag snacks          4 $199.3 3.03% 5 $793.9 2.52%  \n5 Cheese              5 $186.4 2.83% 3 $948.9 3.01%  \n6 Baked breads        6 $163.7 2.49% 4 $874.8 2.78%  \n```\n:::\n:::\n\nThe numeric data is still mushed in column `col_dat`. We can use `tidyr::separate()` to split the values that are separated by spaces into their own columns. We reference the original PDF to descriptively rename the columns (and the commodity column `V1` as well).\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_appendix1_pt3 <-\n  snap_appendix1_pt2 %>%\n  separate(\n    col_dat,\n    \" \",\n    into = c(\n      \"snap_rank\",\n      \"snap_dollars_in_millions\",\n      \"snap_pct_total_expenditures\",\n      \"nonsnap_rank\",\n      \"nonsnap_dollars_in_millions\",\n      \"nonsnap_pct_total_expenditures\"\n    )\n  ) %>%\n  rename(commodity = V1)\n```\n:::\n\nThe numeric values have retained their original formatting, with dollar signs and commas and percentage signs, oh my! We can remove those unnecessary characters and transform those columns into truly numeric values.\n\n::: {.cell}\n\n```{.r .cell-code}\nsnap_appendix1_pt4 <-\n  snap_appendix1_pt3 %>%\n  mutate(across(\n    snap_rank:nonsnap_pct_total_expenditures,\n    ~ as.numeric(str_replace_all(.x, \",|%|\\\\$\", \"\"))\n  ))\n\nhead(snap_appendix1_pt4)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 7\n  commodity           snap_rank snap_dollars_in_mâ€¦ snap_pct_total_â€¦ nonsnap_rank\n  <chr>                   <dbl>              <dbl>            <dbl>        <dbl>\n1 Soft drinks                 1               358.             5.44            2\n2 Fluid milk products         2               254.             3.85            1\n3 Beef:grinds                 3               201              3.05            6\n4 Bag snacks                  4               199.             3.03            5\n5 Cheese                      5               186.             2.83            3\n6 Baked breads                6               164.             2.49            4\n# â€¦ with 2 more variables: nonsnap_dollars_in_millions <dbl>,\n#   nonsnap_pct_total_expenditures <dbl>\n```\n:::\n:::\n\nLast but not least, we convert all the columns with percentages into actual percentages by dividing by 100.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsnap_appendix1_clean <-\n  snap_appendix1_pt4 %>%\n  mutate(across(contains(\"pct\"), ~ . / 100))\n```\n:::\n\nTada! Now we have a clean dataset from the original not-very-usable PDFs.\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(snap_appendix1_clean)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 7\n  commodity           snap_rank snap_dollars_in_mâ€¦ snap_pct_total_â€¦ nonsnap_rank\n  <chr>                   <dbl>              <dbl>            <dbl>        <dbl>\n1 Soft drinks                 1               358.           0.0544            2\n2 Fluid milk products         2               254.           0.0385            1\n3 Beef:grinds                 3               201            0.0305            6\n4 Bag snacks                  4               199.           0.0303            5\n5 Cheese                      5               186.           0.0283            3\n6 Baked breads                6               164.           0.0249            4\n# â€¦ with 2 more variables: nonsnap_dollars_in_millions <dbl>,\n#   nonsnap_pct_total_expenditures <dbl>\n```\n:::\n:::\n\nAt some point, I would like to do a full analysis of what these data show. My hope is that now some of it is available, others can create and share amazing analyses using these data. For the purposes of this post, here is a quick ggplot that compare the rank of commodities between families served by SNAP and those who are not. Based on a Wilcox Signed-Rank Test, the two groups do not statistically differ in their food rankings.\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(snap_appendix1_clean$snap_rank,\n            snap_appendix1_clean$nonsnap_rank,\n            paired = TRUE)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  snap_appendix1_clean$snap_rank and snap_appendix1_clean$nonsnap_rank\nV = 13954, p-value = 0.7538\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n:::\n\nHere is a ggplot that compare the rank of commodities between families served by SNAP and those who are not. The labels display which commodities differ by rank of 30 or more.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggrepel)\n\nsnap_appendix1_plot <-\n  snap_appendix1_clean %>%\n  select(commodity, snap_rank, nonsnap_rank) %>%\n  mutate(rank_diff = snap_rank - nonsnap_rank) %>%\n  pivot_longer(c(\"snap_rank\", \"nonsnap_rank\"),\n               names_to = \"variable\",\n               values_to = \"value\")\n\nggplot(snap_appendix1_plot, aes(x = variable, y = value)) +\n  geom_point(size = 1, col = \"lightgrey\", alpha = 0.2) +\n  geom_line(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff >= 30 | rank_diff <= -30),\n    aes(group = commodity),\n    color = \"#00C3DA\",\n    alpha = 0.6\n  ) +\n  geom_text_repel(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff >= 30,\n             variable == \"nonsnap_rank\"),\n    aes(label = commodity),\n    nudge_y = 0.25,\n    size = 2,\n    direction = \"y\"\n  ) +\n  geom_text_repel(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff <= -30,\n             variable == \"snap_rank\"),\n    aes(label = commodity),\n    nudge_y = 0.25,\n    size = 2,\n    direction = \"y\"\n  ) +\n  ggtitle(\"Rank Comparison\") +\n  xlab(\"Category\") +\n  ylab(\"Rank\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = c(\"Non-SNAP Expenditure Rank\", \"SNAP Expenditure Rank\")) +\n  scale_y_reverse()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' fig-alt='Line plot showing difference in rank by families served and not served by SNAP' width=672}\n:::\n:::\n\n\n\nI'd love to collaborate with others to finish up this project and find more efficient ways of cleaning these data. The repo with the code and final dataset are on [Github](https://github.com/ivelasq/snap).\n\n<center>\n*Liked this article? I'd love for you to retweet!*\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">New blog post ðŸŽ‰ Extracting SNAP Expenditures Data by Transforming PDFs into Useful Tables ðŸ¥’ðŸŒ¶ðŸ¥• <a href=\"https://t.co/UP0hQaHJc9\">https://t.co/UP0hQaHJc9</a> <a href=\"https://twitter.com/hashtag/tabulizer?src=hash&amp;ref_src=twsrc%5Etfw\">#tabulizer</a> <a href=\"https://twitter.com/hashtag/dplyr?src=hash&amp;ref_src=twsrc%5Etfw\">#dplyr</a> <a href=\"https://t.co/SYWbqMiIZh\">pic.twitter.com/SYWbqMiIZh</a></p>&mdash; Isabella VelÃ¡squez (@ivelasq3) <a href=\"https://twitter.com/ivelasq3/status/1074704678858973184?ref_src=twsrc%5Etfw\">December 17, 2018</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n</center>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}