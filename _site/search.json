[
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n¬†\n\n\n\nAdd .gitkeep to a bunch of folders\n\n\nAug 12, 2022\n\n\n\n\n\n\n\nCount the number of characters (or bytes or width)\n\n\nMay 10, 2022\n\n\n\n\n\n\n\nCreate a GIF of code and its output\n\n\nMay 10, 2022\n\n\n\n\n\n\n\nFilter on conditions for more than one variable at the time\n\n\nMar 21, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk.html",
    "href": "talk.html",
    "title": "Talks",
    "section": "",
    "text": "Salt Lake City R Users Group\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Rome\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies St.¬†Louis\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Enterprise Community\n\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Enterprise Community\n\n\n\nJan 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy R? 2021 Conference\n\n\n\nDec 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Johannesburg\n\n\n\nOct 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Seattle\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR4DS\n\n\n\nMar 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Seattle\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeattle useR Group\n\n\n\nMay 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Seattle\n\n\n\nJul 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Seattle\n\n\n\nJul 23, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Setting up macOS as an R data science rig in 2023\n          \n        \n      \n      \n        Let‚Äôs fall together into a pit of success (when configuring macOS)!\n      \n      \n        2022-12-22\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Three packages that port the tidyverse to Python\n          \n        \n      \n      \n        Working in Python but miss tidyverse syntax? These packages can help.\n      \n      \n        2022-05-09\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Understanding the native R pipe |&gt;\n          \n        \n      \n      \n        Or, why `mtcars |&gt; plot(hp, mpg)` doesn't work and what you can do about it.\n      \n      \n        2022-01-18\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Creating a dataset from an image using reticulate in R Markdown\n          \n        \n      \n      \n        A cool paper used R and Python together --- and so can you!\n      \n      \n        2021-09-28\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Why I‚Äôm excited to join RStudio, told through a blogdown metadata project\n          \n        \n      \n      \n        I am now a Sr. Product Marketing Manager at RStudio!\n      \n      \n        2021-09-13\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Politely scraping Wikipedia tables\n          \n        \n      \n      \n        Wikipedia has a lot of wonderful data stored in tables. Here's how to pull them into R.\n      \n      \n        2021-07-27\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Getting started in #rtistry\n          \n        \n      \n      \n        Generative art mixes randomness and order to create beautiful images. The \\#rtistry hashtag helps find work from other Rtists.\n      \n      \n        2021-05-09\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Exploring other ggplot2 geoms\n          \n        \n      \n      \n        Let's explore some cool charts we can make in {ggplot2}.\n      \n      \n        2021-03-28\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Preparing a manuscript for publication using bookdown\n          \n        \n      \n      \n        Lessons learned when we used {bookdown} to prepare a manuscript to submit for publishing.\n      \n      \n        2021-03-08\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Remote pair programming in R using Visual Studio Code and Live Share\n          \n        \n      \n      \n        Setting up a Google Docs-like coding environment in VS Code.\n      \n      \n        2021-02-03\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Browse and search liked Tweets using R\n          \n        \n      \n      \n        Tired of trying to search all your liked Tweets on your timeline? Pull them into R instead!\n      \n      \n        2020-11-10\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          An even easier-to-use R package for school district shapefiles\n          \n        \n      \n      \n        Thanks to contributions from Daniel Anderson, {leaidr} is even easier to use.\n      \n      \n        2020-08-30\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Party with R: How the community enabled us to write a book\n          \n        \n      \n      \n        Celebrating the R community.\n      \n      \n        2020-07-13\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          What it takes to tidy Census data\n          \n        \n      \n      \n        Census data is valuable, but can be stored in messy Excel spreadsheets.\n      \n      \n        2020-05-28\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Introducing an R package for school district shapefiles\n          \n        \n      \n      \n        The {leaidr} package helps us easily create maps of U.S. school districts.\n      \n      \n        2020-05-03\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          You CRAN do it\n          \n        \n      \n      \n        Recommendations for when you want to submit a package to CRAN.\n      \n      \n        2020-02-25\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Six things I always Google when using ggplot2\n          \n        \n      \n      \n        My frequently-used reference for styling {ggplot2} charts.\n      \n      \n        2020-01-27\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Finding the modal school district\n          \n        \n      \n      \n        What's the most common type of district in the U.S.? Let's find out using R.\n      \n      \n        2019-09-11\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Taking a peek into my hiking data\n          \n        \n      \n      \n        I got my hands on a bunch of GPX files.\n      \n      \n        2019-04-06\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Transforming PDFs into useful tables\n          \n        \n      \n      \n        Most of the time, data doesn't come in tidy spreadsheets. With R, though, you can pull data from PDFs to use in analyses.\n      \n      \n        2018-12-16\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          Disaggregating school district data\n          \n        \n      \n      \n        Exploring why disaggregation of data is important by looking at district demographics.\n      \n      \n        2018-09-21\n      \n    \n  \n\n\n  \n    \n      \n        \n          \n        \n      \n    \n      \n      \n        \n          So you're getting a Masters in Analytics\n          \n        \n      \n      \n        My advice to aspiring data scientists and analysts entering these programs.\n      \n      \n        2018-08-04\n      \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/other-geoms/index.html",
    "href": "blog/other-geoms/index.html",
    "title": "Exploring other ggplot2 geoms",
    "section": "",
    "text": "Vincent Van Gogh, Flower Beds in Holland (1883)\nR users are incredibly fortunate to work in an open-source community that creates and shares resources that make our work even better. The {ggplot2} package comes with incredibly useful geoms (geometric objects) to create visualizations. A full list of these can be found in the reference documents for {ggplot2}. These include:\nIn addition, other amazing folks in the R Community have created geoms that can be used with {ggplot2} and similarly use the tidyverse framework. This is fantastic for many reasons, but some include being able to add themes, facets, titles, and other components just like with any ggplot. Here are a few geoms that I‚Äôve tried out with examples!"
  },
  {
    "objectID": "blog/other-geoms/index.html#setup",
    "href": "blog/other-geoms/index.html#setup",
    "title": "Exploring other ggplot2 geoms",
    "section": "Setup",
    "text": "Setup\nTo be able to run this code, be sure to have the tidyverse installed. The {wesanderson} package contains beautiful palettes for visualizations.\n\n# Load required packages\n\nlibrary(tidyverse)\nlibrary(wesanderson)"
  },
  {
    "objectID": "blog/other-geoms/index.html#streamgraphs",
    "href": "blog/other-geoms/index.html#streamgraphs",
    "title": "Exploring other ggplot2 geoms",
    "section": "Streamgraphs",
    "text": "Streamgraphs\nThis post includes three of David Sj√∂berg‚Äôs amazing geoms; he created {ggsankey}, {ggstream}, AND {ggbump}. If you haven‚Äôt seen his GitHub, please check it out now.\nThis first geom, geom_stream(), creates a streamplot (which I‚Äôve also seen called stream graphs). The streamplot is an area graph that usually centers around a central axis and allows us to see large fluctuations over time. More information on streamplot can be found here.\n{ggstream} also has other options available to customize the streamgraphs, such as creating an area chart. Check out the repo here.\n\n# remotes::install_github(\"davidsjoberg/ggstream\")\nlibrary(ggstream)\n\nggplot(blockbusters, aes(year, box_office, fill = genre)) +\n  geom_stream() +\n  scale_fill_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#ridgeline-plots",
    "href": "blog/other-geoms/index.html#ridgeline-plots",
    "title": "Exploring other ggplot2 geoms",
    "section": "Ridgeline plots",
    "text": "Ridgeline plots\nThe {ggridges} package by Claus O. Wilke package also has a variety of geoms; check out the repo here. Ridgeline plots show the distribution of a numeric value for different groups and can look like mountain ranges. The R-Ladies Seattle hex sticker was created using ridgelines (very appropriate for the mountainous Washington!).\n\n# install.packages(\"ggridges\")\nlibrary(ggridges)\n \nggplot(blockbusters, aes(x = box_office, y = genre, fill = genre)) +\n  geom_density_ridges(scale = 4) +\n  scale_fill_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#sankey-diagrams",
    "href": "blog/other-geoms/index.html#sankey-diagrams",
    "title": "Exploring other ggplot2 geoms",
    "section": "Sankey diagrams",
    "text": "Sankey diagrams\nAnother geom by David Sj√∂berg is geom_sankey(), repo here. This geom creates Sankey diagrams and alluvial plots, which show flow and transfers in a system or throughout time. These plots are VERY popular on the subreddit dataisbeautiful (check it out on Mondays to see some examples).\n\n# devtools::install_github(\"davidsjoberg/ggsankey\")\nlibrary(ggsankey)\n\nexample_dat <-\n  mtcars %>%\n  make_long(cyl, vs, am, gear, carb) # function in ggsankey to format data correctly\n\nggplot(example_dat,\n       aes(x = x, \n               next_x = next_x, \n               node = node, \n               next_node = next_node,\n               fill = factor(node))) +\n   geom_sankey(flow.alpha = .6) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnother package for alluvial charts is {ggalluvial} by Jason Cory Brunson, with its repo here. The data can be in more familiar formats than what is required for {ggsankey}.\n\n# install.packages(\"ggalluvial\")\nlibrary(ggalluvial)\n\nggplot(as.data.frame(UCBAdmissions),\n       aes(y = Freq, axis1 = Gender, axis2 = Dept)) +\n  geom_alluvium(aes(fill = Admit), width = 1/12) +\n  scale_fill_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#bump-charts",
    "href": "blog/other-geoms/index.html#bump-charts",
    "title": "Exploring other ggplot2 geoms",
    "section": "Bump charts",
    "text": "Bump charts\nOne last one by David Sj√∂berg is the amazing {ggbump}, repo here. Bump plots help show change in rank over time.\n\n# devtools::install_github(\"davidsjoberg/ggbump\")\nlibrary(ggbump)\n\nblockbusters2 <-\n  blockbusters %>% \n  filter(genre %in% c(\"Action\", \"Comedy\", \"Drama\")) %>% \n  group_by(year) %>% \n  mutate(rank = rank(box_office))\n\nggplot(blockbusters2, aes(year, rank, color = genre)) +\n  geom_point(size = 7) +\n  geom_bump() +\n  scale_color_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#waffle-charts",
    "href": "blog/other-geoms/index.html#waffle-charts",
    "title": "Exploring other ggplot2 geoms",
    "section": "Waffle charts",
    "text": "Waffle charts\nFor waffle charts, which are handy visualizations that show completion or parts of a whole, there is hrbrmstr‚Äôs {waffle}. The repo is here. Check out the ability to bring in other {ggplot2} functions, like facet_wrap. {waffle} also allows you to create pictograms using geom_pictogram, which replaces the squares in the ‚Äòwaffle‚Äô with pictures.\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\nggplot(as_tibble(Titanic), aes(fill = Sex, values = n)) +\n  geom_waffle(n_rows = 20, color = \"white\") +\n  facet_wrap(~ Survived, ncol = 1)  +\n  scale_fill_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#beeswarm-charts",
    "href": "blog/other-geoms/index.html#beeswarm-charts",
    "title": "Exploring other ggplot2 geoms",
    "section": "Beeswarm charts",
    "text": "Beeswarm charts\nBeeswarm charts, similar to jitter plots in {ggplot2}, plot individual points showing distributions without allowing the points to overlap too much. Erik Clarke‚Äôs repo for {ggbeeswarm} is here.\n\n# install.packages(\"ggbeeswarm\")\nlibrary(ggbeeswarm)\n\nggplot(blockbusters, aes(x = genre, y = box_office, color = genre)) + \n  geom_quasirandom() +\n  theme_minimal() +\n  scale_color_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#mosaic-charts",
    "href": "blog/other-geoms/index.html#mosaic-charts",
    "title": "Exploring other ggplot2 geoms",
    "section": "Mosaic charts",
    "text": "Mosaic charts\nMosaic charts are incredibly helpful when displaying proportions of (multiple) categories. The {ggmosaic} package by Haley Jeppson (repo here) uses geom_mosaic to create these visualizations.\n\n# devtools::install_github(\"haleyjeppson/ggmosaic\")\nlibrary(ggmosaic)\n\nggplot(as.data.frame(UCBAdmissions)) +\n  geom_mosaic(aes(x = product(Admit, Dept), fill = Gender, weight = Freq)) +\n  scale_fill_manual(values = wes_palette(\"Darjeeling2\")) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/other-geoms/index.html#other-geoms",
    "href": "blog/other-geoms/index.html#other-geoms",
    "title": "Exploring other ggplot2 geoms",
    "section": "Other geoms",
    "text": "Other geoms\nI know there exist a ton of other geoms that work with {ggplot2} out there. Just as I was writing this blogpost, I discovered {gghilbertstrings}! What other gg packages or geoms do you know of? Let me know on Twitter and I‚Äôll list them here!\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nNew post üöÄ Into the ggplot2niverse! üöÄ We love geom_point, geom_bar, and the built-in geoms in #ggplot2 but have you used #rstats pkgs with other geoms that use the #tidyverse like geom_sankey and geom_waffle? üêùüßá See them here & let me know others üëâ https://t.co/orHyDMeVdd pic.twitter.com/DxKUxIJI08\n\n‚Äî Isabella Vel√°squez (@ivelasq3) March 29, 2021"
  },
  {
    "objectID": "blog/snap-expenditures/index.html",
    "href": "blog/snap-expenditures/index.html",
    "title": "Transforming PDFs into useful tables",
    "section": "",
    "text": "Joseph Decker, Green Plums (1885)\n\n\nWay back in 2016, the USDA released a study entitled ‚ÄúFoods Typically Purchased by Supplemental Nutrition Assistance Program (SNAP) Households‚Äù that included a summary, final report, and appendices. Per the USDA‚Äôs description:\n‚ÄúThis study uses calendar year 2011 point-of-sale transaction data from a leading grocery retailer to examine the food choices of SNAP and non-SNAP households.‚Äù\nAt the time though, I was most interested in looking at the appendices data - 263 pages full of tables detailing the commodities and categories of food bought by both families served and not served by SNAP. Unfortunately, these wonderful data are in PDF format, with ‚Äòfancy‚Äô Excel formatting (merged cells, unnecessary column names), where the formatting varies depending on which appendix you are looking at.\nI emailed SNAP HQ to ask if they had the raw data available in CSVs and was told simply:\n‚ÄúThank you for your inquiry. Unfortunately we do not have the data tables in a CSV file.‚Äù\nAt the time, my R skills were pretty rudimentary and I couldn‚Äôt figure out how to easily and efficiently pull the data into usable tables. Two years later and with a little more experience with R and scraping and cleaning ugly files, I decided to try again using the {tidyverse} and {tabulizer} packages.\n\nlibrary(tidyverse)\nlibrary(tabulizer)\n\nWe use tabulizer::extract_tables() to extract the data from the appendices PDF. Once we extract the tables, we are left with a list (which is slightly more manageable than the original PDFs).\n\nsnap_pdf <-\n  extract_tables(\n    \"https://fns-prod.azureedge.us/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf\"\n  )\n\nUsing the {purrr} package, we create a data frame from the lists while simultaneously removing the unnecessary rows.\n\nsnap_df <-\n  snap_pdf %>%\n  map(as_tibble) %>%\n  map_df( ~ slice(.,-2)) # slicing because of the unnecessary rows\n\nhead(snap_df)\n\n# A tibble: 6 √ó 9\n  V1                    V2    V3             V4    V5    V6    V7    V8    V9   \n  <chr>                 <chr> <chr>          <chr> <chr> <chr> <chr> <chr> <chr>\n1 \"\"                    \"\"    SNAP Househol‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n2 \"Soft drinks\"         \"\"    1 $357.7 5.44‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n3 \"Fluid milk products\" \"\"    2 $253.7 3.85‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n4 \"Beef:grinds\"         \"\"    3 $201.0 3.05‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n5 \"Bag snacks\"          \"\"    4 $199.3 3.03‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n6 \"Cheese\"              \"\"    5 $186.4 2.83‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n\n\nDue to the original formatting of the PDFs, we need to do a lot of cleaning to make the list into a usable table. Using slice(), we isolate only the rows from Appendix 1.\n\nsnap_appendix1 <-\n  snap_df %>%\n  slice(1:244)\n\nNow comes the fun part (yay, data cleaning!). When we look at the data frame, the data for each commodity are in two separate columns (V2 and V3), but only one column or the other. There are several empty columns (V4 through V9), probably created due to the funky original formatting.\nFirst things first: let‚Äôs put all the data in a single column called col_dat. Then, we will remove all the empty rows in the newly created col_dat column.\n\nsnap_appendix1_pt1 <-\n  snap_appendix1 %>%\n  mutate(col_dat = case_when(grepl(\"[0-9]\", V2) ~ V2, # create a column that contains all the data\n                             grepl(\"[0-9]\", V3) ~ V3,\n                             TRUE ~ \"\")) %>%\n  filter(col_dat != \"\") # some rows are empty\n\nhead(snap_appendix1_pt1)\n\n# A tibble: 6 √ó 10\n  V1                  V2    V3       V4    V5    V6    V7    V8    V9    col_dat\n  <chr>               <chr> <chr>    <chr> <chr> <chr> <chr> <chr> <chr> <chr>  \n1 Soft drinks         \"\"    1 $357.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  1 $357‚Ä¶\n2 Fluid milk products \"\"    2 $253.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2 $253‚Ä¶\n3 Beef:grinds         \"\"    3 $201.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  3 $201‚Ä¶\n4 Bag snacks          \"\"    4 $199.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  4 $199‚Ä¶\n5 Cheese              \"\"    5 $186.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  5 $186‚Ä¶\n6 Baked breads        \"\"    6 $163.‚Ä¶ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  6 $163‚Ä¶\n\n\nNow the numeric data we want is in a single column, we can select the columns V1 and col_dat.\n\nsnap_appendix1_pt2 <-\n  snap_appendix1_pt1 %>%\n  select(V1, col_dat)\n\nhead(snap_appendix1_pt2)\n\n# A tibble: 6 √ó 2\n  V1                  col_dat                        \n  <chr>               <chr>                          \n1 Soft drinks         1 $357.7 5.44% 2 $1,263.3 4.01%\n2 Fluid milk products 2 $253.7 3.85% 1 $1,270.3 4.03%\n3 Beef:grinds         3 $201.0 3.05% 6 $621.1 1.97%  \n4 Bag snacks          4 $199.3 3.03% 5 $793.9 2.52%  \n5 Cheese              5 $186.4 2.83% 3 $948.9 3.01%  \n6 Baked breads        6 $163.7 2.49% 4 $874.8 2.78%  \n\n\nThe numeric data is still mushed in column col_dat. We can use tidyr::separate() to split the values that are separated by spaces into their own columns. We reference the original PDF to descriptively rename the columns (and the commodity column V1 as well).\n\nsnap_appendix1_pt3 <-\n  snap_appendix1_pt2 %>%\n  separate(\n    col_dat,\n    \" \",\n    into = c(\n      \"snap_rank\",\n      \"snap_dollars_in_millions\",\n      \"snap_pct_total_expenditures\",\n      \"nonsnap_rank\",\n      \"nonsnap_dollars_in_millions\",\n      \"nonsnap_pct_total_expenditures\"\n    )\n  ) %>%\n  rename(commodity = V1)\n\nThe numeric values have retained their original formatting, with dollar signs and commas and percentage signs, oh my! We can remove those unnecessary characters and transform those columns into truly numeric values.\n\nsnap_appendix1_pt4 <-\n  snap_appendix1_pt3 %>%\n  mutate(across(\n    snap_rank:nonsnap_pct_total_expenditures,\n    ~ as.numeric(str_replace_all(.x, \",|%|\\\\$\", \"\"))\n  ))\n\nhead(snap_appendix1_pt4)\n\n# A tibble: 6 √ó 7\n  commodity           snap_rank snap_dollars_i‚Ä¶¬π snap_‚Ä¶¬≤ nonsn‚Ä¶¬≥ nonsn‚Ä¶‚Å¥ nonsn‚Ä¶‚Åµ\n  <chr>                   <dbl>            <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Soft drinks                 1             358.    5.44       2   1263.    4.01\n2 Fluid milk products         2             254.    3.85       1   1270.    4.03\n3 Beef:grinds                 3             201     3.05       6    621.    1.97\n4 Bag snacks                  4             199.    3.03       5    794.    2.52\n5 Cheese                      5             186.    2.83       3    949.    3.01\n6 Baked breads                6             164.    2.49       4    875.    2.78\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãsnap_dollars_in_millions,\n#   ¬≤‚Äãsnap_pct_total_expenditures, ¬≥‚Äãnonsnap_rank, ‚Å¥‚Äãnonsnap_dollars_in_millions,\n#   ‚Åµ‚Äãnonsnap_pct_total_expenditures\n\n\nLast but not least, we convert all the columns with percentages into actual percentages by dividing by 100.\n\nsnap_appendix1_clean <-\n  snap_appendix1_pt4 %>%\n  mutate(across(contains(\"pct\"), ~ . / 100))\n\nTada! Now we have a clean dataset from the original not-very-usable PDFs.\n\nhead(snap_appendix1_clean)\n\n# A tibble: 6 √ó 7\n  commodity           snap_rank snap_dollars_i‚Ä¶¬π snap_‚Ä¶¬≤ nonsn‚Ä¶¬≥ nonsn‚Ä¶‚Å¥ nonsn‚Ä¶‚Åµ\n  <chr>                   <dbl>            <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Soft drinks                 1             358.  0.0544       2   1263.  0.0401\n2 Fluid milk products         2             254.  0.0385       1   1270.  0.0403\n3 Beef:grinds                 3             201   0.0305       6    621.  0.0197\n4 Bag snacks                  4             199.  0.0303       5    794.  0.0252\n5 Cheese                      5             186.  0.0283       3    949.  0.0301\n6 Baked breads                6             164.  0.0249       4    875.  0.0278\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãsnap_dollars_in_millions,\n#   ¬≤‚Äãsnap_pct_total_expenditures, ¬≥‚Äãnonsnap_rank, ‚Å¥‚Äãnonsnap_dollars_in_millions,\n#   ‚Åµ‚Äãnonsnap_pct_total_expenditures\n\n\nAt some point, I would like to do a full analysis of what these data show. My hope is that now some of it is available, others can create and share amazing analyses using these data. For the purposes of this post, here is a quick ggplot that compare the rank of commodities between families served by SNAP and those who are not. Based on a Wilcox Signed-Rank Test, the two groups do not statistically differ in their food rankings.\n\nwilcox.test(snap_appendix1_clean$snap_rank,\n            snap_appendix1_clean$nonsnap_rank,\n            paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  snap_appendix1_clean$snap_rank and snap_appendix1_clean$nonsnap_rank\nV = 13954, p-value = 0.7538\nalternative hypothesis: true location shift is not equal to 0\n\n\nHere is a ggplot that compare the rank of commodities between families served by SNAP and those who are not. The labels display which commodities differ by rank of 30 or more.\n\nlibrary(ggrepel)\n\nsnap_appendix1_plot <-\n  snap_appendix1_clean %>%\n  select(commodity, snap_rank, nonsnap_rank) %>%\n  mutate(rank_diff = snap_rank - nonsnap_rank) %>%\n  pivot_longer(c(\"snap_rank\", \"nonsnap_rank\"),\n               names_to = \"variable\",\n               values_to = \"value\")\n\nggplot(snap_appendix1_plot, aes(x = variable, y = value)) +\n  geom_point(size = 1, col = \"lightgrey\", alpha = 0.2) +\n  geom_line(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff >= 30 | rank_diff <= -30),\n    aes(group = commodity),\n    color = \"#00C3DA\",\n    alpha = 0.6\n  ) +\n  geom_text_repel(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff >= 30,\n             variable == \"nonsnap_rank\"),\n    aes(label = commodity),\n    nudge_y = 0.25,\n    size = 2,\n    direction = \"y\"\n  ) +\n  geom_text_repel(\n    data = snap_appendix1_plot %>%\n      filter(rank_diff <= -30,\n             variable == \"snap_rank\"),\n    aes(label = commodity),\n    nudge_y = 0.25,\n    size = 2,\n    direction = \"y\"\n  ) +\n  ggtitle(\"Rank Comparison\") +\n  xlab(\"Category\") +\n  ylab(\"Rank\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = c(\"Non-SNAP Expenditure Rank\", \"SNAP Expenditure Rank\")) +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nI‚Äôd love to collaborate with others to finish up this project and find more efficient ways of cleaning these data. The repo with the code and final dataset are on GitHub.\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew blog post üéâ Extracting SNAP Expenditures Data by Transforming PDFs into Useful Tables ü•íüå∂ü•ï https://t.co/UP0hQaHJc9 #tabulizer #dplyr pic.twitter.com/SYWbqMiIZh\n\n‚Äî Isabella Vel√°squez (@ivelasq3) December 17, 2018"
  },
  {
    "objectID": "blog/vscode-live-share/index.html",
    "href": "blog/vscode-live-share/index.html",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "",
    "text": "Edgar Degas, Dancers Practicing at the Barre (1877)"
  },
  {
    "objectID": "blog/vscode-live-share/index.html#the-problem",
    "href": "blog/vscode-live-share/index.html#the-problem",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "The problem",
    "text": "The problem\nWay back in the Before Time, my older brother and I worked together on an R package called {wizehiver}. To collaborate, we used many tools. We had an email thread (of over 35 emails!) that eventually became two email threads. We tried GitHub Issues, but we were in such close communication that the back-and-forth on issues was ineffectual. We also tried pair programming but since we lived in separate cities, we did it while one of us would share our screen on Skype. We wished there was a better, more hands-on, and immediate way to collaborate in a source code editor in real-time, similar to how seamless it is to work on Google Docs."
  },
  {
    "objectID": "blog/vscode-live-share/index.html#state-of-the-code-editors",
    "href": "blog/vscode-live-share/index.html#state-of-the-code-editors",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "State of the code editors",
    "text": "State of the code editors\nThe RStudio IDE is in our opinion the best IDE for R out there; however, live collaboration using RStudio‚Äôs Project Sharing feature is limited to those with a paid RStudio Server Pro license as of this writing. There are many source code editors out there, and notably Atom and Visual Studio (VS) Code both provide extensions for free, collaborative real-time editing.\nWe have been following, with increasing interest, the growing community of developers who have been focused on tools for R in VS Code. There are now several R packages, VS Code extensions, and a command-line R console available, as well as several tutorials dedicated to R in VS Code. Here we set out to see if VS Code could fill the particular gap we identified in our previous work together ‚Äî the need for seamless remote pair programming. A few of our use cases are:\n\nWe wanted to collaborate on this blog post in a shared .Rmd file which we could edit and knit as we wrote. Ultimately we wrote most of this post on Google Docs before transferring it to a versioned .Rmd on GitHub for finishing touches.\nWe wanted to start collaborating on an open-source R package {mutagen} which will (someday) provide useful extensions to {dplyr}‚Äôs mutate():\n\n\n\n\nSo far, we only have a hex sticker ‚Äî #hexdrivendevelopment in action!\n\n\n\nbut‚Ä¶ I‚Ä¶ already‚Ä¶ know‚Ä¶ how‚Ä¶ the‚Ä¶ hexsticker‚Ä¶ should‚Ä¶ look‚Ä¶\n\n‚Äî Thomas Lin Pedersen (@thomasp85) July 26, 2018\n\n\n\nEmbarrassingly, our last commit to the package was in December 2019. While it would be convenient to blame the pandemic, it was a mix of being busy and other ‚Äòhobby‚Äô projects. However, having a synchronous way of collaborating‚Äîwhere we could see the package being built in action‚Äîwould enable us to truly get the project underway. Before we begin the tutorial, first we have a public service announcement: if you would like to see Microsoft devote additional resources to support R in VS Code, please upvote R in this vscode-jupyter issue!"
  },
  {
    "objectID": "blog/vscode-live-share/index.html#installation",
    "href": "blog/vscode-live-share/index.html#installation",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "Installation",
    "text": "Installation\nWe‚Äôre going to use Homebrew to facilitate installation steps on macOS. We only provide instructions for macOS here, but we do provide links for macOS users who prefer to install applications using the point-and-click method, and for Linux and Windows users to find the correct binaries. We also indicate which steps are optional.\n\nOpen Applications > Utilities > Terminal.\nInstall Homebrew using the terminal command below, also provided on the Homebrew landing page. Paste this code into your terminal:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n(Optional) For our favorite free and open-source terminal on macOS, install iTerm2 by pasting this into your terminal:\n\nbrew install --cask iterm2\n\nIf you don‚Äôt have them already, you‚Äôll need R and RStudio Desktop. We‚Äôre partial to the preview version of RStudio Desktop which has all the latest features. You can install (and update) them easily from your terminal:\n\nbrew install --cask r\nbrew tap homebrew/cask-versions\nbrew install --cask rstudio-preview\n\nInstall radian, a ‚Äò21st century R console‚Äô and the recommended R console for VS Code.\n\nUpdate 2021-02-11: Thanks to the efforts of @jdblischak and @randy3k in this closed issue, radian can now be installed with conda-forge instead of only with pip. Most data science tutorials recommend using Python with conda environments, so we suspect that our readers will be more familiar with using conda than pip, as we are. If you do choose to use pip (or python -m pip), beware that pip should be used carefully inside conda environments.\nThe steps below first install Miniforge, which we prefer to Miniconda since it sets conda-forge as the default channel, then create an empty conda environment named r-console into which we install radian and jedi. jedi is an optional package that enables Python autocompletion using the {reticulate} R package. Paste this into your terminal:\nbrew install miniforge\nconda create --name r-console\nconda activate r-console\nconda install radian\nconda install jedi # for {reticulate} python autocompletion\n\nIn RStudio, install the required R packages for VS Code, namely {languageserver} from CRAN and the latest GitHub release of {vscDebugger}:\n\ninstall.packages(\"languageserver\")\nremotes::install_github(\"ManuelHentschel/vscDebugger\", ref = remotes::github_release())\nFor the tutorial, make sure that you have the following R packages installed, if you don‚Äôt already:\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"purrr\")\ninstall.packages(\"tibble\")\ninstall.packages(\"usethis\")\ninstall.packages(\"vctrs\")\n\nInstall VS Code by pasting this into your terminal:\n\nbrew install --cask visual-studio-code\n\nOpen VS Code and install the following extensions from the Extensions gallery (Shift-Command-X):\n\n\n\nR\nR LSP Client\nR Debugger\nLive Share\nLive Share Audio"
  },
  {
    "objectID": "blog/vscode-live-share/index.html#configuration",
    "href": "blog/vscode-live-share/index.html#configuration",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "Configuration",
    "text": "Configuration\n\nMake sure the r-console environment is still active in your terminal. If it is, your terminal prompt will look like this, or similar:\n\n(r-console) ~ %\nIf not, then reactivate the r-console environment:\nconda activate r-console\n\nType which radian in your terminal to display the path to the radian executable. Below is what it is on our local machines. Copy the path to your clipboard.\n\n/usr/local/Caskroom/miniforge/base/envs/r-console/bin/radian\n\nDetermine whether bash or zsh is your default shell. If you don‚Äôt know, type this into your terminal:\n\necho $SHELL\nOlder Macs may still use bash, while newer Macs or those with older Macs who have created new user accounts since upgrading to macOS Catalina will likely run zsh, the new default macOS shell.\n\nLet‚Äôs take VS Code for a spin! An useful feature to know about is that typing code <path/file.ext> in your terminal will open a new or existing file in VS Code at the path you specify. Many more details are available in the help page code -h. Depending on whether your default shell is bash or zsh, type code ~/.bashrc or code ~/.zshrc in your terminal to open your shell configuration file, and paste the path you copied in step 2. above to add an alias to radian and bind it to lowercase r:\n\nalias r=\"/usr/local/Caskroom/miniforge/base/envs/r-console/bin/radian\"\nYou can paste the alias anywhere in the file, but it is probably best to paste it at the very bottom, and avoid inserting it in any preexisting code blocks (e.g., conda initialize blocks) which may be overwritten. Save your configuration file and restart your terminal for the settings to take effect. Now all you need to open radian is to type r in any terminal (Terminal, iTerm2, RStudio, or VS Code)! Useful radian commands to know are:\nq() # to exit R, same as with vanilla R and RStudio\n;   # to enter shell mode, exit by pressing backspace on an empty line\n~   # to enter {reticulate} python mode, exit by pressing backspace on an empty line\n\nTo configure your .Rprofile, type code ~/.Rprofile in your terminal, or if you are already set up with {usethis} in RStudio, run the R command usethis::edit_r_profile(). At a minimum, you will want to enable {rstudioapi} emulation in VS Code:\n\noptions(vsc.rstudioapi = TRUE)\n\nTo configure radian, type code ~/.radian_profile in your terminal which will open up a new blank editor and create a new file named .radian_profile in the home directory. This is our .radian_profile:\n\noptions(\n    radian.insert_new_line = FALSE,\n    radian.escape_key_map = list(\n        list(key = \"-\", value = \" <- \"),\n        list(key = \"m\", value = \" %>% \")\n    )\n)\nSave ~/.radian_profile and restart your terminal for the settings to take effect.\n\nNow we‚Äôll configure VS Code settings. Open VS Code, and navigate to settings.json by using the Command Palette (Shift-Command-P) and navigating to Preferences: Open Settings (JSON). This is our settings.json:\n\n{\n    // Liveshare: Prompt when receiving focus requests\n    \"liveshare.focusBehavior\": \"prompt\",\n    // R: Treat`names.like.this` as one word for selection\n    \"[r]\": {\n        \"editor.wordSeparators\": \"`~!@#%$^&*()-=+[{]}\\\\|;:'\\\",<>/?\"\n    },\n    // R: Use active terminal for all commands\n    \"r.alwaysUseActiveTerminal\": true,\n    // R: Use bracketed paste mode when sending code to console (radian)\n    \"r.bracketedPaste\": true,\n    // R: R or radian path for macOS\n    \"r.rterm.mac\": \"/usr/local/Caskroom/miniforge/base/envs/r-console/bin/radian\",\n    // R: Enable R session watcher (experimental)\n    \"r.sessionWatcher\": true,\n    // R: Remove hidden items when clearing R workspace\n    \"r.workspaceViewer.removeHiddenItems\": true,\n    // Telemetry: Disable Microsoft crash reporter\n    \"telemetry.enableCrashReporter\": false,\n    // Telemetry: Disable Microsoft telemetry\n    \"telemetry.enableTelemetry\": false,\n    // Terminal: Path to integrated shell on macOS\n    \"terminal.integrated.shell.osx\": \"/bin/zsh\"\n}\n\n\n\n\n\n\nNote\n\n\n\nThe r.rterm.mac field above should be the path to radian if you followed the radian installation instructions above, otherwise it should be the path to your R executable (if you don‚Äôt know it, type which R in your terminal). Since we use Homebrew to install the latest zsh using brew install zsh, the path to our zsh is /usr/local/bin/zsh, but we have put /bin/zsh above since that is the default zsh location for most macOS users. You should run which r and which zsh to confirm your local settings are correct.\n\n\n\nBelow are some useful keyboard shortcuts for keybindings.json, found by using the Command Palette (Shift-Command-P) in VS Code and navigating to Preferences: Open Keyboard Shortcuts (JSON). This is our keybindings.json:\n\n[\n    {\n        \"description\": \"View: Show R Workspace\",\n        \"key\": \"alt+r\",\n        \"command\": \"workbench.view.extension.workspaceViewer\"\n    },\n    {\n        \"description\": \"R: Create R Terminal\",\n        \"key\": \"alt+`\",\n        \"command\": \"r.createRTerm\"\n    },\n    {\n        \"description\": \"R: Insert Assignment Operator\",\n        \"key\": \"alt+-\",\n        \"command\": \"type\",\n        \"when\": \"editorLangId == r || editorLangId == rmd && editorTextFocus\",\n        \"args\": { \"text\": \" <- \" }\n    },\n    {\n        \"description\": \"R: Insert Pipe Operator\",\n        \"key\": \"shift+cmd+m\",\n        \"command\": \"type\",\n        \"when\": \"editorLangId == r || editorLangId == rmd && editorTextFocus\",\n        \"args\": { \"text\": \" %>% \" }\n    },\n    {\n        \"description\": \"R: Insert Assignment Pipe Operator\",\n        \"key\": \"shift+cmd+,\",\n        \"command\": \"type\",\n        \"when\": \"editorLangId == r || editorLangId == rmd && editorTextFocus\",\n        \"args\": { \"text\": \" %<>% \" }\n    },\n    {\n        \"description\": \"R: Insert Tee Pipe Operator\",\n        \"key\": \"shift+cmd+.\",\n        \"command\": \"type\",\n        \"when\": \"editorLangId == r || editorLangId == rmd && editorTextFocus\",\n        \"args\": { \"text\": \" %T>% \" }\n    },\n    {\n        \"description\": \"R: Test Package\",\n        \"key\": \"shift+cmd+8\",\n        \"command\": \"r.test\"\n    },\n    {\n        \"description\": \"R: Document\",\n        \"key\": \"shift+cmd+9\",\n        \"command\": \"r.document\"\n    },\n    {\n        \"description\": \"R: Load All\",\n        \"key\": \"shift+cmd+0\",\n        \"command\": \"r.loadAll\",\n    }\n]"
  },
  {
    "objectID": "blog/vscode-live-share/index.html#writing-an-r-package-using-vs-code",
    "href": "blog/vscode-live-share/index.html#writing-an-r-package-using-vs-code",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "Writing an R Package using VS Code",
    "text": "Writing an R Package using VS Code\nIn this section we‚Äôre going to set up a toy R package in VS Code, and in the next section we will enable Live Share.\n\nOpen VS Code and use the Command Palette (Shift-Command-P) to navigate to R: Create R Terminal, or use the suggested keyboard shortcut above (Option-`). Note that if you have an editor open and the focus is on the editor, for the shortcut to work you may first need to use Control-` for View: Toggle Integrated Terminal followed by Option-` for R: Create R Terminal. If our configuration above worked for you, these commands should open a terminal instance with the radian console, and the title of the terminal should be 1: R Interactive (or 2: R Interactive if it‚Äôs the second active terminal).\nIn the VS Code R console, create a {mutagen} package folder on your Desktop, declare dependencies, and start an R script for a new function cast_integers():\n\nlibrary(usethis)\ncreate_package(\"~/Desktop/mutagen\")\ndeps <- c(\"dplyr\", \"purrr\", \"tibble\", \"vctrs\")\npurrr::walk(deps, use_package)\nuse_r(\"cast_integers\")\n\nIn VS Code, open the {mutagen} folder using File > Open‚Ä¶ (Command-O) and selecting the mutagen folder in the Desktop. This will open a new VS Code instance and the title bar will read Welcome ‚Äî mutagen. The R extension icon will be visible on the sidebar since VS Code will detect the .R file in the folder.\nUse the Explorer icon in the sidebar (Shift-Command-E) and open the cast_integers.R file. Paste the following code into the editor and save the file:\n\n#' Safely cast numeric columns to integers\n#'\n#' `cast_integers()` casts all eligible numeric columns in a data frame to integers, without data loss, using `vctrs::vec_cast()` for coercion.\n#' @param .data A data frame\n#' @return A tibble. If the input data frame has rownames, these will be preserved in a new rowname column.\n#' @examples (mtcars_integerized <- cast_integers(mtcars))\n#' @export\ncast_integers <- function(.data) {\n    stopifnot(is.data.frame(.data))\n    .data <- tibble::rownames_to_column(.data)\n    .data <- tibble::as_tibble(.data)\n    int_index <- purrr::map_lgl(\n        .data,\n        ~ !inherits(try(vctrs::vec_cast(.x, to = integer()), silent = TRUE), \"try-error\")\n    )\n    .data <- dplyr::mutate(\n        .data,\n        dplyr::across(\n            .cols = any_of(names(which(int_index))),\n            .fns = ~ vctrs::vec_cast(.x, to = integer())\n        )\n    )\n    return(.data)\n}\n\nNow create a new R console in VS Code using either the keyboard shortcuts (Control-` to toggle the terminal and Option-` to create an R console) or by opening the Command Palette (Shift-Command-P) and navigating to R: Create R Terminal. Focus your view on the R extension by clicking R the sidebar (Option-R), and now test out the package. Below are a series of R console commands to familiarize you with the package development workflow in VS Code. We will load the package, document it, and create a new object in the R workspace that uses the new function we just loaded onto the package environment. To get familiar with the VS Code user interface, we will also print the object in the console, view it with the data viewer, and check out the help file for our new function.\n\nlibrary(dplyr)\ndevtools::load_all() # or Shift-Command-0 using keyboard shortcuts above\ndevtools::document() # or Shift-Command-9 using keyboard shortcuts above\nmtcars_integerized <- mtcars %>% cast_integers()\nmtcars_integerized\nView(mtcars_integerized)\n?cast_integers"
  },
  {
    "objectID": "blog/vscode-live-share/index.html#live-share-tutorial",
    "href": "blog/vscode-live-share/index.html#live-share-tutorial",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "Live share tutorial",
    "text": "Live share tutorial\nNow that we‚Äôve started writing our R package in VS Code, it‚Äôs time to Live Share! Microsoft has a handy walkthrough here, but we will explain what we did and detail R project idiosyncrasies that we found.\nIn order to host a Live Share collaboration session, first you‚Äôll need to sign into Visual Studio Live Share. Click on the Live Share status bar item or press Shift-Command-P and navigate to Live Share: Sign In With Browser. A notification will appear asking you to sign into either a Microsoft or GitHub account using your default web browser.\nOnce you are signed in, open a folder or file for pair programming. For package development, this would be the project folder for the package (e.g., the {mutagen} folder we created above). Click the Live Share status bar item or type Shift-Command-P and navigate to Live Share: Start Collaboration Session (Share). An invite link will be copied automatically to your clipboard. You can send this link to your collaborator(s) and allow them to join a new session that shares the contents of the folder. Once a collaborator clicks the link, a notification will prompt you to approve the guest before they can join. Please note that guests do not necessarily need to be signed into to collaborate using Live Share, since anonymous guests are allowed; however, the host will need to be signed in. In fact, guests don‚Äôt even need to have VS Code installed and can join Live Share from a browser!\nOnce a guest has joined a collaboration session, all collaborators will immediately be able to see each other‚Äôs cursors, edits, and selections in real-time. You can pick a file from the Explorer (Shift-Command-E) and start editing. Both hosts and guests will see edits as they are made.\nAs we worked on {mutagen} files, we noticed the usual loading of a package in development, via devtools::load_all(), or sourcing an entire file using Shift-Command-S (the VS Code version of RStudio‚Äôs Shift-Command-Enter), will not work for the guest. Why? Because the path to the folder directory is different for the host and guest. In our example, instead of the host‚Äôs path ~/Desktop/mutagen/R/cast_integers.R, the guest‚Äôs path was an abbreviated /R/cast_integers.R, which led to errors when loading and sourcing from keyboard shortcuts. However, the guest can still run the code themselves (such as by selecting and running the source code in an R script, or using Command-Enter line by line) as long as they do not try to source the entire package. We did not try to abstract the path-generating rules for the guests‚Äô Live Share instance into new keyboard shortcuts, but this seems like a solvable problem to allow guests to run devtools::load_all() or source() with minor modifications. Another gotcha was the R workspace; we tried to toggle the setting \"liveshare.publishWorkspaceInfo\": true in settings.json, but it did not let us share the R workspace between host and guest. It appears that for now host and guest(s) can only maintain separate R workspaces, though it would be useful to have the option to share the R s and its objects."
  },
  {
    "objectID": "blog/vscode-live-share/index.html#final-thoughts",
    "href": "blog/vscode-live-share/index.html#final-thoughts",
    "title": "Remote pair programming in R using Visual Studio Code and Live Share",
    "section": "Final thoughts",
    "text": "Final thoughts\nWe‚Äôre excited for the future of R programming in VS Code. Live Share is an outstanding innovation for collaborative real-time editing, and is only one of the many amazing features in VS Code. Here we have only scratched the surface of use cases for Live Share. Microsoft keeps a running list of many other Live Share use cases.\nWe are also huge fans of RStudio and the RStudio IDE. On our wish list for RStudio is that a live collaboration feature be supported in future versions of RStudio Desktop (not only the Pro version) and RStudio Cloud. Having many excellent options is a good thing! In the meantime, it is clear that VS Code is maturing into a capable IDE for R. Live Share is an attractive and free feature that enables real-time collaboration for the masses and that we hope others will enjoy as much as we have.\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nüéâüì¢üéâ Collaborating in #rstats? Git got you down? Want to code with others remotely, in real-time, and for free (√† la Google Docs)? Check out how to set up and use @code (#vscode) and #vsliveshare for remote #pairprogramming ü§ù in R! ü§Ø. Tutorial here: https://t.co/Qbrw71V77Z pic.twitter.com/3sy9hAJea7\n\n‚Äî Isabella Vel√°squez (@ivelasq3) February 4, 2021"
  },
  {
    "objectID": "blog/rtistry-intro/index.html",
    "href": "blog/rtistry-intro/index.html",
    "title": "Getting started in #rtistry",
    "section": "",
    "text": "E. A. S√©guy, Vintage flower patterns (1925)\nArtists in the R community have been using the #rtistry hashtag to demonstrate their gorgeous, dynamic art using only the R programming language. Their creations are amazing and they inspired me to try out generative art this dreary Sunday.\nI am proud to showcase my first #rtistry plot ever! Kinda reminds me of KidPix (remember KidPix?!). I wanted to share how I did it and welcome any feedback or advice, as this is totally new and I‚Äôm not even sure if I am doing it right?\nAs always, we start with the packages we will use:"
  },
  {
    "objectID": "blog/rtistry-intro/index.html#write-a-parametric-equation",
    "href": "blog/rtistry-intro/index.html#write-a-parametric-equation",
    "title": "Getting started in #rtistry",
    "section": "Write a parametric equation",
    "text": "Write a parametric equation\nFirst up is figuring out the function you will use to create the plot. I decided to go with a parametric equation for my first #rtistry plot. A parametric equation of a curve expresses the coordinates of points of the curve as functions of a variable. This blog post explains the concept very clearly and also has various examples of wonderful parametric equations.\nWhy parametric equations? First, even simple equations can create beautiful symmetries. Second, they are easy to modify to find the perfect plot.\nThe simplest parametric equation uses cosine and sine to make the unit circle:\n\ncircleFun <- function(center = c(0, 0), diameter = 1, npoints = 100){\n    r = diameter / 2\n    tt <- seq(0,2*pi,length.out = npoints)\n    xx <- center[1] + r * cos(tt)\n    yy <- center[2] + r * sin(tt)\n    return(data.frame(x = xx, y = yy))\n}\n\ndat <- \n  circleFun(c(1, -1), 2.3, npoints = 100)\n\nggplot(dat,aes(x, y)) +\n  geom_path()\n\n\n\n\n\n\n\n\nLet‚Äôs write a function to create a parametric equation. I based this equation on the aforementioned blog post equations. The parameters are:\n\ncenter: the center of the equation\nnpoints: the number of points used to evaluate the function.\nc1, c2, c3, c4: the coefficients for the equation\n\n\ngenFun <- function(center = c(0, 0), npoints = 500, c1 = 2.5, c2 = -5, c3 = 4.28, c4 = 2.3){\n    t <- seq(0, 2*pi, length.out = npoints)\n    xx <- center[1] + c1*(sin(c2*t)*sin(c2*t))*(2^cos(cos(c3*c4*t)))\n    yy <- center[2] + c1*sin(sin(c2*t))*(cos(c3*c4*t)*cos(c3*c4*t))\n    a <- data.frame(x = xx, y = yy)\n    \n    return(a)\n}\n\nPlaying around with the function, we see how the graph gets smoother with more points and how the shape changes with different coefficients.\n\ndat <- \n  genFun(c(1,-1), npoints = 100)\n\nggplot(dat, aes(x, y)) +\n  geom_path()\n\n\n\n\n\n\n\n\n\ndat <- \n  genFun(c(1,-1), npoints = 500, c1 = 5, c2 = -3, c3 = 5, c4 = 2)\n\nggplot(dat, aes(x, y)) +\n  geom_path()"
  },
  {
    "objectID": "blog/rtistry-intro/index.html#create-variation-within-the-graph",
    "href": "blog/rtistry-intro/index.html#create-variation-within-the-graph",
    "title": "Getting started in #rtistry",
    "section": "Create variation within the graph",
    "text": "Create variation within the graph\nNow that we have a basic shape, let‚Äôs play around with different aspects of the graph:\n\ngeoms\nAesthetic specifications\nRepetition\n\n\ngeoms\nWe started off with geom_path but can play around with other geoms too. Here it is with geom_line:\n\ndat <- \n  genFun(c(1,-1), npoints = 5000)\n\nggplot(dat, aes(x, y)) +\n  geom_line()\n\n\n\n\n\n\n\n\nAnd with geom_point:\n\nset.seed(1234)\n\ndat <- \n  genFun(c(1,-1), npoints = 500)\n\ndat %>% \n  ggplot(aes(x, y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nAesthetic specifications\nThe {ggplot2} package has several aesthetic specifications available for plots. A full list can be found here.\nWe‚Äôre going to go ahead and get rid of all the background using theme_void().\n\nSize\nLet‚Äôs go with geom_point. In this case, we can start playing around with the aesthetics to see what would look interesting. With geom_point, you can edit the sizes of the points, so we create a column with random point sizes to create some variation.\n\nset.seed(1111)\n\ndat <- \n  genFun(c(1,-1), npoints = 5000) %>% \n  mutate(rand_w = sample(n())/3000)\n\ndat %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = dat$rand_w) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nShape\nWe could also change the shape of each of the points, but I liked the circles more:\n\ndat %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = dat$rand_w,\n             shape = 8) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nOpacity\nWe could also change the opacity of the points:\n\nset.seed(1234)\n\ndat <- \n  dat %>% \n  mutate(rand_o = sample(n())/5000)\n\ndat %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = dat$rand_w,\n             alpha = dat$rand_o) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nColor\nWe can also create a column for random numbers to ascribe colors to each point. I decided to use the magma color palette from the {viridis} package because it is so vibrant. Now that we‚Äôre not using only black points, we can use dark_theme_void() from the {ggdark} package for a fully black background.\n\nset.seed(1234)\n\ndat <- \n  dat %>% \n  mutate(rand_c = sample(n()))\n\ndat %>% \n  ggplot(aes(x, y,  color = rand_c)) +\n  geom_point(size = dat$rand_w,\n             alpha = dat$rand_o) +\n  scale_color_viridis(option = \"magma\") +\n  dark_theme_void() +\n  theme(legend.position = \"none\") # remove legend\n\nInverted geom defaults of fill and color/colour.\nTo change them back, use invert_geom_defaults().\n\n\n\n\n\n\n\n\n\n\n\n\nRepetition\nNotice we added rand_w, rand_o, and rand_c so that we can randomly change up the size, opacity, and color of the plot. Let‚Äôs go back to our original generative function and them as parameters. Now we can change them without having to add them to the data frame externally from the function. (Apologies for the switching back and forth from dplyr to base R!)\n\ngenFun <- function(center = c(0, 0), npoints = 500, c1 = 2.5, c2 = -5, c3 = 4.28, c4 = 2.3, size_denom = 1, opacity_denom = 1, color_denom = 1){\n    t <- seq(0, 2*pi, length.out = npoints)\n    xx <- center[1] + c1*(sin(c2*t)*sin(c2*t))*(2^cos(cos(c3*c4*t)))\n    yy <- center[2] + c1*sin(sin(c2*t))*(cos(c3*c4*t)*cos(c3*c4*t))\n    rand_w <- sample(0:20, npoints, replace = TRUE)/size_denom\n    rand_o <- sample(1:100, npoints, replace = TRUE)/opacity_denom\n    rand_c <- sample(1:100, npoints, replace = TRUE)/color_denom\n    a <- data.frame(x = xx, y = yy, rand_w = rand_w, rand_o = rand_o, rand_c = rand_c)\n    \n    return(a)\n}\n\nNow playing around with the new parameters, I decided to go with this plot:\n\nset.seed(1111)\n\ndat <- \n  genFun(c(0, 0), npoints = 5000, c1 = 5, c2 = -3, c3 = 5, c4 = 2, size_denom = 1.5, opacity_denom = 50)\n\ndat %>% \n  ggplot(aes(x, y,  color = rand_c)) +\n  geom_point(size = dat$rand_w,\n             alpha = dat$rand_o) +\n  scale_color_viridis(option = \"magma\") +\n  dark_theme_void() +\n  theme(legend.position = \"none\") # remove legend\n\n\n\n\n\n\n\n\nWhat this allows us to do is change up the generative function and build on our plot. I was interested in rotating the plot around the axis.\n\ndat %>% \n  ggplot() +\n  geom_point(aes(x, y,  color = rand_c),\n             size = dat$rand_w,\n             alpha = dat$rand_o) +\n  geom_point(aes(-x, -y,  color = rand_c),\n             size = dat$rand_w,\n             alpha = dat$rand_o) +\n  geom_point(aes(-y, x,  color = rand_c),\n             size = dat$rand_w,\n             alpha = dat$rand_o) +\n  geom_point(aes(-y, -x,  color = rand_c),\n             size = dat$rand_w,\n             alpha = dat$rand_o) +\n  scale_color_viridis(option = \"magma\") +\n  dark_theme_void() +\n  theme(legend.position = \"none\") # remove legend\n\n\n\n\n\n\n\n# ggsave(here::here(\"public\", \"img\", \"rtistry2.png\"), dpi = 320, height = 6, width = 8) # this saves the image\n\nTada! My first ever #rtistry plot."
  },
  {
    "objectID": "blog/rtistry-intro/index.html#conclusion",
    "href": "blog/rtistry-intro/index.html#conclusion",
    "title": "Getting started in #rtistry",
    "section": "Conclusion",
    "text": "Conclusion\nThis was my attempt to create generative art in R! It was fun to figure out how on earth to even get started and see how the plots change with new parameters on different attempts. I welcome any thoughts and please share your art using the #rtistry hashtag with me!\n\nrtists I follow\n\nDanielle Navarro\nIjeamaka A\nGeorgios Karamanis\nTobias Stalder\n\n\n\nPackages for creating generative art\n\ndjnavarro‚Äôs flametree\nijeamaka_a‚Äôs contourR\ncutterkom‚Äôs generativeart\n\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew post! üì£ Interested in using #rstats to create KidPix-like #generativeart üé® ? Check out my first foray into #rtistry using a parametric equation here! üë©üèª‚Äçüé® https://t.co/ykkddoK5tmreposted due to typo üòÖ pic.twitter.com/V5bFSvDsOR\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 11, 2021"
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html",
    "href": "blog/so-youre-getting-a-masters/index.html",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "",
    "text": "Pieter Claesz, Vanitas Still Life with the Spinario (1628)\nThe new school year is right around the corner and I have gotten a few requests from new students entering a Data Science or Analytics program to talk about my experience in graduate school. I graduated with a Masters of Science in Analytics from the University of Chicago in June 2016. It was an amazing experience that helped propel me into a new career but there are definitely things I wish I had known or would have done differently.\nI‚Äôve tried to keep track of the different questions people ask and aggregate them into a list. A few caveats:\nHere is my advice to aspiring data scientists/analysts entering these programs and how I would have made an already great experience even better."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#learn-git-and-put-everything-on-github-or-equivalent",
    "href": "blog/so-youre-getting-a-masters/index.html#learn-git-and-put-everything-on-github-or-equivalent",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "1. Learn Git and put everything on GitHub (or equivalent)",
    "text": "1. Learn Git and put everything on GitHub (or equivalent)\nGit and GitHub was not required for any classes (at least when I was in the program) and I really regret not using it with my classmates.\n\nGitHub is a great way to show off your skills to potential employers.\nIt‚Äôs a much better collaboration tool than what we did (which was email each other with code changes).\nIt‚Äôs also a good history so you remember things you‚Äôve done (which I have forgotten already).\nIt‚Äôs also way neater to have homework in projects in GitHub as opposed to a zillion folders on your Desktop (which is what I went with).\nGit[hub] is also a great skill to have, as many data analysis/science teams use it regularly!\nThere‚Äôs also nothing better than having a filled up contribution graph."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#leave-each-class-with-an-example-that-you-can-show-a-potential-employer",
    "href": "blog/so-youre-getting-a-masters/index.html#leave-each-class-with-an-example-that-you-can-show-a-potential-employer",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "2. Leave each class with an example that you can show a potential employer",
    "text": "2. Leave each class with an example that you can show a potential employer\nThese programs go quickly. I was a part-time student and took my sweet time and even then I felt like I was racing towards the end. Due to competing priorities, it‚Äôs easy to rush through homework assignments and projects. Instead of focusing on completion, remember the goal of this is to enter a new career/role/path in life and that everything you do is an example of a new skill you can showcase. So: document your work. Be clear in the intention behind it and how you went through your thought process. Make it presentable. Related to the advice above, link potential employers to your GitHub.\nOf course, I understand that not all homework applies to this ‚Äî but a lot of it does, in particular your final projects. Are you going to put on your resume that you can produce dashboards with data visualizations? You must have an example to back that up ‚Äî and be able to show how this project illustrates the value you can bring to your future company."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#diversify-your-programming-languages",
    "href": "blog/so-youre-getting-a-masters/index.html#diversify-your-programming-languages",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "3. Diversify your programming languages",
    "text": "3. Diversify your programming languages\nMy classes were primarily in R, with only one course requiring Python. In a short amount of time, I picked up and could use Python ‚Äî and I lost my skills in about the same amount of time once the course was over! Without the discipline of practicing and keeping up my programming skills in languages other than R, they quickly were put on the backburner and eventually forgotten.\n\n\n\n Once I was finishing up school and applying for jobs, I had to admit to potential employers that my R skills were much stronger. This ultimately worked out but it did put me out of the running for any Python shops. To replicate the work I did in Python, I‚Äôd have to refresh myself on even the basics of the language ‚Äî which is a shame considering how much effort I put in when I was in the class."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#choose-your-capstone-project-and-advisor-carefully",
    "href": "blog/so-youre-getting-a-masters/index.html#choose-your-capstone-project-and-advisor-carefully",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "4. Choose your Capstone project and advisor carefully",
    "text": "4. Choose your Capstone project and advisor carefully\nWhen it was Capstone time, the options were listed on a website and we were told to form a group, choose a project, and pick out an advisor. It seemed like those were the only variables but in actuality there were many other considerations that impacted the difficulty and complexity of the project. They directly affected whether or not someone graduated when they wanted so this is a decision to take very seriously.\n\nResearch Question: Is the research question defined (clearly) by the Capstone sponsor? If not, a lot of time will be dedicated to research question development, edits, rewrites, and eventual approval.\nData: The addition of a project to the Capstone list does not mean that the data will be readily available to you when you want to begin your project. Several students chose a project under the assumption that the Capstone sponsor would immediately send over the data and instead had to wait a quarter (!) before they actually received anything and could begin analysis. Ask: Is the data available, and if not, how will it be collected? Will you need to sign a confidentiality agreement and who is in charge of that? How will the data be transferred to you and in what format? How long will it take to get the data? Will you need to update or refresh it throughout the course of your Capstone? Will you refine your research question(s) once you have the data?\nSponsor Availability/Interaction: Some sponsors want to be involved in every step of the process. Some sponsors will see you once a quarter. There are pros and cons to each approach but generally speaking you want a sponsor that provides clear guidance and expectations without micromanaging. Try to find out the kind of sponsor you have so you know how to best work with them. You are in it together for the long haul!\nAdvisor Availability/Interaction: Similar to the sponsor, the Capstone advisor will vary in their expectations of how they will collaborate. Be mindful that some advisors are handling a heavy load ‚Äî another job, classes, and several Capstone projects at the same time ‚Äî and therefore you have to account for how you will get their time and attention. This can be frustrating if you need help or are struggling to meet a deadline. Decide your expectations early ‚Äî my group set up a standing weekly call and biweekly in-person meeting with our advisor.\nAdvisor Specialty: Some advisors have more experience or insight in the particular analysis you are trying to do. My Capstone project dealt with survey respondents ‚Äî and luckily my advisor was a marketing researcher! However, sometimes the advisor‚Äôs specialty differs from your analytic approach and your group will have to do more research on their own to fulfill the sponsor‚Äôs needs.\nGroup Availability/Interaction: In a sudden turn of events, one of the members of a Capstone group quit to start a new job in another state. This left their group in disarray as they tried to figure out how to manage the rest of their project. While I am happy for the person who got a job (that is the point of it all, anyway), they could have helped out their team by being open and honest. Ask your potential teammates: What happens if you were offered a new job? Is your graduation timeline fixed or flexible? How many other courses are you taking and how much time can you dedicate to this project? Along with these questions, figure out how everybody intends to interact and set clear expectations from the beginning so that the group can work towards success together."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#keep-track-of-and-give-recommendations-on-how-the-program-can-improve",
    "href": "blog/so-youre-getting-a-masters/index.html#keep-track-of-and-give-recommendations-on-how-the-program-can-improve",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "5. Keep track of and give recommendations on how the program can improve",
    "text": "5. Keep track of and give recommendations on how the program can improve\nI was in the second cohort of my Analytics program and even though it‚Äôs been a couple of years since then, most data science curriculum are relatively new. There are still kinks to work out in terms of what should be required, what should be optional, what order to take courses in, etc. For example, we were thrust into the world of R because our homework had to be done in R, even though many of us had never even installed a package before! We made the recommendation to administration that they should create a short course covering the basics and now it‚Äôs offered to any student who may benefit from it.\n\n\n\n As you go through your program, everything will seem to go so quickly. The details will get lost in the blur unless you intentionally think about how to improve your experience and the experience of those who will come after you."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#learn-what-you-love-about-the-process",
    "href": "blog/so-youre-getting-a-masters/index.html#learn-what-you-love-about-the-process",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "6. Learn what you love about the process",
    "text": "6. Learn what you love about the process\nAs you progress through your Analytics or Data Science program, you will learn the nuance and complexity of data work. It is a fascinating journey that requires project management, client interaction and understanding, technical expertise, feedback loops, and trial and error. It‚Äôs all amazing and necessary but there will be parts that you enjoy more. I have always been into creating aesthetically pleasing work and knew I would love developing data visualizations. I didn‚Äôt know that I love the creativity and mind puzzles that come with data wrangling. Some people feel the opposite ‚Äî that is their least favorite part of the data science routine. But they may enjoy business requirements gathering a lot more than I do.\nYou should know how to successfully accomplish the data science methodology but graduate school is also an opportunity to learn which part excites you. Because of the enormous world of data analytics out there, this will help hone in on the role that can bring joy to your work as much as possible."
  },
  {
    "objectID": "blog/so-youre-getting-a-masters/index.html#conclusion",
    "href": "blog/so-youre-getting-a-masters/index.html#conclusion",
    "title": "So you‚Äôre getting a Masters in Analytics",
    "section": "Conclusion",
    "text": "Conclusion\nThe purpose of these data science/analytics degrees is to serve as a means to an end but it‚Äôs been two years since I finished my Masters and I still reflect on the experience. Along the journey, there was a lot of change and improvement. My Analytics program impacted how I think, my career trajectory, and what I do outside of work. For that I am thankful, as I also realize that there is no definite end to learning.\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nFirst blog post on my new #blogdown üéâ: So You're Getting a Masters in Analytics üë©üèΩ‚Äçüéìüë®üèΩ‚Äçüéì‚ÄºÔ∏è Reflections from an R-Centric Program https://t.co/mvOPKbcSeG #datascience pic.twitter.com/uaq721m2ZL\n\n‚Äî Isabella Vel√°squez (@ivelasq3) August 8, 2018"
  },
  {
    "objectID": "blog/get-tweet-likes/index.html",
    "href": "blog/get-tweet-likes/index.html",
    "title": "Browse and search liked Tweets using R",
    "section": "",
    "text": "Rijksmuseum, Pigeons in white and blue (1928)\nThe other day, I was scrolling through my ‚Äúliked‚Äù Tweets and got maybe 100 deep when I accidentally clicked a hyperlink and had to restart scrolling from the beginning. The experience made me so frustrated that I started looking up easier ways of perusing one‚Äôs likes. I found this tutorial that walked through how to pull liked Tweets into a CSV using Python - so I decided to give it a shot using R."
  },
  {
    "objectID": "blog/get-tweet-likes/index.html#set-up-your-project",
    "href": "blog/get-tweet-likes/index.html#set-up-your-project",
    "title": "Browse and search liked Tweets using R",
    "section": "Set up your project",
    "text": "Set up your project\nFor this tutorial, I set up a project with the following structure. I walk through how to create like.js and like_csv.csv in the walkthrough.\n+-- code.R\n+-- data\n|   +-- like.js\n+-- output\n|   +-- likes_csv.csv"
  },
  {
    "objectID": "blog/get-tweet-likes/index.html#getting-liked-tweets",
    "href": "blog/get-tweet-likes/index.html#getting-liked-tweets",
    "title": "Browse and search liked Tweets using R",
    "section": "Getting liked Tweets",
    "text": "Getting liked Tweets\n\nStep 1: Download your Twitter archive\nLike the Python tutorial, you have to first download your Twitter archive. Go to https://twitter.com/settings/your_twitter_data, enter your password, and ‚ÄúDownload Archive‚Äù. You will get an email when you can download your data - for me, it took a day.\nOnce downloaded, open the ZIP file. Save the like.js file in your project‚Äôs data folder.\n\n\nStep 2: Load the JS file and convert it to a data frame\nNow, start an R script (I called mine code.R). Install (if you haven‚Äôt already) and load all the packages.\n\nlibrary(tidyverse)\nlibrary(rjson)\nlibrary(rtweet)\n\nThe like.js file is a Javascript file. To convert it to a data frame, first convert to a JSON file by removing the text that makes it Javascript, read in the JSON, and then flatten into a data frame using purrr::flatten_df.\n\njs <- read_file(here::here(\"data\", \"like.js\"))\njson <- sub(\"window.YTD.like.part0 = \", \"\", js)\n \nlikes_raw <-\n  fromJSON(json)\n\nlikes_df <-\n  likes_raw %>% \n  flatten_df()\n\nNow your likes, their status IDs, and URLs are stored in likes_df.\n\n\nStep 3: Authenticate {rtweet} and get tweet info\nTo get the Tweet info, you have to authenticate your Twitter account and get the API keys. Go to your Twitter Developer Portal to do this. The {rtweet} vignette here covers what to do pretty well, but one thing I noticed is now you have to enable ‚Äú3-legged OAuth‚Äù and there, you can add the Callback URL of http://127.0.0.1:1410 (otherwise you will get a 401 error when running the code below).\n\ntoken <-\n  create_token(\n    consumer_key = \"PUT API KEY IN QUOTES HERE\",\n    consumer_secret = \"PUT API KEY SECRET IN QUOTES HERE\"\n  )\n\nOnce your token is saved, you can use rtweet::lookup_statuses to get the Tweet info.\n\nlikes_dat <-\n  lookup_statuses(likes_df$tweetId)\n\nThere you have it! All your likes and their information are saved in the likes_dat data frame. Here, you can do all of your usual data manipulations - lookup_statuses() results in a LOT of data, so I recommend narrowing it down to the columns you care about using select().\n\n\nStep 4: Save to a CSV\nSaving to a CSV is a bit tricky because lookup_statuses() creates list-columns, which cannot be exported to a CSV.\nHere, I‚Äôve selected only the columns status_id, text, and hashtags to look at. hashtags is a list-column because one Tweet may have multiple hashtags, which is then saved as a vector within the column hashtags. To convert it to a data frame, I use unnest(). This creates a data frame with multiple entries for each Tweet, one for each hashtag.\nThat is too long for me since I want to quickly look through my likes. I then create a hashtag ID for each tweet by grouping by the status_id and assigning each of its hashtags an ID using row_number() and then using pivot_wider() to move each hashtag into its own column.\nYou will see that some tweets have blank entries while some tweets have many!\n\nlikes_csv <-\n  likes_dat %>% \n  select(status_id, text, hashtags) %>% \n  unnest() %>% \n  group_by(status_id) %>% \n  mutate(id = row_number()) %>% \n  pivot_wider(names_prefix = \"hashtag_\",\n              names_from = id,\n              values_from = hashtags)\n\nNow, this is savable in a CSV. You can run the code below if your project has an output folder.\n\nwrite_csv(likes_csv, here::here(\"output\", \"likes_csv.csv\"))\n\nThank you for reading this quick entry in looking through your Twitter likes. I hope this reduces the amount of annoying scrolling you need to do to look through your Tweets! I think there are a lot of opportunities to make this an even easier process - maybe a Shiny app? - but hopefully, this is a good starting point.\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nAre you, too, constantly frustrated there's no way to easily look through your liked Tweets on Twitter? üê¶ üò§ You can save all of your liked tweets into a CSV using R and {rtweet}! Quick tutorial here: https://t.co/etoOaczTwa #rstats\n\n‚Äî Isabella Vel√°squez (@ivelasq3) November 11, 2020"
  },
  {
    "objectID": "blog/average-hike/index.html",
    "href": "blog/average-hike/index.html",
    "title": "Taking a peek into my hiking data",
    "section": "",
    "text": "Thomas Moran, Winter in the Rockies (1867)\nI moved to Seattle at the end of 2016 and since then have done over 100 hikes (depending on your definition of ‚Äòa hike‚Äô). I must admit I‚Äôve been abysmal at tracking any data regarding my hiking activity beyond a Google spreadsheet, despite the ubiquity of trail tracking apps that exist.\nRecently, I signed up on AllTrails to start collecting data on my hikes. The Pro service offers many wonderful features, including the ability to download GPX data on hikes. I was so excited by this that I decided to try to visualize the hikes I have done.\nI ran a poll on Twitter in which I asked whether people embed code in the main text of their blog post or at the end. 91% embed in the main text [n = 85]. I structured this post accordingly."
  },
  {
    "objectID": "blog/average-hike/index.html#pulling-data",
    "href": "blog/average-hike/index.html#pulling-data",
    "title": "Taking a peek into my hiking data",
    "section": "Pulling data",
    "text": "Pulling data\n\nChoose packages\nIt took a bit to decide which packages had the functions needed to run the spatial analyses. In the end, I decided on:\n\nplotKML: A package containing functions to read GPX files.\ngeosphere: A package containing functions for geospatial calculations. I decided to use this for finding out distances between lon/lat.\ngoogleway: A package allowing access to the Google Maps API. To run this, you need to obtain a Google Maps API key and load it to R by using set_key(). I use this for elevation calculations but the API can also obtain the distance between points.\n\n\nlibrary(tidyverse)\nlibrary(googleway)\nlibrary(plotKML)\nlibrary(geosphere)\n\ngoogleway::set_key(API_KEY_HERE)\n\n\n\nTidy data\nI downloaded each GPX file from AllTrails and saved them in a file in my project organization. Their file names were TRAILNAME.gpx.\n\nUsing plotKML::readGPX() results in the files being loaded as lists.\nI used purrr in conjunction with plotKML() to handily read them in and add the file name to the list.\n\n\n# find gpx files\ndata_path <- \n  here::here(\"data\", \"raw\", \"gpx_files\")\n\nfiles <-\n  dir(data_path, pattern = \"*.gpx\", full.names = TRUE)\n\n# get trail names\nnames <-\n  dir(data_path, pattern = \"*.gpx\", full.names = FALSE) %>% \n  str_extract(\".+?(?=.gpx)\")\n\n# read all gpx files\ngpx_dat <-\n  map2(files,\n       names,\n       ~ readGPX(.x,\n         metadata = TRUE,\n         bounds = TRUE,\n         waypoints = TRUE,\n         tracks = TRUE,\n         routes = TRUE) %>%\n         list_modify(trail = .y)) # otherwise you can't tell which entry is for which trail\n\n\n\nCalculate elevation\nWe can use googleway::google_elevation() to access the Google Elevation API and calculate elevation for every lon/lat pair from the GPX files. Unfortunately, the API accepts and returns only a few requests at a time (~200 rows for these files). We have over 51,000 rows of data. So, we can create groups for every 200 rows and use a loop to make a call for each\nThis results in a list, so we can then create a tibble pulling out the data we want.\n\nlonlat_dat <-\n  gpx_dat %>%\n  map_df(., ~.x$\"routes\"[[1]], .id = \"trail\") %>%\n  select(trail, lon, lat) %>% \n  group_by(trail) %>% \n  ungroup() %>% \n  mutate(group_number = (1:nrow(.) %/% 200) + 1) # https://stackoverflow.com/questions/32078578/how-to-group-by-every-7-rows-and-aggregate-those-7-values-by-median\n\ndat_lapply <- lapply(1:max(lonlat_dat$group_number), function(x) {\n  Sys.sleep(3)\n  \n  lonlat_dat %>%\n    filter(group_number == x) %>% # added a filter so you only pull a subset of the data.\n    do(elev_dat =\n         data.frame(\n           google_elevation(\n             df_locations = dplyr::select(., lon, lat),\n             location_type = \"individual\",\n             simplify = TRUE)))\n  })\n\ndat_lapply_elev_dat <-\n  dat_lapply %>%\n  map(., ~ .x$\"elev_dat\"[[1]])\n\nelev_df <-\n  dat_lapply_elev_dat %>% {\n    tibble(\n      elevation = map(., ~ .x$\"results.elevation\"),\n      lon = map(., ~ .x$\"results.location\"[[\"lng\"]]),\n      lat = map(.,  ~ .x$\"results.location\"[[\"lat\"]])\n    )\n  } %>% \n  unnest(.id = \"group_number\") %>% \n  select(group_number, elevation, lon, lat)\n\n\n\nCalculate distance\nNow we have a list of trails, longitudes and latitudes along their paths, and the elevation for each of those points. Now we want to calculate the distance along the paths.\n\nWe bring back lonlat_dat so we know what trails with which each point are associated.\nTo use calculate distance, we can use distHaversine() with two sets of lon/lat. We create the second set of lon/lat by creating a new variable that takes the ‚Äúnext‚Äù value in a vector (so we‚Äôre calculating the distance between point A and point B, point B to point C, and so on).\ncumsum() accumulates the distances between each set of lon/lat.\nFinally, we calculate the elevation gain for each hike.\n\n\nhiking_dat <-\n  plyr::join(elev_df, lonlat_dat, type = \"left\", match = \"first\") %>% \n  group_by(trail) %>% \n  mutate(elev_feet = elevation * 3.281, # to convert to feet\n         lon2 = lead(lon, 1),\n         lat2 = lead(lat, 1)) %>%\n  ungroup() %>% \n  mutate(dist = distHaversine(hiking_dat[, 2:3], hiking_dat[, 7:8])/1609.344) %>% # to convert to miles\n  group_by(trail) %>% \n  mutate(cumdist = cumsum(dist),\n         elev_gain = elev_feet - first(elev_feet)) %>%\n  ungroup()\n\n\n\nCreate additional tables\nFor nerdy kicks, I also wanted to find out my ‚Äòaverage‚Äô hike - that is, the average distance, the average elevation, and the average elevation for each distance. I also wanted to see the total distance and elevation for each trail for which I pulled data.\n\navg_elev <- # average elevation by distance\n  hiking_dat %>% \n  group_by(round(cumdist, 1)) %>% \n  summarize(mean(elev_gain))\n\nhiking_dat_by_trail <- # total gain/distance by trail\n  hiking_dat %>% \n  select(trail, cumdist, elev_gain) %>% \n  group_by(trail) %>%\n  summarize(tot_dist = max(cumdist, na.rm = T),\n            tot_elev_gain = max(elev_gain)) %>% \n  mutate(tot_dist_scaled = scale(tot_dist), # for cluster analysis\n         tot_elev_scaled = scale(tot_elev_gain))"
  },
  {
    "objectID": "blog/average-hike/index.html#creating-visualizations",
    "href": "blog/average-hike/index.html#creating-visualizations",
    "title": "Taking a peek into my hiking data",
    "section": "Creating visualizations",
    "text": "Creating visualizations"
  },
  {
    "objectID": "blog/average-hike/index.html#disclaimer",
    "href": "blog/average-hike/index.html#disclaimer",
    "title": "Taking a peek into my hiking data",
    "section": "Disclaimer",
    "text": "Disclaimer\nFor data collection, I downloaded each trail‚Äôs GPX files from AllTrails. Because these data are proprietary, I will not be providing them. Some things to note:\n\nBecause these are data pulled from the website, they are not indicative of my actual hiking path (for example, Franklin Falls is a 2-mile hike in the summer, but in the winter is a 6-mile snowshoe).\nThere are hikes that I did back-to-back that I‚Äôd consider one hike but the trails might be listed separately on the site. For example, Deception Pass is actually made up of three small loops."
  },
  {
    "objectID": "blog/average-hike/index.html#the-hikes-are-wide-and-varied",
    "href": "blog/average-hike/index.html#the-hikes-are-wide-and-varied",
    "title": "Taking a peek into my hiking data",
    "section": "The hikes are wide and varied",
    "text": "The hikes are wide and varied\nBeing fortunate enough to live near multiple mountain ranges, the hikes come in all shapes and sizes.\n\nggplot() + \n  geom_density_ridges(data = na.omit(hiking_dat),\n                      aes(x = cumdist,\n                          y = trail,\n                          group = trail),\n                      fill = \"#00204c\",\n                      rel_min_height = 0.01\n                      ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\nI calculated my ‚Äòaverage hike‚Äô - that is, the average elevation given the cumulative distance traveled.\n\nggplot() + \n  geom_ridgeline(data = hiking_dat,\n                 aes(x = cumdist,\n                     y = trail,\n                     group = trail,\n                     height = elev_gain),\n                 color = \"#c9b869\",\n                 alpha = 0) +\n  geom_line(data = avg_elev,\n            aes(x = `round(cumdist, 1)`,\n                y = `mean(elev_gain)`),\n            color = \"#00204c\",\n            size = 2) +\n  scale_x_continuous(name = \"Cumulative Distance (miles)\") +\n  scale_y_continuous(name = \"Cumulative Elevation (ft)\", limits = c(0, 5000)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/average-hike/index.html#aggregated-data-by-trail",
    "href": "blog/average-hike/index.html#aggregated-data-by-trail",
    "title": "Taking a peek into my hiking data",
    "section": "Aggregated Data by Trail",
    "text": "Aggregated Data by Trail\nIn the aggregate, there seems to be a correlation (r^2 = 0.48) between total distance and total elevation.\n\nggplot() + \n  geom_point(data = hiking_dat_by_trail,\n             aes(x = tot_dist,\n                 y = tot_elev_gain,\n                 color = tot_elev_gain,\n                 size = tot_dist)) +\n  scale_x_continuous(name = \"Total Distance (miles)\") +\n  scale_y_continuous(name = \"Total Elevation (ft)\") +\n  scale_color_viridis(option = \"cividis\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/average-hike/index.html#there-exist-clusters-of-hikes",
    "href": "blog/average-hike/index.html#there-exist-clusters-of-hikes",
    "title": "Taking a peek into my hiking data",
    "section": "There exist ‚Äúclusters‚Äù of hikes",
    "text": "There exist ‚Äúclusters‚Äù of hikes\nI ran a quick cluster analysis to see if I can categorize my hikes in any way. Code is in the Methodology section. Four clusters seemed to be optimal. I have dubbed them:\n\nCluster 1: ‚ÄúLet‚Äôs Get This Over With‚Äù (steep & hard)\nCluster 2: ‚ÄúEasy Peasy Lemon Squeezy‚Äù (short & flat)\nCluster 3: ‚ÄúThe Sweet Spot‚Äù (not too long, not too high)\nCluster 4: ‚ÄúI Don‚Äôt Care About My Knees Anyway‚Äù (too long for my own good)\n\n\nfviz_nbclust(hiking_dat_by_trail[, 4:5], kmeans, method = \"wss\") # finding optimal number of clusters\nk4 <- kmeans(hiking_dat_by_trail[, 4:5], centers = 4, nstart = 25) # calculating clusters\n\nfviz_cluster(k4, data = hiking_dat_by_trail)  +\n  scale_x_continuous(name = \"Scaled Total Distance (miles)\") +\n  scale_y_continuous(name = \"Scaled Total Elevation (ft)\") +\n  scale_color_viridis(option = \"cividis\", discrete = T) +\n  scale_fill_viridis(option = \"cividis\", discrete = T) +\n  theme_minimal()"
  },
  {
    "objectID": "blog/average-hike/index.html#i-dont-particularly-love-long-hikes",
    "href": "blog/average-hike/index.html#i-dont-particularly-love-long-hikes",
    "title": "Taking a peek into my hiking data",
    "section": "I don‚Äôt particularly love long hikes",
    "text": "I don‚Äôt particularly love long hikes\nMy average hike is 6.4 miles - and most of them are concentrated around that distance. This makes sense as I usually day hike and need to get back at a reasonable time. My shortest hike was 1.18 miles and my longest was 17.85 (the Enchantments‚Ä¶). In these 90 hikes, I hiked around 576 miles.\n\nhiking_dat_by_trail %>% \n  ggplot(aes(x = tot_dist)) +\n  geom_histogram(fill = \"#00204c\") +\n  xlab(\"Trail Total Distance (miles)\") +\n  ylab(\"Count\") +\n  scale_fill_viridis(option = \"cividis\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nHistogram of hike total distance\n\n\n\nhiking_dat_by_trail %>% \n  mutate(cumdist = cumsum(tot_dist)) %>% \n  ggplot(aes(x = trail,\n             y = cumdist,\n             fill = cumdist)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis(option = \"cividis\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nCumulative bar plot of distance"
  },
  {
    "objectID": "blog/average-hike/index.html#i-dont-dislike-high-elevation-hikes-though",
    "href": "blog/average-hike/index.html#i-dont-dislike-high-elevation-hikes-though",
    "title": "Taking a peek into my hiking data",
    "section": "I don‚Äôt dislike high elevation hikes though",
    "text": "I don‚Äôt dislike high elevation hikes though\nElevation on these hikes ranged from ~0 feet to 4580 feet gain. I averaged 1455.4 feet gain and have climbed 130,984 feet (~24 miles!).\n\nhiking_dat_by_trail %>% \n  ggplot(aes(x = tot_elev_gain)) +\n  geom_histogram(fill = \"#00204c\") +\n  xlab(\"Trail Total Elevation (ft)\") +\n  ylab(\"Count\") +\n  scale_fill_viridis(option = \"cividis\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\nhiking_dat_by_trail %>% \n  mutate(cumelev = cumsum(tot_elev_gain)) %>% \n  ggplot(aes(x = trail,\n             y = cumelev,\n             fill = cumelev)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis(option = \"cividis\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew blog post üéâ: Taking A Peek into My Hiking Data accessing Google Maps API with googleway ‚õ∞Ô∏èüó∫ https://t.co/2foOUgPKIy #rstats pic.twitter.com/pEevVI9uxN\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 6, 2019"
  },
  {
    "objectID": "blog/leaidr2/index.html",
    "href": "blog/leaidr2/index.html",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "",
    "text": "Piet Mondrian, Tableau No.¬†2/Composition No.¬†VII (1913)"
  },
  {
    "objectID": "blog/leaidr2/index.html#an-update-to-leaidr",
    "href": "blog/leaidr2/index.html#an-update-to-leaidr",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "An update to leaidr",
    "text": "An update to leaidr\nA few months ago, I created {leaidr} for easier download of U.S. school district shapefiles. Daniel Anderson went through and greatly improved the package, making it even easier to download and use the shapefiles (thanks Dan!).\nNow, instead of having to run lea_prep(), you can download the shapefiles from Dan‚Äôs Github repository like so:\n\nlibrary(leaidr)\n\ntn <- lea_get(\"tn\")\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/private/var/folders/pj/nmg9b8_93dq4kwt8nt2d4cj40000gn/T/RtmpqlCmYd/47\", layer: \"47\"\nwith 158 features\nIt has 18 fields\n\ntn %>% \n  sf::st_as_sf() %>% \n  ggplot2::ggplot() +\n  ggplot2::geom_sf()\n\n\n\n\n\n\n\n\nAnd, if you use lea_get() (i.e., with state = NULL), you‚Äôll get the shapefiles for the whole U.S. back via ROpenSci‚Äôs {piggyback}.\nSo much easier and better!\nBecause Dan helped make the package so great, I wanted to add on and showcase what can be done with it! So, today‚Äôs post is jam-packed with choices‚Ä¶\n\nCreate a beautiful map in Mapbox\nCreate a Shiny app with your beautiful map\nAdd a Shiny app to a package"
  },
  {
    "objectID": "blog/leaidr2/index.html#create-a-beautiful-map-in-mapbox",
    "href": "blog/leaidr2/index.html#create-a-beautiful-map-in-mapbox",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "Create a beautiful map in Mapbox",
    "text": "Create a beautiful map in Mapbox\nAsmae Toumi wrote a blog post on how to make maps using R and Mapbox. So, I figured: why not announce the new and improved {leaidr} functions to create a beautiful Mapbox map??\nThis walkthrough will go a little deeper with Mapbox, as I am an extreme beginner and had to do a lot of investigating to figure out how to use it.\nI suggest first reading through Asmae‚Äôs tutorial as there are a few things you need to do before being able to run the below: download the {mapboxapi} package, create a Mapbox account, and install Tippecanoe.\n\nLoad the packages\nHere are the packages you will need:\n\nlibrary(tidyverse)\n\n# remotes::install_github(\"walkerke/mapboxapi\")\nlibrary(mapboxapi)\n\n# if you haven't installed the package yet\n# devtools::install_github(\"ivelasq/leaidr\")\nlibrary(leaidr)\n\nlibrary(rmapshaper)\nlibrary(mapdeck)\n\n\n\nDownload the data\nDownload your shapefiles. If you want to make a choropleth map, also read in the data that you will append to your shapefiles and merge them by a common ID. (Sorry for using a local file!)\n\nshp <- # leaidr shapefiles\n  lea_get(c(\"or\", \"wa\")) %>% \n  sf::st_as_sf()\n\ndat <- # data to append \n  read_csv(\"/Users/shortessay/Downloads/ccd_lea_141_1819_l_1a_091019/ccd_lea_141_1819_l_1a_091019.csv\") %>%\n  filter(ST %in% c(\"OR\", \"WA\"))\n\nnorthwest <-\n  shp %>%\n  select(GEOID, geometry) %>%\n  sp::merge(dat, by.x = \"GEOID\", by.y = \"LEAID\")\n\n\n\nCreate the tileset\nNow, following the original tutorial, we use Tippecanoe to optimize the shapefiles and data and then upload the ‚Äútiles‚Äù to Mapbox.\n\ntippecanoe(\n  input = northwest,\n  output = \"nw.mbtiles\",\n  layer_name = \"northwest\")\n\nupload_tiles(input = \"nw.mbtiles\",\n             username = \"ivelasq3\", \n             tileset_id = \"northwest\",\n             multipart = TRUE)\n\n\n\nStyle the tiles\nThis is the part that I had to figure out on the Mapbox website. This walkthrough was helpful. Once you have uploaded the tiles using upload_tiles(), you should see them available under ‚ÄúCustom tileset‚Äù at the bottom of this webpage: https://studio.mapbox.com/tilesets/.\n\n\n\nMapbox Tilesets\n\n\nThen, go to Styles on this webpage: https://studio.mapbox.com/. Click ‚ÄúNew Style‚Äù and choose the template you want, then Customize.\n\n\n\nMapbox Template\n\n\nTo add your tileset, go to Layers, click the Plus sign, then under ‚ÄúSource‚Äù, find your uploaded tileset, or add the tileset by the ID given by upload_tiles().\n\n\n\nMapbox Layers\n\n\nI zoomed to where my tiles are located (Oregon and Washington) and started editing. This section of the walkthrough explains how to create a choropleth map, where each geography has a different color according to a value.\nOnce done styling, I clicked ‚ÄúPublish‚Äù on the top right of Mapbox Studio.\n\n\nUsing the map in R\nTo get the information to bring it back into R and be able to use the map in a Shiny app, I clicked ‚ÄúShare‚Äù and scrolled to find the Style ID.\n\n\n\nMapbox Sharing\n\n\nI copied the Share URL to include in the function below. For the location parameter, I used the latitude/longitude listed in the browser URL. I played around with the zoom level until I found one I liked.\n\nmapdeck(token = Sys.getenv(\"MAPBOX_PUBLIC_TOKEN\"),\n        style = \"mapbox://styles/ivelasq3/ckehhzzld3l3p19mht1n8hksj\",\n        zoom = 4,\n        location = c(-120.161, 45.843))"
  },
  {
    "objectID": "blog/leaidr2/index.html#create-a-shiny-app-with-your-beautiful-map",
    "href": "blog/leaidr2/index.html#create-a-shiny-app-with-your-beautiful-map",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "Create a Shiny app with your beautiful map",
    "text": "Create a Shiny app with your beautiful map\nOnce you have the mapdeck() function all set up, you can use it in a Shiny app. Here‚Äôs some reference code that I found useful for using renderMapdeck(). Thank you Greg Huang!\nThis is an example of a very bare-bones Shiny app. For the UI, use mapdeckOutput():\n\nlibrary(tidyverse)\nlibrary(shiny)\nlibrary(mapdeck)\n\nui <- fluidPage(\n  mapdeckOutput(outputId = \"createMap\")\n)\n\nAnd for the server, paste the mapdeck() function in renderMapdeck():\n\nserver <- function(input, output) {\n\n  output$createMap <- renderMapdeck({\n    mapdeck(token = Sys.getenv(\"MAPBOX_PUBLIC_TOKEN\"),\n            style = \"mapbox://styles/ivelasq3/ckehhzzld3l3p19mht1n8hksj\",\n            zoom = 4,\n            location = c(-120.161, 45.843))\n  })\n}\n\nI uploaded the app here."
  },
  {
    "objectID": "blog/leaidr2/index.html#add-a-shiny-app-to-a-package",
    "href": "blog/leaidr2/index.html#add-a-shiny-app-to-a-package",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "Add a Shiny app to a package",
    "text": "Add a Shiny app to a package\nNow, say you would like to add the Shiny app to your package as well as upload it to shinyapps.io / instead of uploading to shinyapps.io. Thankfully, Dean Attali has a great walkthrough on how to do this!\n\nAdd {shiny} to your dependencies in your DESCRIPTION file (I do this with usethis::use_package(\"shiny\")).\nCreate a folder called inst in your package with another folder for the Shiny example, and your UI/server file(s) within.\nCreate an R file to run your example (I used usethis::use_r(\"runExample.R\")) to create this file.\nDon‚Äôt forget to document! devtools::document()\n\nSo, if you were to install and load {leaidr}, you can run leaidr::runExample() to launch the Shiny app. To see what the files look like, check out the Github repo files here."
  },
  {
    "objectID": "blog/leaidr2/index.html#conclusion",
    "href": "blog/leaidr2/index.html#conclusion",
    "title": "An even easier-to-use R package for school district shapefiles",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, {leaidr} can help you map your data as long as you have school district LEAID‚Äôs or names in there somewhere. I hope that it helps you in your education data projects!\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew goodies for leaidr, an #rstats pkg for school district shapefiles!ü§© Major upgrade thx to @dataloraxüìì New docs: https://t.co/yOszd3SG7eüó∫Ô∏è Mapbox example thx to @asmae_toumiüåê Shiny app IN package thx to @daattali‚ÑπÔ∏è Walkthrough: https://t.co/i2I121b6a3Happy mapping ü§ì pic.twitter.com/4hIgJ9evoX\n\n‚Äî Isabella Vel√°squez (@ivelasq3) August 31, 2020"
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html",
    "href": "blog/why-disaggregate-data/index.html",
    "title": "Disaggregating school district data",
    "section": "",
    "text": "Samuel Colman, The Rock of Salvation (1837)\nI am a data analyst committed to nonprofit work with the aim of reducing racial and socioeconomic inequities. Data, and in particular disaggregated data, can provide measures of the equity of a system, process, or program. Displaying data by subgroups is an important step in any data team‚Äôs equity journey, and exploring these data beyond averages can highlight the most prominent equity needs.\nIn this post, I define what disaggregated data is and how it can inform teams on their equity work. Then, I run various cuts on Minneapolis Public Schools data. The district‚Äôs reporting of the composition of their schools provides an excellent opportunity to explore inequities in a system.\nAlthough this post heavily references education and student data, the principle of using distributions is paramount in any field that strives to close equity gaps."
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html#what-is-equity",
    "href": "blog/why-disaggregate-data/index.html#what-is-equity",
    "title": "Disaggregating school district data",
    "section": "What is Equity",
    "text": "What is Equity\nInequities are disparities in opportunity, resources, and treatment. Racial and socioeconomic inequities are those that are as a result of one‚Äôs race or socioeconomic status. The unequal distribution of opportunity, resource, and treatment can be due to many factors, including patterns in society. Examples include:\n\nRacial attitudes/bias that act subtly to undermine and exclude,\nContinued redlining in lending,\nEmbedded biases in education (images, language, school discipline),\nLong term ramifications of poor health/healthcare.\n\nBecause the definition of equity varies from team to team, it is important to decide which inequities to identify and focus on."
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html#what-is-disaggregated-data",
    "href": "blog/why-disaggregate-data/index.html#what-is-disaggregated-data",
    "title": "Disaggregating school district data",
    "section": "What is disaggregated data",
    "text": "What is disaggregated data\nAccording to the Glossary of Education Reform, the formal definition of disaggregated data is:\n\nDisaggregated data refers to numerical or non-numerical information that has been (1) collected from multiple sources and/or on multiple measures, variables, or individuals; (2) compiled into aggregate data‚Äîi.e., summaries of data‚Äîtypically for the purposes of public reporting or statistical analysis; and then (3) broken down in component parts or smaller units of data.\n\nAggregate population numbers are broken down into smaller groupings that analysts can compare and contrast. These groupings depend on your team‚Äôs definition of equity, whether it be focused on race, socioeconomic status, race AND socioeconomic status, age, ethnicity, etc."
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html#why-use-disaggregated-data",
    "href": "blog/why-disaggregate-data/index.html#why-use-disaggregated-data",
    "title": "Disaggregating school district data",
    "section": "Why use disaggregated data",
    "text": "Why use disaggregated data\nParticularly in education, disaggregated data is essential in identifying where solutions are needed to solve inequities. Per NCES‚Äô Forum Guide to Collecting and Using Disaggregated Data on Racial/Ethnic Subgroups:\n\nEducators need both high-level data summaries as well as disaggregated data that accurately describe smaller groups of students they serve. Access to and analysis of more detailed data‚Äîthat is, disaggregated data‚Äîcan be a useful tool for improving educational outcomes for small groups of students who otherwise would not be distinguishable in the aggregated data used for federal reporting. Disaggregating student data into subpopulations can help schools and communities plan appropriate programs; decide which interventions to implement; target limited resources; and recognize trends in educational participation, outcomes, and achievement."
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html#minneapolis-public-schools-example",
    "href": "blog/why-disaggregate-data/index.html#minneapolis-public-schools-example",
    "title": "Disaggregating school district data",
    "section": "Minneapolis Public Schools example",
    "text": "Minneapolis Public Schools example\nMinneapolis Public Schools (MPS) reports their student demographics in a robust, complete way. Not only do they report the percentage of students in a subgroup, but they also include the number of students in each subgroup. This allows a deep look into their individual school demographics and gives us the opportunity to explore equity in their district.\n\nPulling data\nMPS publishes their school data in PDFs. Thankfully, the {tabulizer} package exists! It easily and quickly pulled the data into lists which I then transformed to data frames and tidied up.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tabulizer)\nlibrary(janitor)\n\n# Extract tables with race data\nrace_pdf <-\n  extract_tables(\"http://studentaccounting.mpls.k12.mn.us/uploads/racial_ethnic_school_gradefall2017.pdf\")\n\n# Create data frame\nrace_df <- # many thanks to my brother @gvelasq for purrrifying this\n  race_pdf %>%\n  map(as_tibble) %>%\n  map_df(~ slice(., -1:-2)) %>% \n  set_names(c(\"school_group\",\n              \"school_name\",\n              \"grade\", \"na_num\",\n              \"na_pct\",\n              \"aa_num\",\n              \"aa_pct\",\n              \"as_num\",\n              \"as_pct\",\n              \"hi_num\",\n              \"hi_pct\",\n              \"wh_num\",\n              \"wh_pct\",\n              \"pi_pct\",\n              \"blank_col\",\n              \"tot\"))\n\nrace_filter <-\n  race_df %>%\n  select(-school_group, -grade, -pi_pct, -blank_col) %>% # Remove unnecessary or blank columns\n  filter(str_detect(school_name, \"Total\"),\n         school_name != \"Grand Total\") %>% # otherwise totals are duplicated\n  mutate(school_name = str_replace(school_name, \"Total\", \"\"),\n         across(where(is.character), trimws))\n\n# Extract tables with FRPL data\nfrpl_pdf <-\n  extract_tables(\"https://studentaccounting.mpls.k12.mn.us/uploads/free_reduced_meal_fall_2017_2.pdf\")\n\n# Create data frame\nfrpl_df <- # many thanks to my brother @gvelasq for purrrifying this\n  frpl_pdf %>%\n  map(as_tibble) %>%\n  map_df(~ slice(., -1)) %>%  \n  set_names(c(\"school_grades\",\n              \"school_name\",\n              \"total_students\",\n              \"frpl_pct\",\n              \"free_num\",\n              \"reduce_num\",\n              \"not_eligible_num\"))\n\nfrpl_filter <-\n  frpl_df %>% \n  filter(school_name != \"\") %>%\n  select(-school_grades)\n\n# Merged data\nmerged_df <-\n  left_join(race_filter, frpl_filter, by = c(\"school_name\")) %>% \n  mutate(across(2:17, as.numeric)) %>%\n  mutate(frpl_pct = (free_num + reduce_num)/total_students,\n         hi_povnum = case_when(frpl_pct > .75 ~ hi_num),\n         aa_povnum = case_when(frpl_pct > .75 ~ aa_num),\n         wh_povnum = case_when(frpl_pct > .75 ~ wh_num),\n         as_povnum = case_when(frpl_pct > .75 ~ as_num),\n         na_povnum = case_when(frpl_pct > .75 ~ na_num)) %>%\n  adorn_totals() %>%\n  mutate(na_pct = na_num/tot,\n         aa_pct = aa_num/tot,\n         as_pct = as_num/tot,\n         hi_pct = hi_num/tot,\n         wh_pct = wh_num/tot,\n         frpl_pct = (free_num + reduce_num)/total_students, # otherwise total frpl_pct is off\n         hi_povsch = hi_povnum/hi_num[which(school_name == \"Total\")],\n         aa_povsch = aa_povnum/aa_num[which(school_name == \"Total\")],\n         as_povsch = as_povnum/as_num[which(school_name == \"Total\")],\n         wh_povsch = wh_povnum/wh_num[which(school_name == \"Total\")],\n         na_povsch = na_povnum/na_num[which(school_name == \"Total\")])\n\n# Finish up the data frame\ntidy_df <-\n  merged_df %>%\n  pivot_longer(cols = na_num:na_povsch,\n               names_to = \"category\",\n               values_to = \"value\")\n\n\n\nFirst glance: MPS District demographics\nHere is a barplot which shows the percentage of different subgroups in the school district. FRPL stands for Free/Reduced Price Lunch, often used as a proxy for poverty. Students from a household with an income up to 185 percent of the poverty threshold are eligible for free or reduced price lunch.\n\n\n\n\n\n\nWarning\n\n\n\nDefinitions are very important in disaggregated data. FRPL is used because it‚Äôs ubiquitous and reporting is mandated but there is debate as to whether it actually reflects the level of poverty among students.\n\n\n\ntidy_df %>%\n  filter(school_name == \"Total\",\n         str_detect(category, \"pct\")) %>%\n  mutate(category = factor(\n    category,\n    levels = c(\"aa_pct\", \"wh_pct\", \"hi_pct\", \"as_pct\", \"na_pct\", \"frpl_pct\")\n  )) %>%\n  ggplot(aes(x = category, y = value)) +\n  geom_bar(stat = \"identity\", aes(fill = factor(category))) +\n  xlab(\"Subgroup\") +\n  ylab(\"Percentage of Population\") +\n  scale_x_discrete(labels = c(\"Black\", \"White\", \"Hispanic\", \"Asian\", \"Native Am.\", \"FRPL\")) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_iv() +\n  theme_iv() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen we look at these data, MPS looks like a diverse school district. Almost 40% of students are Black and around 35% are White. 60% of the students are eligible for FRPL, which is high for Minnesota but close to the US average of 52.1%.\nHowever, let‚Äôs explore if there‚Äôs more to this story.\n\n\nDiscover distributions\nAnother view of the data can be visualizing the distribution of percentage of a demographic within schools. Here is a histogram for the percentage of White students within the 74 MPS schools for which we have data.\n\nmerged_df %>% \n  filter(school_name != \"Total\") %>% \n  ggplot(aes(x = wh_pct)) +\n  geom_histogram(fill = \"#00C3DA\", breaks= seq(0, 1, by = .1)) +\n  xlab(\"White Percentage\") +\n  ylab(\"Count\") +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  scale_fill_iv() +\n  theme_iv() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n27 of the 74 (36%) of schools have between 0-10% White students. This implies that even though the school district may be diverse, the demographics are not evenly distributed across the schools. More than half of schools enroll fewer than 30% of White students even though White students make up 35% of the district student population.\nThe school race demographics are not representative of the district populations but does that hold for socioeconomic status as well?\n\n\nCreate categories\nHigh-poverty schools are defined as public schools where more than 75% of the students are eligible for FRPL. According to NCES, 24% of public school students attended high-poverty schools. However, different subgroups were overrepresented and underrepresented within the high poverty schools. Is this the case for MPS?\n\ntidy_df %>%\n  filter(school_name == \"Total\",\n         str_detect(category, \"povsch\")) %>% \n  mutate(category = factor(category, levels = c(\"hi_povsch\", \"na_povsch\", \"aa_povsch\", \"as_povsch\", \"wh_povsch\"))) %>%  \n  ggplot(aes(x = category, y = value)) +\n  geom_bar(stat = \"identity\", aes(fill = factor(category))) +\n  xlab(\"Subgroup\") +\n  ylab(\"Percentage in High Poverty Schools\") +\n  scale_x_discrete(labels = c(\"Hispanic\", \"Native Am.\", \"Black\", \"Asian\", \"White\")) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_iv() +\n  theme_iv() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n9% of White students attend high poverty schools, compared to 46% of Black students, 51% of Hispanic students, 46% of Asian students, and 49% of Native American students. These students are disproportionately attending high poverty schools.\n\n\nReveal relationships\nLet‚Äôs explore what happens when we correlate race and FRPL percentage by school.\n\nmerged_df %>% \n  filter(school_name != \"Total\") %>% \n  ggplot(aes(x = wh_pct, y = frpl_pct)) +\n  geom_point(color = \"#00C3DA\") +\n  xlab(\"White Percentage\") +\n  ylab(\"FRPL Percentage\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme_iv() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nSimilarly to the result above, there is a strong negative correlation between FRPL percentage and the percentage of White students in a school. High poverty schools have a lower percentage of White students and low poverty schools have a higher percentage of White students.\n\n\nConclusion\nAccording to the Urban Institute, the disproportionate percentage of students of color attending high poverty schools ‚Äúis a defining feature of almost all Midwestern and northeastern metropolitan school systems.‚Äù Among other issues, high poverty schools tend to lack the educational resources‚Äîlike highly qualified and experienced teachers, low student-teacher ratios, college prerequisite and advanced placement courses, and extracurricular activities‚Äîavailable in low-poverty schools. This has a huge impact on these students and their futures.\nBecause of the disaggregated data Minneapolis Public Schools provides, we can go deeper than the average of demographics across the district and see what it looks like on the school level. These views display that (1) there exists a distribution of race/ethnicity within schools that are not representative of the district, (2) that students of color are overrepresented in high poverty schools, and (3) there is a relationship between the percentage of White students in a school and the percentage of students eligible for FRPL."
  },
  {
    "objectID": "blog/why-disaggregate-data/index.html#examples-of-disaggregated-data-reporting",
    "href": "blog/why-disaggregate-data/index.html#examples-of-disaggregated-data-reporting",
    "title": "Disaggregating school district data",
    "section": "Examples of disaggregated data reporting",
    "text": "Examples of disaggregated data reporting\nThere are so many amazing examples out there using disaggregated data. These two are my favorites:\n\nMoney, Race and Success: How Your School District Compares\nWhy Pinellas County is the worst place in Florida to be black and go to public school\n\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew blog post üéâ: Why Equity Requires Looking Beyond Averages üïµüèΩ‚Äç‚ôÄÔ∏èüìä A Look at Minneapolis Public Schools using the awesome packages #tabulizer üìë and #janitor ‚ú® https://t.co/w4WiuDo0Bl #rstats pic.twitter.com/2MnY3ctFYH\n\n‚Äî Isabella Vel√°squez (@ivelasq3) September 24, 2018"
  },
  {
    "objectID": "blog/modal-district/index.html",
    "href": "blog/modal-district/index.html",
    "title": "Finding the modal school district",
    "section": "",
    "text": "Village by the River, fourth quarter 19th century\nMy good friend Ryan Estrellado recommended an NPR Planet Money podcast episode on the ‚ÄúModal American.‚Äù Working with Ben Casselman, Planet Money explored the most ‚Äòtypical‚Äô American. It was a fantastic, engaging episode about a common statistical technique that I‚Äôve never seen applied in this way before. They ran the numbers on a variety of demographic variables to see what group is the most common in the U.S. (I highly recommend the episode - it‚Äôs pretty surprising!).\nBen Casselman shared the code he ran for the analysis. Ryan suggested running a similar analysis to find the modal school district, which I thought was brilliant! The analysis is below, but if you are desperate for the answer, the modal school district in the U.S. is [drum roll please]: (1) Majority (>=50%) White, (2) Small, (3) Rural, and (4) Medium-Low FRPL (between 25% and 50% FRPL)."
  },
  {
    "objectID": "blog/modal-district/index.html#downloading-and-parsing-data",
    "href": "blog/modal-district/index.html#downloading-and-parsing-data",
    "title": "Finding the modal school district",
    "section": "Downloading and parsing data",
    "text": "Downloading and parsing data\nI used NCES Common Core of Data (CCD) to pull Local Educational Agency (LEA) (a.k.a. school districts) demographic data. I wanted to try Urban Institute‚Äôs {educationdata} R package. They have enrollment and racial demographic data available to download through their package. For Free/Reduced Price Lunch data, I had to pull the data directly from the CCD as it‚Äôs not available via the Urban API.\nWhat buckets do you choose for analysis? This is a bit tricky. I chose buckets I typically refer to for work. I also looked at some NCES reports to see how they typically think about districts. Ultimately, I decided on four buckets: urban-centric locale bucket, race/ethnicity bucket, size bucket, and FRPL bucket.\n\nlibrary(tidyverse)\nlibrary(educationdata)\nlibrary(janitor)\nlibrary(ggalluvial)\n\nknitr::opts_chunk$set(out.width = '100%') \n\n\nUrban-centric locale bucket\nFirst, we download the directory data using the Urban Institute‚Äôs API. These data include the districts and their urban-centric_locale.\n\nccd_dir <- get_education_data(level = 'school-districts',\n                              source = 'ccd', \n                              topic = 'directory', \n                              filters = list(year = 2016),\n                              add_labels = TRUE)\n\nBecause there are so many options for urban-centric locale, let‚Äôs collapse them to just the biggest buckets: City, Town, Rural, and Suburb.\n\nccd_urbanicity <-\n  ccd_dir %>% \n  select(leaid, urban_centric_locale) %>% \n  mutate(leaid = as.character(leaid),\n         geography = case_when(str_detect(urban_centric_locale, \"City|city\") ~ \"City\",\n                               str_detect(urban_centric_locale, \"Town|town\") ~ \"Town\",\n                               str_detect(urban_centric_locale, \"Rural|rural\") ~ \"Rural\",\n                               str_detect(urban_centric_locale, \"Suburb|suburb\") ~ \"Suburb\")) %>% \n  select(-urban_centric_locale)\n\n\n\nSize & race/ethnicity buckets\nThen, we download the enrollment data from the Urban Institute.\n\nccd_enroll <- get_education_data(level = 'school-districts',\n                                 source = 'ccd', \n                                 topic = 'enrollment', \n                                 filters = list(year = 2016,\n                                             grade = 99),\n                                 by = list('race'),\n                                 add_labels = TRUE) \n\nWe want to create two buckets here:\n\nSize: Bucket districts by enrollment size, Small (less than 2500 students), Medium (between 2500 and 10,000 students), or Large (10,000 students or more). I used MDR‚Äôs definition for this.\nRace/Ethnicity: Bucket districts by Majority Non-White (50% or fewer of students are White) or Majority White (more than 50% of students are White).\n\n\nccd_enroll_buckets <-\n  ccd_enroll %>% \n  select(leaid, race, enrollment) %>% \n  spread(race, enrollment, -1) %>%\n  mutate(white_na = case_when(White == 0 ~ NA_real_, # districts with missing data loaded as 0 instead of NA for some reason\n                              TRUE ~ White),\n         pct_white = white_na/Total,\n         enroll_na = case_when(Total < 0 ~ NA_real_, # districts with missing data loaded as 0 instead of NA for some reason\n                               TRUE ~ Total)) %>% \n  mutate(demographics = case_when(pct_white >= .5 ~ \"Majority White\",\n                                  pct_white < .5 ~ \"Majority Non-White\",\n                                  TRUE ~ NA_character_),\n         size = case_when(enroll_na >= 10000 ~ \"Large\",\n                          enroll_na < 10000 & enroll_na >= 2500 ~ \"Medium\",\n                          enroll_na < 2500 ~ \"Small\"))\n\n\n\nFree/reduced price lunch bucket\nNow, we load the FRPL data.\n\nccd_frpl <-\n  read_csv(\"https://media.githubusercontent.com/media/ivelasq/modal_district/master/data/raw/ccd_sch_033_1617_l_2a_11212017.csv\")\n\nWe want Free + Reduced Price Lunch numbers by district. This file also has Free Lunch and Reduced Lunch numbers, and it is school-level. We aggregate up Free + Reduced Price Lunch by district.\n\nccd_frpl_wide <-\n  ccd_frpl %>% \n  clean_names() %>% \n  mutate(leaid = as.character(leaid)) %>% \n  filter(data_group == \"Free and Reduced-price Lunch Table\",\n         lunch_program == \"No Category Codes\") %>% \n  select(leaid, student_count) %>% \n  group_by(leaid) %>% \n  summarise(frpl_student_count = sum(student_count))\n\nThen, we merge with the enrollment data to calculate percent FRPL by district. Our buckets are Low FRPL (less than 25%), Medium-Low FRPL (between 25% and 50% FRPL), Medium-High FRPL (between 50% and 75% FRPL), and High FRPL (greater than 75% FRPL). I originally did not have Medium-Low and Medium High split out but that bucket was so large that I thought this would bring more nuance.\n\nccd_enroll_buckets <-\n  left_join(ccd_enroll_buckets, ccd_frpl_wide) %>% \n  mutate(pct_frpl = frpl_student_count/enroll_na,\n         frpl = case_when(pct_frpl <= .25 ~ \"Low FRPL\",\n                          pct_frpl > .25 & pct_frpl <= .5 ~ \"Medium-Low FRPL\",\n                          pct_frpl > .5 & pct_frpl <= .75 ~ \"Medium-High FRPL\",\n                          pct_frpl > .75 ~ \"High FRPL\"))\n\n\n\nBringing it all together\nFinally, we put all the datasets together. One question raised was what to do with missing data. Some districts, either because of small n-sizes or policy, do not report their demographic or FRPL numbers. In fact, some districts were missing enrollment data altogether! For the purposes of this analysis, I removed any district that was missing data in the FRPL or Race/Ethnicity Buckets. Any data without enrollment data is also excluded.\n\nccd_all <-\n  full_join(ccd_urbanicity, ccd_enroll_buckets, by = \"leaid\") %>% \n  filter(!is.na(frpl),\n         !is.na(demographics))\n\nccd_all <- # reordering factors\n  ccd_all %>% \n  mutate(demographics = factor(demographics, c(\"Majority White\", \"Majority Non-White\")),\n         size = factor(size, c(\"Small\", \"Medium\", \"Large\")),\n         geography = factor(geography, c(\"Rural\", \"Suburb\", \"Town\", \"City\")),\n         frpl = factor(frpl, c(\"Low FRPL\", \"Medium-Low FRPL\", \"Medium-High FRPL\", \"High FRPL\")))\n\n\n\nRunning the analysis\nWe can see what is the modal school district - that is, the combination of our buckets that represents the most districts in the U.S.\nUpdate 2019-09-18: Thanks to the input from Alison Hill on Twitter, instead of the grouper() function in the original analysis I‚Äôm using dplyr::count().\n\n\n\nThis is a great post! I think you don't need the grouper function- I think what you need is dplyr::count(sort = TRUE) (it also does the ungroup for you :) https://t.co/G6ccoQIHFC\n\n‚Äî Alison Presmanes Hill (@apreshill) September 18, 2019\n\n\n\nThe n shows how many districts fit these criteria. Here are the first four but the full table is in the Appendix.\n\nmodal <- \n  ccd_all %>% \n  count(demographics, frpl, geography, size, sort = TRUE)\n\nmodal %>% \n  top_n(4) %>% \n  knitr::kable()\n\n\n\n\ndemographics\nfrpl\ngeography\nsize\nn\n\n\n\n\nMajority White\nMedium-Low FRPL\nRural\nSmall\n2223\n\n\nMajority White\nMedium-High FRPL\nRural\nSmall\n1454\n\n\nMajority White\nLow FRPL\nRural\nSmall\n803\n\n\nMajority Non-White\nHigh FRPL\nCity\nSmall\n576\n\n\n\n\n\nThe modal school district ends up being: Majority White, Rural, Small, Medium-Low FRPL districts. 18% (2484) of the 13,464 districts in our analysis fall within these criteria."
  },
  {
    "objectID": "blog/modal-district/index.html#takeaways",
    "href": "blog/modal-district/index.html#takeaways",
    "title": "Finding the modal school district",
    "section": "Takeaways",
    "text": "Takeaways\n\nSome districts are all alike; every district is different in its own way\nIf you run each bucket individually, you get the same results as all of the groups together (as in, when running only based on size, the result is that Rural districts are the modal districts). This is slightly different from the Modal American analysis from NPR - none of these demographics seem to ‚Äòsplit‚Äô the groups.\n\n\nThis makes sense\n\nThe U.S. is majority White and areas tend to be segregated.\nMajority of districts fall in the Medium FRPL buckets. Majority Non-White districts tend to have higher levels of FRPL, but there are so few of them compared to Majority White Districts, that they do not change the modal district much.\nThe U.S. is very large and districts cannot get too geographically big in order to serve their students. In places with little population density, which is most of the U.S., these districts must remain small.\nSimilarly, there are not that many Cities/Towns compared to Rural districts, probably because you need many districts in Rural areas to serve the spread-out population of students.\n\n\n\nIt‚Äôs really hard to visualize this\nIt was interesting trying to visualize these data, especially since there ended up being 87 (!) combinations. I tried a Sankey chart but it is a bit bananas.\n\nggplot(as.data.frame(modal),\n       aes(y = n,\n           axis1 = demographics, \n           axis2 = geography, \n           axis3 = frpl,\n           axis4 = size)) +\n  stat_alluvium(aes(fill = n),\n                width = 1/6, \n            alpha = 2/3,\n            aes.bind = FALSE) +\n  guides(fill = FALSE) +\n  geom_stratum(width = 1/12, fill = \"black\", color = \"grey\") +\n  geom_label(stat = \"stratum\", label.strata = TRUE, size = 2, hjust = \"inward\") +\n  scale_x_continuous(breaks = 1:4, labels = c(\"Demographics\", \"Geography\", \"FRPL\", \"Size\")) +\n  ggtitle(\"Modal District Sankey Chart\") +\n  theme_minimal()\n\n\n\n\nI tried to visualize the data another way - using bar plots. Again, this gets a bit unruly with all the variables. This leads me to think it‚Äôs better to just look at the table.\n\nccd_all %>% \n  select(leaid, demographics, geography, size, frpl) %>% \n  mutate(count = 1) %>% \n  ggplot(aes(count)) +\n  geom_bar() +\n  facet_grid(demographics + frpl ~ size + geography)"
  },
  {
    "objectID": "blog/modal-district/index.html#next-steps",
    "href": "blog/modal-district/index.html#next-steps",
    "title": "Finding the modal school district",
    "section": "Next steps",
    "text": "Next steps\nIn addition to analyzing more variables (perhaps data that are less likely to correlate), it would be interesting to cut the data using smaller grain size for the buckets. There are probably very interesting nuances between Majority White districts for example - we just need the data (and bandwidth!) to find out.\nI am in awe of social media, which allows us to learn and share in so many ways. My friend retweeted a news article, which prompted the person who wrote the code to share, and then we crafted an idea based on that. I think that‚Äôs pretty great!\nUpdate 2019-09-18: Ben Casselman wrote a reply on how it would be interesting to conduct an analysis on the modal school experience. So exciting! Can‚Äôt wait to continue the analysis (and always looking for collaborators!)\n\n\n\nCurious what this would look like if you weighted by student population ‚Äì in essence, the modal school experience. (More students attend this type of school than any other.) Suspect the results would look quite different.\n\n‚Äî Ben Casselman (@bencasselman) September 16, 2019"
  },
  {
    "objectID": "blog/modal-district/index.html#appendix",
    "href": "blog/modal-district/index.html#appendix",
    "title": "Finding the modal school district",
    "section": "Appendix",
    "text": "Appendix\n\nFull table\n\nccd_all %>% \n  count(demographics, frpl, geography, size) %>% \n  knitr::kable()\n\n\n\n\ndemographics\nfrpl\ngeography\nsize\nn\n\n\n\n\nMajority White\nLow FRPL\nRural\nSmall\n803\n\n\nMajority White\nLow FRPL\nRural\nMedium\n49\n\n\nMajority White\nLow FRPL\nRural\nLarge\n2\n\n\nMajority White\nLow FRPL\nSuburb\nSmall\n459\n\n\nMajority White\nLow FRPL\nSuburb\nMedium\n373\n\n\nMajority White\nLow FRPL\nSuburb\nLarge\n35\n\n\nMajority White\nLow FRPL\nTown\nSmall\n145\n\n\nMajority White\nLow FRPL\nTown\nMedium\n32\n\n\nMajority White\nLow FRPL\nCity\nSmall\n79\n\n\nMajority White\nLow FRPL\nCity\nMedium\n9\n\n\nMajority White\nLow FRPL\nCity\nLarge\n9\n\n\nMajority White\nMedium-Low FRPL\nRural\nSmall\n2223\n\n\nMajority White\nMedium-Low FRPL\nRural\nMedium\n107\n\n\nMajority White\nMedium-Low FRPL\nRural\nLarge\n17\n\n\nMajority White\nMedium-Low FRPL\nSuburb\nSmall\n312\n\n\nMajority White\nMedium-Low FRPL\nSuburb\nMedium\n241\n\n\nMajority White\nMedium-Low FRPL\nSuburb\nLarge\n47\n\n\nMajority White\nMedium-Low FRPL\nTown\nSmall\n545\n\n\nMajority White\nMedium-Low FRPL\nTown\nMedium\n145\n\n\nMajority White\nMedium-Low FRPL\nCity\nSmall\n60\n\n\nMajority White\nMedium-Low FRPL\nCity\nMedium\n37\n\n\nMajority White\nMedium-Low FRPL\nCity\nLarge\n21\n\n\nMajority White\nMedium-High FRPL\nRural\nSmall\n1454\n\n\nMajority White\nMedium-High FRPL\nRural\nMedium\n101\n\n\nMajority White\nMedium-High FRPL\nRural\nLarge\n14\n\n\nMajority White\nMedium-High FRPL\nSuburb\nSmall\n136\n\n\nMajority White\nMedium-High FRPL\nSuburb\nMedium\n68\n\n\nMajority White\nMedium-High FRPL\nSuburb\nLarge\n24\n\n\nMajority White\nMedium-High FRPL\nTown\nSmall\n397\n\n\nMajority White\nMedium-High FRPL\nTown\nMedium\n143\n\n\nMajority White\nMedium-High FRPL\nTown\nLarge\n3\n\n\nMajority White\nMedium-High FRPL\nCity\nSmall\n43\n\n\nMajority White\nMedium-High FRPL\nCity\nMedium\n38\n\n\nMajority White\nMedium-High FRPL\nCity\nLarge\n9\n\n\nMajority White\nHigh FRPL\nRural\nSmall\n303\n\n\nMajority White\nHigh FRPL\nRural\nMedium\n33\n\n\nMajority White\nHigh FRPL\nSuburb\nSmall\n41\n\n\nMajority White\nHigh FRPL\nSuburb\nMedium\n8\n\n\nMajority White\nHigh FRPL\nTown\nSmall\n47\n\n\nMajority White\nHigh FRPL\nTown\nMedium\n14\n\n\nMajority White\nHigh FRPL\nCity\nSmall\n19\n\n\nMajority White\nHigh FRPL\nCity\nMedium\n2\n\n\nMajority Non-White\nLow FRPL\nRural\nSmall\n10\n\n\nMajority Non-White\nLow FRPL\nSuburb\nSmall\n35\n\n\nMajority Non-White\nLow FRPL\nSuburb\nMedium\n16\n\n\nMajority Non-White\nLow FRPL\nSuburb\nLarge\n2\n\n\nMajority Non-White\nLow FRPL\nTown\nSmall\n2\n\n\nMajority Non-White\nLow FRPL\nCity\nSmall\n37\n\n\nMajority Non-White\nLow FRPL\nCity\nLarge\n3\n\n\nMajority Non-White\nMedium-Low FRPL\nRural\nSmall\n23\n\n\nMajority Non-White\nMedium-Low FRPL\nRural\nMedium\n6\n\n\nMajority Non-White\nMedium-Low FRPL\nSuburb\nSmall\n83\n\n\nMajority Non-White\nMedium-Low FRPL\nSuburb\nMedium\n59\n\n\nMajority Non-White\nMedium-Low FRPL\nSuburb\nLarge\n24\n\n\nMajority Non-White\nMedium-Low FRPL\nTown\nSmall\n8\n\n\nMajority Non-White\nMedium-Low FRPL\nTown\nMedium\n4\n\n\nMajority Non-White\nMedium-Low FRPL\nCity\nSmall\n75\n\n\nMajority Non-White\nMedium-Low FRPL\nCity\nMedium\n7\n\n\nMajority Non-White\nMedium-Low FRPL\nCity\nLarge\n6\n\n\nMajority Non-White\nMedium-High FRPL\nRural\nSmall\n209\n\n\nMajority Non-White\nMedium-High FRPL\nRural\nMedium\n20\n\n\nMajority Non-White\nMedium-High FRPL\nRural\nLarge\n8\n\n\nMajority Non-White\nMedium-High FRPL\nSuburb\nSmall\n128\n\n\nMajority Non-White\nMedium-High FRPL\nSuburb\nMedium\n105\n\n\nMajority Non-White\nMedium-High FRPL\nSuburb\nLarge\n24\n\n\nMajority Non-White\nMedium-High FRPL\nTown\nSmall\n95\n\n\nMajority Non-White\nMedium-High FRPL\nTown\nMedium\n49\n\n\nMajority Non-White\nMedium-High FRPL\nCity\nSmall\n207\n\n\nMajority Non-White\nMedium-High FRPL\nCity\nMedium\n43\n\n\nMajority Non-White\nMedium-High FRPL\nCity\nLarge\n74\n\n\nMajority Non-White\nHigh FRPL\nRural\nSmall\n359\n\n\nMajority Non-White\nHigh FRPL\nRural\nMedium\n50\n\n\nMajority Non-White\nHigh FRPL\nRural\nLarge\n5\n\n\nMajority Non-White\nHigh FRPL\nSuburb\nSmall\n159\n\n\nMajority Non-White\nHigh FRPL\nSuburb\nMedium\n49\n\n\nMajority Non-White\nHigh FRPL\nSuburb\nLarge\n16\n\n\nMajority Non-White\nHigh FRPL\nTown\nSmall\n129\n\n\nMajority Non-White\nHigh FRPL\nTown\nMedium\n73\n\n\nMajority Non-White\nHigh FRPL\nTown\nLarge\n6\n\n\nMajority Non-White\nHigh FRPL\nCity\nSmall\n576\n\n\nMajority Non-White\nHigh FRPL\nCity\nMedium\n28\n\n\nMajority Non-White\nHigh FRPL\nCity\nLarge\n39"
  },
  {
    "objectID": "blog/reticulate-data-recreation/index.html",
    "href": "blog/reticulate-data-recreation/index.html",
    "title": "Creating a dataset from an image using reticulate in R Markdown",
    "section": "",
    "text": "Henri Rousseau, Virgin Forest with Sunset (1910)\nLast week, a paper started making the Twitter rounds. Selective attention in hypothesis-driven data analysis by Professors Itai Yanai and Martin Lerche looked into whether providing specific hypotheses prevented students from fully exploring a dataset. The authors artificially created a dataset that, when plotted, clearly showed the outline of a cartoon gorilla.\nI will leave you to read the paper to find out the results but something that interested me was the dataset created from the image. The authors mentioned that they used a Python function called ‚Äògetpixel‚Äô, manipulated the dataset into groups, and plotted it in {ggplot2}. I am learning the {reticulate} package which allows R to interface with Python and thought this would be a fun exercise to try.\nWith that, let‚Äôs recreate this dataset entirely within R Markdown! In addition to being able to copy/paste the code below, I have also provided it as a project on RStudio Cloud if you‚Äôd like to run the whole thing at once."
  },
  {
    "objectID": "blog/reticulate-data-recreation/index.html#install-packages",
    "href": "blog/reticulate-data-recreation/index.html#install-packages",
    "title": "Creating a dataset from an image using reticulate in R Markdown",
    "section": "Install packages",
    "text": "Install packages\nFirst up is installing the R packages:\n```{r}\n# Install packages if not already installed\n# install.packages(c(\"tidyverse\", \"reticulate\"))\nlibrary(tidyverse)\nlibrary(reticulate)\n```\nIn your project, create a folder called ‚Äúimage‚Äù. Save the image you would like to convert to a dataset in that folder. To recreate this paper‚Äôs dataset, go to this page and download the cartoon gorilla.\nThe paper mentions the ‚Äògetpixel‚Äô function. With a bit of digging, we find that is from the package pillow (a fork from a package called PIL). Like in R, we need to install the package. In an R Markdown document, I run the functions from {reticulate} below.\n```{r}\nuse_python(\"/usr/local/bin/python\")\n# If you haven't installed Python, the line below will prompt you!\npy_install(\"pillow\")\n```\nSince {reticulate} is the package that allows you to call Python, if you do not have Python installed, then you will get a message (which you would reply Y to):\nNo non-system installation of Python could be found.\nWould you like to download and install Miniconda?\nMiniconda is an open source environment management system for Python.\nSee https://docs.conda.io/en/latest/miniconda.html for more details."
  },
  {
    "objectID": "blog/reticulate-data-recreation/index.html#create-coordinates",
    "href": "blog/reticulate-data-recreation/index.html#create-coordinates",
    "title": "Creating a dataset from an image using reticulate in R Markdown",
    "section": "Create coordinates",
    "text": "Create coordinates\nThis Stack Overflow thread was very helpful to determine what we need to do with the ‚Äògetpixel‚Äô function in pillow. Again, like in R, we need to call the functions we need but this time, we are doing it in a python chunk.\n```{python}\nimport numpy as np\nfrom PIL import Image\n```\nThen, we point to the image and create coordinates pixels for the outline of the cartoon. We can set a threshold level for which pixels to keep/discard.\n```{python}\n# Thanks to Bart Huntley for pointing out a typo previously in this chunk!\nim = Image.open(\"/cloud/project/image/gorilla.jpg\")\npixels = np.asarray(im)\n\n# Set threshold level\nthreshold_level = 50\n\n# Find coordinates of all pixels below threshold\ncoords = np.column_stack(np.where(pixels < threshold_level))\n```"
  },
  {
    "objectID": "blog/reticulate-data-recreation/index.html#bring-back-into-r",
    "href": "blog/reticulate-data-recreation/index.html#bring-back-into-r",
    "title": "Creating a dataset from an image using reticulate in R Markdown",
    "section": "Bring back into R",
    "text": "Bring back into R\nThis results in a NumPy ndarray called coords that contains the coordinates of the pixels of the outline. That‚Äôs great but‚Ä¶ it‚Äôs in Python! How do we bring it back into R?\nThe {reticulate} Cheat Sheet was very helpful in figuring this out. The section ‚ÄúPython in R Markdown‚Äù shows that you can use the py object to access objects created in Python chunks from R chunks.\n\n\n\n\nPython Cheatsheet\n\n\n\n\nKnowing that, we can create an R object from coords using py. Next, we follow the steps outlined in the paper for the data preparation.\n```{r}\ncoords <-\n  as.data.frame(py$coords) %>%\n  sample_n(1768) %>%\n  mutate(bmi = V2 * 17 + 15,\n         steps = 15000 - V1 * 15000/max(V1)) %>%\n  mutate(randvar = rnorm(n(), mean = 0, sd = 10),\n         randi = steps * (1 + randvar),\n         gender = case_when(randi < median(steps) ~ \"Female\",\n                            TRUE ~ \"Male\"))\n```\nIf we‚Äôd like to see the distribution of data by male/female, we can use count().\n```{r}\ncoords %>%\n  count(gender)\n```"
  },
  {
    "objectID": "blog/reticulate-data-recreation/index.html#create-plots",
    "href": "blog/reticulate-data-recreation/index.html#create-plots",
    "title": "Creating a dataset from an image using reticulate in R Markdown",
    "section": "Create plots",
    "text": "Create plots\nNow for the fun part - visualizing the plots (spoiler: one should always do this before starting an analysis)!\n```{r}\ncoords %>%\n  ggplot(aes(x = bmi,\n              y = steps)) +\n  geom_point() +\n  theme_void() +\n  xlim(0, 15000)\n```\n\n\n\n```{r}\ncoords %>%\n  ggplot(aes(x = bmi,\n             y = steps,\n             color = gender)) +\n  geom_point() +\n  theme_void() +\n  xlim(0, 15000)\n```\n\n\n\nWith that, we‚Äôve seamlessly gone from Python to R and created a dataset leveraging the power of both languages. Thanks to Professors Yanai and Lerche for their publication!\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nDid you see the paper where a dataset was created from a cartoon and thought, how do I create that using #rstats? This blog post walks through using reticulate to use #Python and ggplot2 in the same R markdown notebook üòéüêç https://t.co/4CbXzRd3PN pic.twitter.com/Kq4WsaKHMk\n\n‚Äî Isabella Vel√°squez (@ivelasq3) September 29, 2021"
  },
  {
    "objectID": "blog/you-cran-do-it/index.html",
    "href": "blog/you-cran-do-it/index.html",
    "title": "You CRAN do it",
    "section": "",
    "text": "J. Howard Miller, We Can Do It!\nAs the @WeAreRLadies curator, I asked the Twitterverse for advice when submitting a package to the Comprehensive R Archive Network (CRAN) for the first time. Many people replied and offer their tips, experience, and well wishes. I have summarized everybody‚Äôs replies below. THANK YOU all for participating!\nHere‚Äôs what to expect when succeeding to get your package on CRAN!"
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#read-the-official-cran-documentation",
    "href": "blog/you-cran-do-it/index.html#read-the-official-cran-documentation",
    "title": "You CRAN do it",
    "section": "Read the official CRAN documentation",
    "text": "Read the official CRAN documentation\n‚Ä¶ which is Writing R Extensions! ‚ÄúNecessary and sufficient,‚Äù according to Avraham Adler."
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#you-can-get-by-with-a-little-help-from-your-friends",
    "href": "blog/you-cran-do-it/index.html#you-can-get-by-with-a-little-help-from-your-friends",
    "title": "You CRAN do it",
    "section": "You can get by with a little help from your friends",
    "text": "You can get by with a little help from your friends\n{usethis}, {devtools}, and {roxygen2}, although not necessary, are useful tools for creating CRAN-ready packages."
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#documentation-is-key",
    "href": "blog/you-cran-do-it/index.html#documentation-is-key",
    "title": "You CRAN do it",
    "section": "Documentation is key",
    "text": "Documentation is key\n\nHave tests and coverage via {covr}, and make sure they run without issues on all platforms\nCRAN will ask that you add examples to your package, wrapped in \\dontrun{} and \\donttest{}.\nAsk others read your stuff and provide feedback!"
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#let-reviewers-know-why-this-should-be-on-cran",
    "href": "blog/you-cran-do-it/index.html#let-reviewers-know-why-this-should-be-on-cran",
    "title": "You CRAN do it",
    "section": "Let reviewers know why this should be on CRAN",
    "text": "Let reviewers know why this should be on CRAN\nA quick note letting the reviewers know why your package should be on CRAN goes a long way.\n\n\n\nReviewer asking ‚Äúwhy do we need this on CRAN?‚Äù isn‚Äôt a rejection, just respond politely with a reason. Also, if https://t.co/9Mgm9C20E6 can make it to CRAN, your package belongs there too!\n\n‚Äî Jay Qi (@jayyqi) February 13, 2020"
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#be-patient",
    "href": "blog/you-cran-do-it/index.html#be-patient",
    "title": "You CRAN do it",
    "section": "Be patient",
    "text": "Be patient\nThe people at CRAN are busy! It might take a while for them to reply and accept your package. If you want to track the progress of your package, you can take a look at the CRAN Incoming Dashboard."
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#emails-will-be-terse",
    "href": "blog/you-cran-do-it/index.html#emails-will-be-terse",
    "title": "You CRAN do it",
    "section": "Emails will be terse",
    "text": "Emails will be terse\nThis is particularly difficult for me to hear, as someone who provides! ample! exclamation marks! in emails! in order to sound cheery and polite, but responses can be short and to-the-point. Breathe, and address everything mentioned. When you do reply, respond to the CRAN list."
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#persist",
    "href": "blog/you-cran-do-it/index.html#persist",
    "title": "You CRAN do it",
    "section": "Persist",
    "text": "Persist\nThe reviewers will let you know what you need to do for approval. Go through their list carefully and try again!\n\n\n\nDon‚Äôt get discouraged if you don‚Äôt succeed on your first try. Everyone (including me, my team, and R core) gets rejected from time to time. It sucks, but don‚Äôt give up ‚Äî just make the requested changes and resubmit.\n\n‚Äî Hadley Wickham (@hadleywickham) February 13, 2020\n\n\n\nMany thanks to the R Community for their words of advice and encouragement ‚ù§"
  },
  {
    "objectID": "blog/you-cran-do-it/index.html#guides",
    "href": "blog/you-cran-do-it/index.html#guides",
    "title": "You CRAN do it",
    "section": "Guides",
    "text": "Guides\n\nWriting R Extensions (Official CRAN Documentation)\nHadley Wickham and Jenny Bryan‚Äôs R Packages\nJulia Silge‚Äôs How I Learned to Stop Worrying and Love R CMD Check\nMa√´lle Salmon‚Äôs Code Examples in R Package Manuals\nR-hub‚Äôs Prepare a CRAN submission\nR-hub‚Äôs CRAN submission preparation with rhub\nKarl Broman‚Äôs Getting Your R Package on CRAN\nColin Fay‚Äôs Preparing your package for a CRAN submission\nJp Mar√≠n D√≠az‚Äôs How to publish a package to CRAN and From 0 to CRAN in 1 day\nJim Hester‚Äôs Submitting vroom to CRAN, LIVE!\nRyo Nakagawara‚Äôs CRAN Trial(s) and Error(s) and Future Releases\n\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nüì¢ New blogpost üì¢ You CRAN Do It: What CRAN First-Timers Should Know üí™a summary of a thread from @WeAreRLadies. Thank you for all your advice and encouragement! https://t.co/BimejGvqdr #rstats pic.twitter.com/0Ygk7qHc76\n\n‚Äî Isabella Vel√°squez (@ivelasq3) February 27, 2020"
  },
  {
    "objectID": "blog/tidyverse-python-ports/index.html",
    "href": "blog/tidyverse-python-ports/index.html",
    "title": "Three packages that port the tidyverse to Python",
    "section": "",
    "text": "Johannes Vermeer, The Allegory of Painting (1666)\nAs I‚Äôve been saying every year for the past seven years or so, I am learning Python. (It‚Äôs been a journey.)\nPython packages like pandas have several ways to work with data. There are several options for indexing, slicing, etc. They have a lot of flexibility but also a lot of conventions to remember.\nI am familiar with the grammar of the tidyverse, which provides a consistent set of verbs to solve common data manipulation challenges. I investigated ways to port tidyverse-like verbs to Python (hopefully making Python a little easier to grasp).\nHere are three packages that do just that."
  },
  {
    "objectID": "blog/tidyverse-python-ports/index.html#siuba",
    "href": "blog/tidyverse-python-ports/index.html#siuba",
    "title": "Three packages that port the tidyverse to Python",
    "section": "siuba",
    "text": "siuba\nThe siuba package, created by Michael Chow, allows you to use dplyr-like syntax with pandas. Siuba ports over several functions, including select(), filter(), mutate(), summarize(), and arrange(). The package also allows you to use group_by() and a >> pipe.\nLet‚Äôs check out a few examples using the palmerpenguins dataset (R, Python).\n\nPythonR\n\n\n\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\nfrom siuba import group_by, summarize, _\n\n(penguins\n  >> group_by(_.species)\n  >> summarize(n = _.species.count())\n)\n\n\n\n\n\n  \n    \n      \n      species\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\n\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n(penguins %>% \n    group_by(species) %>%\n    summarize(n = n()))\n# A tibble: 3 √ó 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\nThanks to the documentation and interactive tutorials available for siuba, it‚Äôs easy to see the parallels and differences with dplyr so that you can craft these data manipulation tasks yourself.\n\nPythonR\n\n\n\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\nfrom siuba import select\n\n(penguins\n  >> select(-_.isalpha(), _.species)\n  >> group_by(_.species)\n  >> summarize(\n      bill_length_mm = _.bill_length_mm.mean(),\n      bill_depth_mm = _.bill_depth_mm.mean(),\n      flipper_length_mm = _.flipper_length_mm.mean(),\n      body_mass_g = _.body_mass_g.mean()\n  )\n)\n\n\n\n\n\n  \n    \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n    \n  \n  \n    \n      0\n      Adelie\n      38.791391\n      18.346358\n      189.953642\n      3700.662252\n    \n    \n      1\n      Chinstrap\n      48.833824\n      18.420588\n      195.823529\n      3733.088235\n    \n    \n      2\n      Gentoo\n      47.504878\n      14.982114\n      217.186992\n      5076.016260\n    \n  \n\n\n\n\n\n\n(penguins %>%\n  group_by(species) %>%\n  summarize(across(!where(is.character), mean, na.rm = TRUE)))\n# A tibble: 3 √ó 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>      <dbl>          <dbl>         <dbl>             <dbl>       <dbl>\n1 Adelie        NA           38.8          18.3              190.       3701.\n2 Chinstrap     NA           48.8          18.4              196.       3733.\n3 Gentoo        NA           47.5          15.0              217.       5076.\n# ‚Ä¶ with 2 more variables: sex <dbl>, year <dbl>"
  },
  {
    "objectID": "blog/tidyverse-python-ports/index.html#plotnine",
    "href": "blog/tidyverse-python-ports/index.html#plotnine",
    "title": "Three packages that port the tidyverse to Python",
    "section": "plotnine",
    "text": "plotnine\nThe plotnine package, created by Hassan Kibirige, lets you use a grammar of graphics for Python.\nYou can use siuba and plotnine together, similar to how you would use dplyr and ggplot2 together.\n\nfrom siuba import *\nfrom plotnine import *\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\n\n(penguins\n # using siuba pipe\n >> ggplot(aes(x = 'flipper_length_mm', y = 'body_mass_g'))\n # creating plotnine plot\n  + geom_point(aes(color = 'species', shape = 'species'),\n             size = 3,\n             alpha = 0.8)\n  + theme_minimal()\n  + labs(title = \"Penguin size, Palmer Station LTER\",\n         subtitle = \"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\",\n        x = \"Flipper length (mm)\",\n        y = \"Body mass (g)\",\n        color = \"Penguin species\",\n        shape = \"Penguin species\"))\n\n\n\n\n<ggplot: (314581751)>\n\n\nFolks have heuristics to translate ggplot2 code to plotnine. These help understand the nuances between the two."
  },
  {
    "objectID": "blog/tidyverse-python-ports/index.html#pyjanitor",
    "href": "blog/tidyverse-python-ports/index.html#pyjanitor",
    "title": "Three packages that port the tidyverse to Python",
    "section": "pyjanitor",
    "text": "pyjanitor\nOK, this one is cheating a bit because the janitor package by Sam Firke is not part of the tidyverse. One more package that uses ‚Äòtidyverse-like‚Äô verbs is pyjanitor. With pyjanitor, you can clean column names, identify duplicate entries, and more.\n\nfrom janitor import clean_names\nimport pandas as pd\nimport numpy as np\n\nexample_df = {\n    'Terrible Name 1': ['name1', 'name2', 'name3', 'name4'],\n    'PascalCase': [150.0, 200.0, 300.0, 400.0],\n    'this_has_punctuation?': [np.nan, np.nan, np.nan, np.nan],\n}\n\npd.DataFrame.from_dict(example_df).clean_names()\n\n\n\n\n\n  \n    \n      \n      terrible_name_1\n      pascalcase\n      this_has_punctuation_\n    \n  \n  \n    \n      0\n      name1\n      150.0\n      NaN\n    \n    \n      1\n      name2\n      200.0\n      NaN\n    \n    \n      2\n      name3\n      300.0\n      NaN\n    \n    \n      3\n      name4\n      400.0\n      NaN\n    \n  \n\n\n\n\nPython‚Äôs pandas allows you to method chain with pipes. They can be used with pyjanitor, as well.\n\nfrom janitor import clean_names, remove_empty\nimport pandas as pd\nimport numpy as np\n\nexample_df = {\n    'Terrible Name 1': ['name1', 'name2', 'name3', 'name4'],\n    'PascalCase': [150.0, 200.0, 300.0, 400.0],\n    'this_has_punctuation?': [np.nan, np.nan, np.nan, np.nan],\n}\n\n(pd.DataFrame.from_dict(example_df)\n    .pipe(clean_names)\n    .pipe(remove_empty)\n)\n\n\n\n\n\n  \n    \n      \n      terrible_name_1\n      pascalcase\n    \n  \n  \n    \n      0\n      name1\n      150.0\n    \n    \n      1\n      name2\n      200.0\n    \n    \n      2\n      name3\n      300.0\n    \n    \n      3\n      name4\n      400.0"
  },
  {
    "objectID": "blog/tidyverse-python-ports/index.html#conclusion",
    "href": "blog/tidyverse-python-ports/index.html#conclusion",
    "title": "Three packages that port the tidyverse to Python",
    "section": "Conclusion",
    "text": "Conclusion\nWhile Python syntax and conventions are still on my ‚Äúto-learn‚Äù list, it is helpful to know there are packages that can bring the familiarity of the tidyverse to Python.\n\nAdditional resources\nI came across other packages that port the tidyverse to Python. I didn‚Äôt investigate them deeply but wanted to share in case they are helpful.\n\ntidypandas\nplydata\ntidypolars\ndatar\n\nIf you recommend these packages or know of others, please let me know on Twitter!\n\n\n\nWorking in Python but miss #rstats' tidyverse? üêç Here are three+ packages that port tidyverse-like syntax to use in your #pydata work.Blog post: https://t.co/ScGycjYyOo\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 9, 2022"
  },
  {
    "objectID": "blog/politely-scraping/index.html",
    "href": "blog/politely-scraping/index.html",
    "title": "Politely scraping Wikipedia tables",
    "section": "",
    "text": "Pauline Powell Williams, Violets (1890)\nWikipedia is such an amazing website, and also a fantastic source of data for analysis projects. I often find myself scraping Wikipedia‚Äôs tables for information and then cleaning and using the data for whatever I am working on.\nHowever, scraping a website can create issues if not done properly. Though not required, I like to use the {polite} package to introduce myself to the website and ask for permission to scrape.\nRecently, I submitted the Wikipedia table on independence days around the world to TidyTuesday - resulting in many beautiful and creative visualizations from the R Community! This post walks through how to ‚Äúpolitely‚Äù scrape the table and pull it into a flat data frame so that it‚Äôs ready for use."
  },
  {
    "objectID": "blog/politely-scraping/index.html#load-packages",
    "href": "blog/politely-scraping/index.html#load-packages",
    "title": "Politely scraping Wikipedia tables",
    "section": "Load packages",
    "text": "Load packages\nThere are several packages needed for this walkthrough:\n# To clean data\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n\n# To scrape data\nlibrary(rvest)\nlibrary(httr)\nlibrary(polite)"
  },
  {
    "objectID": "blog/politely-scraping/index.html#scrape-table",
    "href": "blog/politely-scraping/index.html#scrape-table",
    "title": "Politely scraping Wikipedia tables",
    "section": "Scrape table",
    "text": "Scrape table\nFirst, we save the web page with the table that we would like as url:\nurl <- \"https://en.wikipedia.org/wiki/List_of_national_independence_days\"\nNext, we use polite::bow() to introduce ourselves to the host, Wikipedia. This reads the rules from robots.txt and makes sure we follow them. The object (url_bow in this case) is saved as an object of class polite.\nurl_bow <- polite::bow(url)\nurl_bow\nNext, we actually ‚Äòscrape‚Äô (pull the content of) the web page using polite::scrape(). This needs an object of class polite (created with bow() from before).\nSince we politely scraped the entire web page, we want to use {rvest} to specify what exact content we‚Äôd like to pull out. We can do this using html_nodes().\nHow do we know which node we want? There are probably other ways of doing this. As a Firefox and Mac user, I click Cmd + Shift + C which opens up the Developer Tools so that I can select a specific element from the web page. I hover over the table to determine what the HTML is called, in this case, table.wikitable.\n\n\n\nThis object is saved as an HTML table which is great, but a data frame would be preferable for analysis. So the final step is to use rvest::html_table() to read this table as something with which we can use tidyverse tools. The parameter fill = TRUE allows you to fill empty rows with NA.\nind_html <-\n  polite::scrape(url_bow) %>%  # scrape web page\n  rvest::html_nodes(\"table.wikitable\") %>% # pull out specific table\n  rvest::html_table(fill = TRUE)"
  },
  {
    "objectID": "blog/politely-scraping/index.html#flatten-table",
    "href": "blog/politely-scraping/index.html#flatten-table",
    "title": "Politely scraping Wikipedia tables",
    "section": "Flatten table",
    "text": "Flatten table\nYou will notice that ind_html is saved as a single object (a list) in which each element is a data frame. If we want to convert it to a flat data frame, we can specify that we want the content from only the first element [[1]]. We can then use janitor::clean_names() for nice, standardized column names.\nind_tab <- \n  ind_html[[1]] %>% \n  clean_names()\nThat‚Äôs it! Now we‚Äôve ‚Äúpolitely‚Äù scraped the Wikipedia table into an analysis-ready data frame."
  },
  {
    "objectID": "blog/politely-scraping/index.html#conclusion",
    "href": "blog/politely-scraping/index.html#conclusion",
    "title": "Politely scraping Wikipedia tables",
    "section": "Conclusion",
    "text": "Conclusion\nAdditional steps to clean the file can be found in my GitHub repo. After doing so, I submitted to #TidyTuesday by submitting an issue on their page. Then they approved and shared the dataset!\nThis was a very quick walkthrough. I recommend Ryo Nakagawara‚Äôs blog post on politely scraping websites, especially if you would like (1) more in-depth explanations of what {polite} does and (2) more complex scraping examples.\nWhich Wikipedia table will you analyze next?\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nüì¢ A short blog post on how to \"politely\" scrape a Wikipedia table using #rstats. Which page will you scrape next?!https://t.co/cTIy8UIFKm\n\n‚Äî Isabella Vel√°squez (@ivelasq3) July 27, 2021"
  },
  {
    "objectID": "blog/things-i-google/index.html",
    "href": "blog/things-i-google/index.html",
    "title": "Six things I always Google when using ggplot2",
    "section": "",
    "text": "Henri Edmond Cross, Two Women by the Shore, Mediterranean (1896)\nI often use {ggplot2} to create graphs but there are certain things I always have to Google. I figured I‚Äôd create a post for quick reference for myself but I‚Äôd love to hear what you always have to look up!\nTo showcase what‚Äôs happening, I am going to use a TidyTuesday dataset: Spotify songs! Let‚Äôs start by creating a simple graph."
  },
  {
    "objectID": "blog/things-i-google/index.html#remove-the-legend",
    "href": "blog/things-i-google/index.html#remove-the-legend",
    "title": "Six things I always Google when using ggplot2",
    "section": "Remove the legend",
    "text": "Remove the legend\ntheme(legend.position = \"none\")\nAhh‚Ä¶ this one always gets me. Sometimes when your color is mostly just for aesthetics, it doesn‚Äôt make sense to also have a color legend. This removes the legend and makes the graph look cleaner.\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/things-i-google/index.html#change-legend-title-and-labels",
    "href": "blog/things-i-google/index.html#change-legend-title-and-labels",
    "title": "Six things I always Google when using ggplot2",
    "section": "Change legend title and labels",
    "text": "Change legend title and labels\nscale_fill_discrete(name = \"New Legend Title\", labels = c(\"lab1\" = \"Label 1\", \"lab2\" = \"Label 2\"))\nAlright, say I do want the legend. How do I make it something readable?\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_discrete(name = \"Playlist Genre\", \n                      labels = c(\"edm\" = \"EDM\", \n                                 \"latin\" = \"Latin\", \n                                 \"pop\" = \"Pop\", \n                                 \"r&b\" = \"R&B\", \n                                 \"rap\" = \"Rap\", \n                                 \"rock\" = \"Rock\"))"
  },
  {
    "objectID": "blog/things-i-google/index.html#manually-change-colors",
    "href": "blog/things-i-google/index.html#manually-change-colors",
    "title": "Six things I always Google when using ggplot2",
    "section": "Manually change colors",
    "text": "Manually change colors\nscale_fill_manual(\"New Legend Title\", values = c(\"lab1\" = \"#000000\", \"lab2\" = \"#FFFFFF\"))\nThis is a bit trickier, in that you cannot use scale_fill_manual and scale_fill_discrete separately on the same plot as they override each other. However, if you want to change the labels and the colors together, you can use scale_fill_manual like below.\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_manual(name = \"Playlist Genre\", \n                    labels = c(\"edm\" = \"EDM\", \n                               \"latin\" = \"Latin\", \n                               \"pop\" = \"Pop\", \n                               \"r&b\" = \"R&B\", \n                               \"rap\" = \"Rap\", \n                               \"rock\" = \"Rock\"),\n                    values = c(\"edm\" = \"#0081e8\", \n                               \"latin\" = \"#9597f0\", \n                               \"pop\" = \"#d4b4f6\", \n                               \"r&b\" = \"#ffd6ff\", \n                               \"rap\" = \"#ffa1d4\", \n                               \"rock\" = \"#ff688c\"))"
  },
  {
    "objectID": "blog/things-i-google/index.html#remove-x-axis-labels",
    "href": "blog/things-i-google/index.html#remove-x-axis-labels",
    "title": "Six things I always Google when using ggplot2",
    "section": "Remove x-axis labels",
    "text": "Remove x-axis labels\ntheme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank())\nIn this case, since we have a legend, we don‚Äôt need any x axis labels. Sometimes I use this if there‚Äôs redundant information or if it otherwise makes the graph look cleaner.\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_manual(name = \"Playlist Genre\", \n                    labels = c(\"edm\" = \"EDM\", \n                               \"latin\" = \"Latin\", \n                               \"pop\" = \"Pop\", \n                               \"r&b\" = \"R&B\", \n                               \"rap\" = \"Rap\", \n                               \"rock\" = \"Rock\"),\n                    values = c(\"edm\" = \"#0081e8\", \n                               \"latin\" = \"#9597f0\", \n                               \"pop\" = \"#d4b4f6\", \n                               \"r&b\" = \"#ffd6ff\", \n                               \"rap\" = \"#ffa1d4\", \n                               \"rock\" = \"#ff688c\")) +\n  theme(axis.title.x = element_blank(),\n         axis.text.x = element_blank(),\n         axis.ticks.x = element_blank())"
  },
  {
    "objectID": "blog/things-i-google/index.html#start-the-y-axis-at-a-specific-number",
    "href": "blog/things-i-google/index.html#start-the-y-axis-at-a-specific-number",
    "title": "Six things I always Google when using ggplot2",
    "section": "Start the y-axis at a specific number",
    "text": "Start the y-axis at a specific number\nscale_y_continuous(name = \"New Y Axis Title\", limits = c(0, 1000000))\nOften times, we want our graph‚Äôs y axis to start at 0. In this example it already does, but this handy parameter allows us to set exactly what we want our y axis to be.\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_manual(name = \"Playlist Genre\", \n                    labels = c(\"edm\" = \"EDM\", \n                               \"latin\" = \"Latin\", \n                               \"pop\" = \"Pop\", \n                               \"r&b\" = \"R&B\", \n                               \"rap\" = \"Rap\", \n                               \"rock\" = \"Rock\"),\n                    values = c(\"edm\" = \"#0081e8\", \n                               \"latin\" = \"#9597f0\", \n                               \"pop\" = \"#d4b4f6\", \n                               \"r&b\" = \"#ffd6ff\", \n                               \"rap\" = \"#ffa1d4\", \n                               \"rock\" = \"#ff688c\")) +\n  theme(axis.title.x = element_blank(),\n         axis.text.x = element_blank(),\n         axis.ticks.x = element_blank()) +\n  scale_y_continuous(name = \"Count\", limits = c(0, 10000))"
  },
  {
    "objectID": "blog/things-i-google/index.html#use-scales-on-the-y-axis",
    "href": "blog/things-i-google/index.html#use-scales-on-the-y-axis",
    "title": "Six things I always Google when using ggplot2",
    "section": "Use scales on the y-axis",
    "text": "Use scales on the y-axis\nscale_y_continuous(label = scales::format)\nDepending on our data, we may want the y axis to be formatted a certain way (using dollar signs, commas, percentage signs, etc.). The handy {scales} package allows us to do that easily.\n\nspotify_songs %>% \n  ggplot(aes(x = playlist_genre, fill = playlist_genre)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_manual(name = \"Playlist Genre\", \n                    labels = c(\"edm\" = \"EDM\", \n                               \"latin\" = \"Latin\", \n                               \"pop\" = \"Pop\", \n                               \"r&b\" = \"R&B\", \n                               \"rap\" = \"Rap\", \n                               \"rock\" = \"Rock\"),\n                    values = c(\"edm\" = \"#0081e8\", \n                               \"latin\" = \"#9597f0\", \n                               \"pop\" = \"#d4b4f6\", \n                               \"r&b\" = \"#ffd6ff\", \n                               \"rap\" = \"#ffa1d4\", \n                               \"rock\" = \"#ff688c\")) +\n  theme(axis.title.x = element_blank(),\n         axis.text.x = element_blank(),\n         axis.ticks.x = element_blank()) +\n  scale_y_continuous(name = \"Count\", limits = c(0, 10000),\n                     labels = scales::comma)\n\n\n\n\nThere we have it! Six things I always eventually end up Googling when I am making plots using {ggplot2}. Hopefully now I can just look at this page instead of searching each and every time!\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nI wrote a quick #rstats blogpost: \"Six Things I Always Google When Using ggplot2\" üîé üìä What do you always have to look up when creating your #ggplot2 graphs? ü§îü§î https://t.co/jEOR3RDDIh\n\n‚Äî Isabella Vel√°squez (@ivelasq3) January 28, 2020"
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html",
    "href": "blog/bookdown-manuscript/index.html",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "",
    "text": "Winslow Homer, Maine Coast (1896)\nMy co-authors Ryan, Emily, Jesse, Josh, and I published our book last year, but Data Science in Education Using R is also available online as a bookdown on a website, available to all.\n{bookdown} is an R package that facilitates writing books and long-form articles/reports with R Markdown. R Markdown is a file format that allows you to write using markdown, a language for formatted text, and integrate R code, analyses, and visualizations. By using R Markdown, my co-authors and I were able to collaborate using familiar scripts, share and run code, and edit our manuscript for publication. By using {bookdown}, we were able to publish these R Markdown files to the web so people could access our book as we were writing it.\nWe have written a few times about the open and collaborative nature of this work. This article will walk through the more technical aspects of using {bookdown} from our beginners‚Äô standpoint. We hope it highlights the lessons we learned along the way. To peek at the actual files of our book, check out the GitHub repo.\nSo without further ado, here are some tips about {bookdown} that may be helpful on your book manuscript creation:"
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#learning-r-markdown-and-bookdown",
    "href": "blog/bookdown-manuscript/index.html#learning-r-markdown-and-bookdown",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Learning R Markdown and {bookdown}",
    "text": "Learning R Markdown and {bookdown}\nThe first place to start to learn {bookdown} is Yihui Xie‚Äôs bookdown: Authoring Books and Technical Documents with R Markdown. In addition to going into {bookdown}, it provides some information on R Markdown and how to use it as well.\nIf you are new to R Markdown, Yihui, J. J. Allaire, and Garrett Grolemund wrote a definitive guide, available here.\nThese books offer great guidance on {bookdown} and R Markdown, but below are some details if you want to get started with a minimal example of a bookdown.\n\nOK, you installed {bookdown}. Now what?\nYou may notice that a lot of tutorials have you start using {bookdown} with a demo project. This is certainly one way to start - exploring the demo project and changing things to see how your book changes when it‚Äôs re-rendered. As far as I know, there is no standard template to begin a bookdown. The bare minimum of what you need is an index.Rmd file, your other R Markdown files, a _bookdown.yml file, and _output.yml.\n\nindex.Rmd\nIn your project folder, start with an index.Rmd file with title, author, site, and output in the YAML header of the file (the YAML header is the top part of the R Markdown file, see here between the ---).\n---\ntitle: \"Test Book\"\nauthor: \"Me\"\nsite: bookdown::bookdown_site\noutput: bookdown::gitbook\n---\nBelow the YAML header, you can use regular R Markdown syntax. Start it off with a first-level header. If you do not want it to be numbered, write {-} beside the header.\n# Welcome {-}\nThis will be the first page of your rendered book.\n\n\nOther R Markdown files\nBesides index.Rmd, other R Markdown files will make up the chapters of your book. By default, bookdown merges all Rmd files by the order of filenames, e.g., 01-intro.Rmd will appear before 02-literature.Rmd. As referenced in Yihui‚Äôs book, we named our files with a number first (see here].\nSome tips:\nWe recommend each R Markdown file have only one first-level header; otherwise, your chapters will be cut off at any first-level header if you use the K-M rendering approach (see here). Do not use the same first-level header in the same bookdown!\n\n\n_bookdown.yml\nHey, did you notice there‚Äôs an underscore in the file name? I didn‚Äôt and spent an hour trying to figure out why the book wouldn‚Äôt render. So another tip, underscores are important :)\nThe _bookdown.yml file does not need much. Critical is the book_filename - we used main.Rmd. Here, you also specify the rendering method you would like (mentioned above), and also whether you want to delete the merged file (we recommend yes).\nbook_filename: \"bookdown-demo\"\ndelete_merged_file: true\nnew_session: yes\nBesides this, you can add labels to figures and chapters, add scripts to run before each R Markdown file, change the order of your chapters manually (remember, by default it will be index.Rmd then by alphabetical order), and a lot more - but all of that is optional.\n\n\n_output.yml\nOh heyy, another underscore!\nThis file will output your file according to what is specified. It‚Äôs also how figure height/width is specified, allows you to specify the TOC, etc. But really, if you want a bookdown all it needs is to know what you want your output to be.\nbookdown::gitbook\n\n\n\nRendering the book\nOKAY! So you have your files and the bare minimum of what you need to create a bookdown. To see it, go ahead and run:\nbookdown::serve_book()\nSo now you have a bookdown! And you eagerly look at your accomplishment and it is great. But how do we find out what else we can do with {bookdown}?"
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#snooping-on-others-github-files",
    "href": "blog/bookdown-manuscript/index.html#snooping-on-others-github-files",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Snooping on others‚Äô GitHub files",
    "text": "Snooping on others‚Äô GitHub files\nWhen I do not know how to do something in R, I look at people who I want to emulate and snoop on their GitHub repository pages. It is so incredibly handy (and wonderful) that others post their code for all to see.\nThis is how we learned just how to even start using bookdown (copying people‚Äôs files, seeing what happened when we ran them), but also the creative ways that others went about customizing their books.\nWhile there‚Äôs an archive of many books using {bookdown}, here are some specific works we used during our bookdown development:\n\nForecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos (book, repo) for customizing CSS on a bookdown with custom fonts, colors, etc.\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow (book, repo) to learn how to give figures labels.\n\nWe highly recommend seeing what others have done in the past and trying things out so your book reflects what you would like it to look like!\n\nCustomizing CSS\nSpeaking of CSS, this is a nice way to make your bookdown your own. Once you add a css/style.css file in your bookdown repository, you can add css: css/style.css in your _output.yml file. Then, anytime your book renders, it will reference your CSS file. While not explicitly necessary for a manuscript (at least not ours), it is nice to customize one‚Äôs work.\n\nWait, what if I don‚Äôt know CSS?\nDo not fret, reader. I also do not know CSS. But thankfully, the Google Developer Tool exists.\nBy right-clicking any part of a web page (including an existing bookdown) in Chrome, you can then click ‚ÄúInspect Element‚Äù to see its HTML file and the hierarchy of the thing you just right-clicked on. Then, by clicking the arrow on the top left of your Inspector pane, you can click on anything to see its HTML file.\nThis, and a combination of snooping on GitHub, allowed us to find ways to customize our bookdown. Here is where ours is located and where you can see what is referenced and changed with CSS. For example, this bit of code is saying for the book left-hand pane, when hovering, make the color white and the text #ffbc49.\n.book .book-summary ul.summary li a:hover {\n  background-color: #ffffff;\n  color: #ffbc49;\n}\nThis process takes a lot of trial and error, I will admit. It may be easier to pick up a bit of CSS if you have specific ideas in mind. However, if you need a quick edit of some pages, seeing how others have done it and exploring with the Developer Tools is very handy."
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#caching-code-chunks",
    "href": "blog/bookdown-manuscript/index.html#caching-code-chunks",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Caching code chunks",
    "text": "Caching code chunks\nAs mentioned in Yihui‚Äôs book, you can cache certain code chunks so they do not have to rerun each time you render the book. We found this very handy, as some complex models took some time to run.\nImportant to note though: if you use set.seed, you will need to include it in every code chunk; setting it in only one code chunk will not mean it is set for all code chunks."
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#figuring-out-your-publishers-requirements",
    "href": "blog/bookdown-manuscript/index.html#figuring-out-your-publishers-requirements",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Figuring out your publisher‚Äôs requirements",
    "text": "Figuring out your publisher‚Äôs requirements\nCreating a book using R/R Studio/{bookdown} is great because all of these tools are so versatile and we were able to use each one to meet our publisher requirements, minimize manual work, and reduce redundancies.\nFor example, here‚Äôs how {bookdown} helped us do what we had to do:\n\nOur manuscript had to be in Word: we were able to render the bookdown using Word.\nOur manuscript needed all tables and figures to be labeled: we were able to automate this using the .yml files.\nOur figures had to be of a certain size and dimension: we were also able to automate this using the .yml files.\nWe needed to send all of our figures separately from the manuscript: they are all saved separately due to the rendering function of {bookdown}.\nWe needed a bibliography: we were able to create a bibliography of works and packages, all automated with {bookdown}.\n\nBecause of R:\n\nAll tables had to be in Word format: at the time, we hadn‚Äôt seen this from {gtsummary}, and we spent quite a bit of time trying to use {gt} for our tables before realizing that we wouldn‚Äôt be able to output using Word. We then switched to {sjPlot}, which got us exactly what we needed.\n\n\n\n\n\ngtsummary Reference\n\n\n\n\nOur bibliography had to be in APA format: thanks to a file that Josh had (saved here), all we had to do was save that file and add biblio-style: \"apalike\" to our index.Rmd file for everything to show up in the format we needed.\n\nAnd, thanks to R Studio:\n\nWe were able to spell-check files within the IDE!\nWe used its handy search function to create the index.\n\nSo, we recommend gathering all of your publisher‚Äôs requirements and then mapping them to the different tools at your disposal. This saves a lot of time and headache when it‚Äôs finally time to send over the final manuscript."
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#determining-how-to-collaborate",
    "href": "blog/bookdown-manuscript/index.html#determining-how-to-collaborate",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Determining how to collaborate",
    "text": "Determining how to collaborate\nSince my co-authors and I all knew GitHub, we decided to use GitHub to write the book. This was great because it was a transparent way of writing the book (non-authors added issues and contributed as they saw fit). We were able to go back in time if needed to fix errors or see what had been changed.\nWe had to decide how updating the book would work. Ultimately, we decided that one person would be in charge of rendering the book - this helped reduce any code conflicts. However, there may be other ways of working that would be better for you."
  },
  {
    "objectID": "blog/bookdown-manuscript/index.html#conclusion",
    "href": "blog/bookdown-manuscript/index.html#conclusion",
    "title": "Preparing a manuscript for publication using bookdown",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we chose to use {bookdown} to create a book manuscript because we wanted to be transparent in our work, leverage R and its many tools, and be as efficient as possible when writing a book across five people. Despite some technical lift (and a lot of trial and error), it eventually resulted in a smooth workflow for our group. Thanks to the many resources and code available out there, we were able to learn {bookdown} as we wrote. Thanks to {bookdown}, we output the final manuscript that resulted in our book!\n\n\n\nNew blogpost! üéâ We created our book's manuscript using #rstats and #bookdown! How did we learn how to do it? #1 tip: snoop on other people's GitHub repos üëÄ Read more here: https://t.co/jSozoz86gj\n\n‚Äî Isabella Vel√°squez (@ivelasq3) March 9, 2021"
  },
  {
    "objectID": "blog/tidying-census-data/index.html",
    "href": "blog/tidying-census-data/index.html",
    "title": "What it takes to tidy Census data",
    "section": "",
    "text": "Hendrick Avercamp, Ice-skating in a Village (1610)\nThe U.S. Census releases aggregate data on their Household Pulse Survey. These data are valuable and cover a range of important topics, particularly those related to the COVID-19 pandemic.\nFirst of all, let me clarify that I think that the work that the Census does is amazing and I am so glad that these data are available. But, when you download the data, you will see that it is a highly stylized Excel spreadsheet. There may be upsides for those who want to see the data quickly and easily. As an R user though, seeing all those merged cells, non-numeric numerics, and category names in rows makes me feel emo::ji('unamused').\nHowever, this is not terribly surprising (and with public data, somewhat expected). As stated in the tidy data paper:\nThankfully, we have the very powerful R and tidyverse available to address our data woes. Let‚Äôs go through the process of tidying these data with tidyverse packages to show how easily they can become ready for analysis!"
  },
  {
    "objectID": "blog/tidying-census-data/index.html#loading-the-data",
    "href": "blog/tidying-census-data/index.html#loading-the-data",
    "title": "What it takes to tidy Census data",
    "section": "Loading the data",
    "text": "Loading the data\nAs usual, we begin by loading our packages.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\n\nWe have the option to download the Excel file and load it in R. But what if we want to load the data directly from the website? We can use {httr}! The following code ‚Äògets‚Äô the file from the internet, writes it in a temporary file path, and loads it in an object called path.\n\nMany thanks to Liz Potamites for pointing out: if the below doesn‚Äôt work, it may be that the link is changed or broken. It should be Table 2 from the second week of the Household Pulse Survey, which as of July 21, 2020, is located here.\n\n\nGET(\"https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk2/educ2_week2.xlsx\", write_disk(path <- tempfile(fileext = \".xlsx\")))\n\nResponse [https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk2/educ2_week2.xlsx]\n  Date: 2022-03-13 15:45\n  Status: 200\n  Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n  Size: 442 kB\n<ON DISK>  /var/folders/pj/nmg9b8_93dq4kwt8nt2d4cj40000gn/T//RtmpRY4m58/file2eca17a48009.xlsx"
  },
  {
    "objectID": "blog/tidying-census-data/index.html#cleaning-the-data",
    "href": "blog/tidying-census-data/index.html#cleaning-the-data",
    "title": "What it takes to tidy Census data",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nAs mentioned in the figure above, each sheet comprises a state‚Äôs data. It‚Äôd be good to have all of the data in one single data structure. One option is to try to force all of the sheets together at once in a data frame (which is a 2D structure). But we also saw that each sheet requires a lot of cleaning before it can be useful, and it may be difficult to clean if they‚Äôre all merged in one data frame. Therefore, let‚Äôs instead first read in all the data as a list (which is a higher dimension structure), clean it up, and then put it together in a data frame.\nHowever, I am not very good at thinking about things in a list format and it‚Äôs a little harder to see what‚Äôs going on compared to looking at a data frame using View(). Before I clean up a list, I usually work on a single cut of the list as a data frame to know what exactly I am going to do. Thankfully, all the sheets in this Excel sheet are formatted the same across states. This isn‚Äôt always the case! Because they are identically formatted, we know if our processing works on one sheet, it will work across all of them.\nLet‚Äôs look at a single sheet!\n\nSingle sheet cleaning\n\ncensus_sheet1 <-\n  read_excel(path, sheet = 1)\n\nView(census_sheet1)\n\n\n\n\nFirst three columns of the imported dataset\n\n\nImmediately, we see that the top lines are superfluous rows (the headers from the original dataset). We can use skip in read_excel() to not have them read in.\n\ncensus_sheet1 <-\n  read_excel(path, sheet = 1, skip = 3)\n\n# View(census_sheet1)\n\n\n\n\nDataset with skipped lines\n\n\nNow that the unnecessary rows are gone, we see that the column names aren‚Äôt reading in super well because of the merged cells in the original sheet. In this case, we manually create a vector of the column names and replace the old ones with set_names().\n\nnew_names <-\n  c(\"select_characteristics\", \"total\", \"using_online_resources\", \"using_paper_materials_sent_home\", \"where_classes_were_cancelled\", \"where_classes_changed_in_another_way\", \"where_no_change_to_classes\", \"did_not_respond\")\n\ncensus_example <-\n  census_sheet1 %>% \n  set_names(new_names)\n\n\n\n\nDataset with cleaned names\n\n\nWe still have some empty rows (and also a row at the very bottom which is a note in the original dataset). We can eliminate these rows using slice(). Here, we‚Äôre saying to ‚Äòslice‚Äô rows 1 through 3 and 60.\n\ncensus_example <-\n  census_example %>% \n  slice(-1:-3, -60:-61)\n\n\n\n\nDataset with removed rows\n\n\nNow to deal with the fact that the category names are embedded within the first column select_characteristics. There may be other ways to do this, but again I manually create a vector with all the values that I want to get rid of and use filter() to keep only the rows that do not contain the items in the vector.\n\nfilter_var <- \n  c(\"Age\", \"Sex\", \"Hispanic origin and Race\", \"Education\", \"Marital status\", \"Presence of children under 18 years old\", \"Respondent or household member experienced loss of employment income\", \"Mean weekly hours spent on‚Ä¶\", \"Respondent currently employed\", \"Food sufficiency for households prior to March 13, 2020\", \"Household income\")\n\ncensus_example <-\n  census_example %>% \n  filter(!select_characteristics %in% filter_var) \n\n\n\n\nDataset with filtered rows\n\n\nEven though we removed the characteristic names from the rows, they contain very useful information. Also, we run into an issue in which two of the characteristic categories had the same options (‚Äúyes‚Äù and ‚Äúno‚Äù). If we don‚Äôt address this, we‚Äôll forget which rows are for which characteristic. To fix this, we manually create a column with the characteristics for each of the response options and append it to the data.\n\ncategory_column <-\n  c(\"age\", \"age\", \"age\", \"age\", \"age\", \"sex\", \"sex\", \"race\", \"race\", \"race\", \"race\", \"race\", \"education\", \"education\", \"education\", \"education\", \"marital_status\", \"marital_status\", \"marital_status\", \"marital_status\", \"marital_status\", \"children\", \"children\", \"loss_employment\", \"loss_employment\", \"loss_employment\", \"hours_spent\", \"hours_spent\", \"employed\", \"employed\", \"employed\", \"food_sufficiency\", \"food_sufficiency\", \"food_sufficiency\", \"food_sufficiency\", \"food_sufficiency\", \"income\", \"income\", \"income\", \"income\", \"income\", \"income\", \"income\", \"income\", \"income\")\n\ncensus_example <-\n  census_example %>% \n  add_column(category_column)\n\n\n\n\n\nColumn with variable names\n\n\n\nFinally, you may have noticed that some of the rows did not read in as numbers but as characters.\n\n\n\n\nColumn with numbers but with character type\n\n\n\nWe can use mutate_at() and specify which variables we want to be numeric.\n\ncensus_example <-\n  census_example %>% \n  mutate_at(vars(total, using_online_resources:did_not_respond), list(~ as.numeric(.)))\n\nHooray - now we have a tidy dataset we could use for analysis! Which is great, but it‚Äôs only one sheet. How do we do this for the additional 66?\n\n\nMulti-sheet cleaning\nWe will now download the data and store it in a list, where each sheet (which represents a state) is saved as a tibble within the list. To work across all the lists, we use the tidyverse package {purrr} and its handy functions.\nYou may notice that the multi-sheet cleaning looks a lot like the single sheet cleaning but everything is wrapped in the function map(). That‚Äôs true! The wonderful thing about {purrr} being in the tidyverse is that it‚Äôs really easy to integrate with all the tidyverse functions.\nReading the data into one list is slightly more complicated than reading in a single sheet. We begin with the file path from before and then use excel_sheets() to create a vector of the sheet names. set_names() ensures that we have a named list that contains the state names, which will be important later. If we don‚Äôt use set_names(), then the tibbles have generic names instead of ‚ÄòUS‚Äô, ‚ÄòAL‚Äô, etc. Then using purrr::map(), we ask R to download each of the sheets of the dataset and store it together in a list (map() always returns a list).\n\ncensus_list <-\n  path %>% \n  excel_sheets() %>% \n  set_names() %>% \n  map(~ read_excel(path = path, sheet = .x, skip = 3), .id = \"Sheet\")\n\nIf you take a look at the list using View(census_list), you can see the data is stored as tibbles within the list. If you expand US, you‚Äôll see the same data as when we did the single sheet example. You can also see the same data if you run census_list[[\"US\"]].\n\n\n\nViewing the list\n\n\nUsing the same thinking as we did with the single sheet example, let‚Äôs go through and clean up this list - without having to go into each tibble!\nFirst, we set the names within each list using set_names(). We tell map() the names of the columns by defining nm.\n\ncensus_list <- \n  census_list %>% \n  map(., set_names, nm = new_names)\n\nFor each tibble in the list (.x), remove the rows 1 through 3 and 60.\n\ncensus_list <- \n  census_list %>% \n  map(~ slice(.x, -1:-3, -60:-61))\n\nNow for each tibble, filter out the rows in select_characteristics that contain the items in filter_var.\n\ncensus_list <- \n  census_list %>% \n  map(~ filter(.x, !select_characteristics %in% filter_var))\n\nLike before, we want a new column that lets us know the category for each of the characteristic options.\n\ncensus_list <- \n  census_list %>% \n  map(~ add_column(.x, category_column))\n\nAnd like before, we want to make sure our numeric columns are actually numeric.\n\ncensus_list <- \n  census_list %>% \n  map(~ mutate_at(.x, vars(total, using_online_resources:did_not_respond), list(~ as.numeric(.))))\n\nNow that our tibbles are all clean and uniform, let‚Äôs make this a single, 2D data frame! As I mentioned before, our list should contain the state abbreviations. We can use map_df() to create a data frame with an ID column called state that stores each of the sheet names. With this column, we‚Äôll easily know which column is for which state/geography.\n\ncensus_df <- \n  census_list %>% \n  map_df(~ as.data.frame(.x), .id = \"state\")\n\nCongrats! We have successfully tidied a Census dataset!"
  },
  {
    "objectID": "blog/tidying-census-data/index.html#using-the-data",
    "href": "blog/tidying-census-data/index.html#using-the-data",
    "title": "What it takes to tidy Census data",
    "section": "Using the data",
    "text": "Using the data\nThe purpose of all this work is to be able to use it easily in R and with the tidyverse specifically. Let‚Äôs use the plotting package {ggplot2} to make something!\nAccording to the Census website, we can calculate percentages by removing those that did not respond from the total for the denominator (let‚Äôs presume that NA in the column means that everybody responded). Let‚Äôs say we want to see the proportion of respondents in the U.S. who say their classes were canceled by income level.\n\ncensus_us_income <-\n  census_df %>% \n  filter(state == \"US\", category_column == \"income\") %>% \n  mutate(responses = case_when(!is.na(did_not_respond) ~ total - did_not_respond, \n                               is.na(did_not_respond) ~ total),# calculate denominator\n         pct_cancelled = where_classes_were_cancelled / responses) # calculate percentage\n\ncensus_us_income <- # setting factor levels so graph shows correct order\n  census_us_income %>% \n  mutate(select_characteristics = factor(select_characteristics,\n                                         levels = c(\"Less than $25,000\", \n                                                    \"$25,000 - $34,999\",\n                                                    \"$35,000 - $49,999\",\n                                                    \"$50,000 - $74,999\",\n                                                    \"$75,000 - $99,999\",\n                                                    \"$100,000 - $149,999\",\n                                                    \"$150,000 - $199,999\",\n                                                    \"$200,000 and above\")))\n\ncensus_us_income %>% \n  filter(select_characteristics != \"Did not report\") %>% \n  ggplot(aes(x = select_characteristics, y = pct_cancelled)) +\n  geom_bar(stat = \"identity\",\n           fill = \"#00C3DA\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Percent of Respondents Whose Children's Classes Were Cancelled\",\n       x = \"Income\",\n       y = \"Percent with Classes Cancelled\",\n       caption = \"Source: U.S. Census\")\n\n\n\n\n\n\n\n\nFrom this graph, we can see that respondents from the lower-income bands were more likely to say that classes were canceled for their children due to COVID.\n\nLiked this article? I‚Äôd love for you to retweet!\n\n\nNew blogpost üì£: What It Takes to Tidy Census Aggregate Data! ‚≠ê Let's take all those heavily formatted Excel sheets and make them a plain and simple data frame together! https://t.co/IsPb8BJ0ZT #rstats pic.twitter.com/XABr07wYfP\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 29, 2020"
  },
  {
    "objectID": "blog/macos-rig/index.html#tldr",
    "href": "blog/macos-rig/index.html#tldr",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "TL;DR",
    "text": "TL;DR\nIn this post, we cover our preferred setup for R on macOS going into 2023. We highlight these awesome tools: Homebrew, Git, zsh, Oh My Zsh, rig, the R packages {usethis} and {gitcreds}, and Quarto.\nIf you prefer to look at a short list than read through a long-form blog post, much of the information below is summarized in this dotfiles repository. Keeping your own dotfiles repository, or bookmarking this post or others, will help you stay sane next time you are setting yourself up for data science in R on a brand new macOS installation.\n\nThe Problem\nInstallation\nFinal Thoughts\nRelated Work"
  },
  {
    "objectID": "blog/macos-rig/index.html#the-problem",
    "href": "blog/macos-rig/index.html#the-problem",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "The Problem",
    "text": "The Problem\nLike a new year üóìÔ∏è, a new computer üñ•Ô∏è (or a fresh reinstall) can bring so many opportunities. A clear desktop! So much memory! And then you remember that you have to install and configure all of your programs again, and that initial excitement gives way to despair and hopelessness. üò±\nOkay, that‚Äôs a bit of an exaggeration, ü§≠ but it can be tedious to reconfigure your setup the way that you like it. However, by using dotfiles, or hidden configuration files (whose names start with a dot/period, which on macOS hides them from view in the Finder), you can store your user preferences on a hosted version control service like GitHub. By dropping dotfiles in the correct directory, you can access and reuse your exact configuration and settings across machines, saving time and hassle. ‚ôªÔ∏è\nIn this post, we‚Äôll share some helpful dotfiles for all you RStudio users who are rocking a new macOS üçé installation in 2023. This post is geared towards you if:\n\nYou‚Äôve been installing R and RStudio on macOS and setting them up in your own way already, but either forget the many steps (Git, etc.) or are always having to relearn them;\nYou do a lot of point-and-click downloading of files for installation, and have been considering jumping into command-line adventures üë©üèª‚Äçüíª with Homebrew;\nYou‚Äôve never heard of rig;\nYou want a one-stop-shop for setting up your rig next time;\nYou have a well-oiled, well-defined workflow but are interested in seeing how others set up their machines, to perhaps learn something new!\n\nSince it is the year of ChatGPT, we did ask its suggested setup. We thought we could do better (at least for now, maybe not in 2024)! ü§ó\nLet‚Äôs fall into a pit of success TM (when configuring macOS)!"
  },
  {
    "objectID": "blog/macos-rig/index.html#installation",
    "href": "blog/macos-rig/index.html#installation",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "Installation",
    "text": "Installation\nWe‚Äôre going to use Homebrew to facilitate installation steps on macOS. Homebrew styles itself as ‚Äòthe missing package manager for macOS.‚Äô\n\nOpen Applications > Utilities > Terminal. Install Homebrew using the terminal command below, also provided on the Homebrew landing page. Paste this code into your terminal and hit enter:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Git. This version control system is going to come in handy for future projects on your new installation.\n\nbrew install git\n\nConfigure global Git settings.\n\nRun the code below (replacing the text in the quotation marks with your information) to adjust your Git settings.\ngit config --global user.name \"<full-name>\"\ngit config --global user.email \"<email>\"\n\nInstall zsh.\n\nSeveral years back, macOS switched from using the bash shell to using zsh as the default shell. Installing the latest version of zsh from Homebrew gives you the latest version ‚Äî otherwise, zsh and Git are both stuck at the version bundled the last time you updated macOS ‚Äî which makes them open to vulnerabilities and/or don‚Äôt have the latest features.\nbrew install zsh\n\nInstall Oh My Zsh.\n\nOh My Zsh (OMZ) is ‚Äòa delightful & open source framework for Zsh.‚Äô Or as ChatGPT succinctly put it:\n\nIt comes with a ton of plugins, themes, and functions. For most users, having the great themes available from OMZ are enough to switch. But for us, the real value is all the built-in OMZ plugins ‚Äî which provide a rich set of aliases for Homebrew, Git, and many other command-line programs. For example, gs is git status, bubo is brew update && brew outdated (very helpful), etc. Here are some you should know:\n\nThe brew plugin and its aliases\nThe git plugin\nThe macos plugin for easier living through the Terminal\nThere‚Äôs even a plugin that reminds you what your aliases are! ‚Äì for maximum discoverability\n\nYou can also hand-code your themes into your .zshrc dotfile instead of using the OMZ themes. The aliases not provided by OMZ can be added to the custom.zsh dotfile, such as the ones in the dotfiles linked below.\nInstall OMZ with:\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\nConfigure Oh My Zsh.\n\nTo configure oh-my-zsh, you can:\n\nEdit your settings: Navigate to your home directory ~/ and edit the .zshrc file.\nUse preconfigured settings: Navigate to your home directory ~/. Replace the default .zshrc file with this file. Then, navigate to the hidden folder ~/.oh-my-zsh/custom and drop this file in the folder.\n\n\n\n\n\n\n\nTip\n\n\n\nOn a Mac, you can press Command + Shift + . to see your hidden folders.\n\n\nA custom.zsh file might be helpful because you can add many more commands for OMZ plugins than what are built in. For instance,\nalias bog=\"brew outdated --greedy\"\nThis allows you to use bog to run brew outdated --greedy, instead of having to type the whole command out. That particular command is helpful if you want to find ALL the programs that could potentially be upgraded.\n\n\n\n\n\n\nGo back to the Terminal and install rig.\n\nrig, the ‚ÄòR Installation Manager,‚Äô is an amazing tool. It not only lets you download R, but install, configure, switch between, and remove multiple R versions. It also allows you to manage user-level package libraries (meaning you can delete a version of R without losing all your packages!). Mara Averick wrote a great summary on Day 10 of Tidy Advent 2022.\nInstall rig by running these commands in the Terminal:\nbrew tap r-lib/rig\nbrew install --cask rig\n\nInstall R.\n\nInstall the latest version of R using rig:\nrig add release\n\nInstall RStudio Desktop.\n\nYou want the latest and greatest RStudio, no? Use Homebrew to download and install the latest version. Once a new version is released, the community is very quick at updating the cask entry for RStudio in Homebrew.\nbrew install --cask rstudio\n\nConfigure .Rprofile.\n\nIn RStudio, install the {usethis} package, a tremendous tool for increased productivity with your R projects. In other words, you should use {usethis}. üòÄ\ninstall.packages(‚Äúusethis‚Äù)\nOnce installed, open your .Rprofile by running:\nusethis::edit_r_profile()\nHere‚Äôs an example of an .Rprofile script. This script [1] sets {devtools} and {usethis} to load with each R session (to write install_github() without a library(devtools)), [2] sets Posit Public Package Manager (P3M) as the default repo for Linux (not relevant for this macOS guide), [3] sets a bunch of {usethis} defaults for package development which need to be overridden with personal details for each person, [4] sets vsc.rstudioapi = TRUE for VS Code (again not relevant for this guide), [5] sets several strict warnings which are helpful when developing packages (e.g., not using partial argument matching).\nFor your rig configuration, you would create a similar script, edit the .Rprofile, and restart the session.\n\nConfigure RStudio.\n\nSnippets help quickly insert common short pieces of code. Open RStudio‚Äôs snippet config for R scripts:\nusethis::edit_rstudio_snippets(\"r\")\nAppend this script to the bottom of the file for some useful examples.\n\n\n\n\n\n\nNote\n\n\n\nChange the prefix of your snippets to your initials. The default in these examples is gv.\n\n\nTry a snippet out! After appending the script to your snippet file, type dml in the Console and press tab:\n\n\n\nFor whenever you want to remember De Morgan‚Äôs laws. üòâ\n\n\nNow, open RStudio‚Äôs preference file by running:\nusethis::edit_rstudio_prefs()\nAdd the content of this file to the rstudio-prefs.json file. This file configures RStudio to look and act the way that you‚Äôd like (rainbow parentheses, anybody?). üåà\n\n\n\n\n\n\nNote\n\n\n\n\nChange default_project_location to your preferred directory. The default in this example is ~/code.\nChange git_exe_path to the location of your Git executable, which can be found by typing which git in a terminal. The default in this example is /usr/local/bin/git.\nChange document_author to your name. The default in this example is gvelasq.\n\n\n\nFinally, add the contents of rstudio_bindings.json to the local version found in this hidden folder (I don‚Äôt think there‚Äôs a {usethis} command for this, unfortunately!):\n~/.config/rstudio/keybindings/rstudio_bindings.json\nNow, anytime you want to make sure you‚Äôre using the most current version of RStudio, you just have to run Shift+Cmd+R!\n\nSet Git credentials in R.\n\nThe {gitcreds} package allows you to set your GitHub token once and use it everywhere (Git, R, and the RStudio IDE). Use it to set up RStudio with a Personal Access Token (PAT) with GitHub and have it work every time.\nInstall the {gitcreds} package:\ninstall.packages(\"gitcreds\")\nThen, run usethis::create_github_token() to open a browser window to the GitHub form to generate a PAT with suggested, pre-selected scopes. This helps you make the right choice of scopes every time.\nusethis::create_github_token()\nThen, the command will suggest to use gitcreds::gitcreds_set() to save the PAT locally using the macOS Git credential manager (after copying it from GitHub):\ngitcreds::gitcreds_set()\n\nInstall Quarto.\n\nQuarto is a (kinda new) open-source scientific and technical publishing system built on Pandoc. It allows you to create dynamic content with R and Python, like documents, websites (like this blog!), and presentations.\nQuarto comes bundled with RStudio v. 2022.07 and later, so the installation in step 9 above should have this covered. But, if you‚Äôd like to install Quarto yourself, run this in your Terminal:\nbrew install --cask quarto\nYou‚Äôve done it! Congratulations on your configured, custom R data science setup."
  },
  {
    "objectID": "blog/macos-rig/index.html#final-thoughts",
    "href": "blog/macos-rig/index.html#final-thoughts",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWe hope that this post is helpful in setting up an R data science rig on macOS! We know there were a lot of steps. In summary:\n\nDotfiles are hidden configuration files where you can store your user preferences and use them across machines and installations.\nThe configuration steps above ensure that you‚Äôre using the best tools for the macOS terminal: Homebrew (for package management) and zsh (for execution).\nThey also ensure you‚Äôre using the best tools for running analyses: Git (for version control), rig (for R versioning), R (the best programming language [in our humble opinion]), RStudio (the GOAT), {usethis} (for project setup and development), {gitcreds} (for managing Git), and Quarto (for content creation and publishing).\nTools like OMZ and snippets help you use aliases that make running your workflow easier to remember.\n\nWe know there are a ton of ways of setting up a data science rig, and we‚Äôd love to learn about your dotfile repositories, best practices, or favorite workflow packages on Twitter/Mastodon!\n\nTwitterMastodon\n\n\n\n\n\nMastodon Toot Link"
  },
  {
    "objectID": "blog/macos-rig/index.html#related-work",
    "href": "blog/macos-rig/index.html#related-work",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "Related Work",
    "text": "Related Work\n\nRSwitch was a menubar app that let you switch between different R versions; it is now archived.\n@grrrck‚Äôs guide, published in 2020, documents an epic adventure of setting up a Macbook Pro.\n{pak} is an awesome R package and the only reason we didn‚Äôt use it is because it doesn‚Äôt yet work with renv. Since we use {renv}, we‚Äôre holding off on {pak} for now, but we do look forward to using both tools together!"
  },
  {
    "objectID": "blog/macos-rig/index.html#chat-gpt",
    "href": "blog/macos-rig/index.html#chat-gpt",
    "title": "Setting up macOS as an R data science rig in 2023",
    "section": "Where we have ChatGPT write our blog post for us",
    "text": "Where we have ChatGPT write our blog post for us\nHere‚Äôs ChatGPT‚Äôs not bad (but still unnerving) response to: ‚ÄòWrite a blog post, for a technical blog about the R programming language, entitled ‚ÄúSetting up macOS as an R data science rig in 2023.‚Äù Include installation and configuration examples for Homebrew, Git, zsh, oh-my-zsh, R, RStudio, and Quarto.‚Äô\n\n\nSee response.\n\n As a data scientist, it is important to have a reliable and efficient setup for your work environment. In this tutorial, we will walk through the steps of setting up macOS as an R data science rig in 2023. We will cover the installation and configuration of Homebrew, Git, zsh, oh-my-zsh, R, RStudio, and Quarto. By the end of this tutorial, you will have a fully functional and customizable R development environment on your Mac.\nStep 1: Install Homebrew\nHomebrew is a package manager for macOS that makes it easy to install and manage software on your Mac. To install Homebrew, open a terminal window and paste the following command:\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nHit enter and follow the prompts to complete the installation.\nStep 2: Install Git\nGit is a version control system that allows you to track changes to your code and collaborate with others. To install Git using Homebrew, run the following command in the terminal:\nbrew install git\nStep 3: Install zsh and oh-my-zsh\nzsh is a shell (command line interface) that allows you to execute commands on your computer. oh-my-zsh is a framework for managing your zsh configuration. To install zsh and oh-my-zsh, run the following commands in the terminal:\nbrew install zsh\n\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nStep 4: Install R\nTo install the latest version of R on your Mac, run the following command in the terminal:\nbrew install r\nStep 5: Install RStudio\nRStudio is a popular integrated development environment (IDE) for R. To install RStudio, visit the RStudio website (https://rstudio.com/) and download the latest version for Mac. Once the download is complete, open the downloaded file and follow the prompts to complete the installation.\nStep 6: Install Quarto\nQuarto is a powerful tool for creating reproducible research documents in R. To install Quarto, run the following command in the terminal:\ninstall.packages(\"quarto\")\nConclusion:\nBy following the steps outlined in this tutorial, you should now have a fully functional and customizable R development environment on your Mac. With Homebrew, Git, zsh, oh-my-zsh, R, RStudio, and Quarto installed and configured, you are ready to start your data science journey with R on macOS. Happy coding!\nWe told you it was unnerving üëÄ"
  },
  {
    "objectID": "blog/leaidr-shapefiles/index.html",
    "href": "blog/leaidr-shapefiles/index.html",
    "title": "Introducing an R package for school district shapefiles",
    "section": "",
    "text": "Abraham Mignon, Still Life with Flowers and a Watch (1660 - 1679)\nI would like to introduce {leaidr}, a package for mapping U.S. school district shapefiles!\nInspired by my coauthor Joshua Rosenberg, the goal of {leaidr} is to facilitate the download and use of school district shapefiles.\nSchool districts in the U.S. have associated local education agency identification numbers (LEAID) used in the National Center for Education Statistics (NCES) Common Core of Data. These are very useful because if you have other datasets with NCES IDs, then you can (sometimes easily) join them.\nIt can be very useful to visualize districts and associated data. District shapefiles are available in different places, such as through the NCES and Stanford Education Data Archive. The package {tigris} also has a school district option, but unfortunately, it is missing a few district polygons.\n{leaidr} downloads NCES‚Äô U.S. district shapefile from Github using ROpenSci‚Äôs {piggyback} package. This is a super helpful package, as Github caps file uploads at 100 MB (and the shapefile is ~170 MB). I originally tried Github Large File Storage (LFS), but it stores files as a hash, not as an actual file. Therefore, I couldn‚Äôt figure out how to use it for a package that others can use.\nThe function lea_get() downloads an R Data file from the Github repo to your designated path and then writes the necessary shapefiles. Then, create an object with lea_prep() by designating where the shapefiles exist and which state(s) you would like. Note: For now, you must use the state‚Äôs FIPS code. FIPS state codes are numeric and two-letter alphabetic codes to identify U.S. states and certain other associated areas. A reference table is found here.\nOnce you have the shapefile, then you can merge with other datasets and plot using packages like {leaflet} and {ggplot2}."
  },
  {
    "objectID": "blog/leaidr-shapefiles/index.html#example-of-creating-a-school-district-map",
    "href": "blog/leaidr-shapefiles/index.html#example-of-creating-a-school-district-map",
    "title": "Introducing an R package for school district shapefiles",
    "section": "Example of creating a school district map",
    "text": "Example of creating a school district map\nLet‚Äôs walk through an example where we will merge external data to the shapefile and then map all the districts in California. The external data is from Josh‚Äôs covidedu project, which scrapes district websites for specific words. In this case, the search terms were ‚Äúcovid*‚Äù, ‚Äúcoron*‚Äù, and ‚Äúclosure‚Äù. I highly recommend using covidedu for easy scraping from a lot of district websites!\nFirst, let‚Äôs call our packages.\n\nlibrary(tidyverse)\n# if you haven't installed the package yet\n# devtools::install_github(\"ivelasq/leaidr\")\nlibrary(leaidr)\nlibrary(maptools)\n# if you don't have this downloaded\n# install.packages(\"mapproj\")\n\nTime to get your data! Use {leaidr} to download and prep your shapefiles for California (FIPS == 06). Read in the external data (in this case, summary-of-table-of-links.csv).\n\n\n\n\n\n\nNote\n\n\n\nYou must have a GitHub PAT set to run lea_get(). Happy git with R has a great walkthrough on how to get one if you do not have it already.\n\n\n\n# download the shapefile into a designated folder\nleaidr::lea_get(path = \"./test\")\n\n# prep the shapefile for the state(s) you'd like\nca_shapefile <-\n  leaidr::lea_prep(path = \"./test\", fips = \"06\")\n\n# read in the external data that also has NCES ID's\n# this is from the covidedu project\nca_data <-\n  read_csv(\"https://raw.githubusercontent.com/making-data-science-count/covidedu/master/output/2020-04-29/summary-of-table-of-links.csv\")\n\nJoin the CSV to the shapefile.\n\nca_merge <-\n  sp::merge(ca_shapefile, ca_data, by.x = \"GEOID\", by.y = \"nces_id\")\n\nNow ‚Äòfortify‚Äô the data - this converts the polygons into points. This is so that ggplot can create the plot.\nIf you get the error isTRUE(gpclibPermitStatus()) is not TRUE, then you need to enable gpclib by running gpclibPermit() (this is part of the {maptools} package, which should have been loaded above). Note that support for gpclib will be withdrawn from maptools at the next major release, so you might have to try something else if the package has been upgraded.\nIf you run gpclibPermit() and you keep getting FALSE, then you are missing the package {gpclib}. Install the package, then run gpclibPermit() to set it to TRUE.\n(I don‚Äôt know if this is the best/only way to do this - if anybody has suggestions, please let me know!)\n\n# install.packages(\"gpclib\")\ngpclibPermit()\nca_points <- fortify(ca_merge, region = \"GEOID\")\n\nNow, join the points and the shapefile data.\n\nca_df <- left_join(ca_merge@data, ca_points, by = c(\"GEOID\" = \"id\"))\n\nWe can finally plot the shapefile and its data!\n\nca_map <-\n  ca_df %>% \n  ggplot() +\n  geom_polygon(aes(x = long, \n                   y = lat, \n                   group = group,\n                   fill = any_link_found),\n               color = \"gray\", \n               size = .2) +\n  theme_void() +\n  scale_fill_iv() +\n  ggtitle(\"COVID-Related Links Found on CA School District Sites\")\n\nTo make a nicer looking map, then you can use coord_map().\n\nmap_projected <- ca_map +\n  coord_map()\n\nprint(map_projected)\n\n\nTada! A full school district map for California."
  },
  {
    "objectID": "blog/leaidr-shapefiles/index.html#call-for-collaboration",
    "href": "blog/leaidr-shapefiles/index.html#call-for-collaboration",
    "title": "Introducing an R package for school district shapefiles",
    "section": "Call for collaboration",
    "text": "Call for collaboration\nPlease try out {leaidr}! I hope that it is useful to you in your work. I‚Äôd love any collaborators to join me in making it easier/better!\n\nOther functionality: For example: being able to filter shapefiles by NCES IDs as well as states; adding commonly used data (like district demographics).\nIssues: If you run into any issues, please post on the GitHub page!"
  },
  {
    "objectID": "blog/leaidr-shapefiles/index.html#resources",
    "href": "blog/leaidr-shapefiles/index.html#resources",
    "title": "Introducing an R package for school district shapefiles",
    "section": "Resources",
    "text": "Resources\n\nJoining Spatial Data\nAnalyzing U.S. Census Data Using R\n\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nEver wish you could easily create U.S. school district maps üó∫üè´ in #rstats? Check out {leaidr}, which downloads the shapefile from GitHub using @rOpenSci 's {piggyback} üê∑! Would love any thoughts on how to improve! Repo: https://t.co/yOszd4agYM Tutorial: https://t.co/JuqGQcVR7Z\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 11, 2020"
  },
  {
    "objectID": "blog/why-rstudio/index.html",
    "href": "blog/why-rstudio/index.html",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "",
    "text": "Vincent van Gogh, The Olive Trees (1889)\nI am excited to announce that I‚Äôve joined the RStudio Marketing team! I will be creating content on the RStudio Blog that will announce news and share stories from the company, users, and partners. I just started this week and, ready to jump in, I decided that I‚Äôd like to deep dive into previous blog posts.\nThe RStudio blog is written using blogdown and each blog entry contains metadata in the YAML header. Since there are hundreds of blog entries, I decided to use R to quickly pull all this information into an analyzable data frame. While figuring out how to do this, I was reminded of why I am so excited to join RStudio in the first place (and if you want to jump straight into the code, click here!)"
  },
  {
    "objectID": "blog/why-rstudio/index.html#the-awesomeness-that-is-the-r-community",
    "href": "blog/why-rstudio/index.html#the-awesomeness-that-is-the-r-community",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "The awesomeness that is the R community",
    "text": "The awesomeness that is the R community\nAs I started the task of pulling the YAML information, I remembered a blogpost from Garrick Aden-Buie called Find, count and list tags in all blogdown posts (thanks, Garrick!). The post taught me about blogdown:::scan_yaml() which was exactly what I needed to get the information from all the YAML headers in the files.\nThe members of the R Community, including others at RStudio, are incredibly collaborative and helpful, and I hope to be able to contribute to others‚Äô learning as well."
  },
  {
    "objectID": "blog/why-rstudio/index.html#rstudio-tools-rock",
    "href": "blog/why-rstudio/index.html#rstudio-tools-rock",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "RStudio tools rock",
    "text": "RStudio tools rock\nI remember when my older brother introduced me to the tidyverse in graduate school after I had been struggling to understand how to program in R. I was amazed by its usability and functionality and it enabled me to learn more, learn quicker, and learn better. I‚Äôve been fortunate enough to use RStudio tools for my work, like Shiny and shinyapps.io, and each time appreciated the thoughtful and deep work that RStudio does with its services and products.\nHow great is it that the blogdown team anticipated the need to look at a blog‚Äôs metadata and already created a function that scans the YAML?! And that we can create a blog using R? And doing so is excellently documented?\nIn addition, below you will see many different tidyverse packages (dplyr, here, tibble, tidyr) being used as well. The seamless integration of R packages, tools, and services makes projects like this one possible and easy."
  },
  {
    "objectID": "blog/why-rstudio/index.html#so-much-to-write-about",
    "href": "blog/why-rstudio/index.html#so-much-to-write-about",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "So much to write about",
    "text": "So much to write about\nIf you check out the tags from the RStudio blog, you will see so many topics. APIs! BI tools! Package development! Interoperability! RStudio is working on so many things, open source and enterprise, and it is thrilling to be part of an organization that advocates for code-first development, thrives in a diverse and supportive community, and thinks creatively about what challenges to tackle next."
  },
  {
    "objectID": "blog/why-rstudio/index.html#a-new-chapter-in-my-rstudio-journey",
    "href": "blog/why-rstudio/index.html#a-new-chapter-in-my-rstudio-journey",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "A new chapter in my RStudio journey",
    "text": "A new chapter in my RStudio journey\nI am proud to join an organization that I‚Äôve long admired and hope that my contributions in this realm of RStudio‚Äôs business will promote the uptake of amazing RStudio products, and the crucial work RStudio does in free open-source software, data science education, and other spaces as a Public Benefit Corporation. I look forward to working alongside my colleagues at RStudio to achieve the company‚Äôs vision.\nAnd with that‚Ä¶ I give you, the code!"
  },
  {
    "objectID": "blog/why-rstudio/index.html#the-code",
    "href": "blog/why-rstudio/index.html#the-code",
    "title": "Why I‚Äôm excited to join RStudio, told through a blogdown metadata project",
    "section": "The code",
    "text": "The code\n\nPulling YAML metadata for each blog post\nI intended to create a data frame from a blogdown where every row represents a blog entry, and where we will have columns for (1) its title, (2), its author(s), (3) associated tags, and (4) associated categories. If you are using this code on your blogdown, you can change them to whichever YAML fields you would like.\n# Load libraries\nlibrary(tidyverse)\n\n# Find the folder with the blog posts\nblogdown_content_path <- here::here(\"content\", \"post\")\n\n# Scan yaml of all posts (make sure the blogdown project is open)\nmeta_list <-\n  blogdown:::scan_yaml(blogdown_content_path)\n\n# Create a data frame that pulls out the title, date, categories, and tags for each entry\nmeta_df <- tibble(\n  title = map_chr(meta_list, \"title\"),\n  date = map(meta_list, \"date\"),\n  categories = map(meta_list, \"categories\"),\n  tags = map(meta_list, \"tags\"))\nIn this case, there are entries where there is a vector within categories or tags - this is because a single blog post can have multiple categories or tags (see example here, which has ‚Äúrstudio connect‚Äù and ‚Äúfeatured‚Äù as categories).\n\n\n\n\nCell names\n\n\n\nThe following code separates the tags or categories with multiple values so that each value has its own column using a pivot_longer() followed by unnest(), then re-pivots the table to wide format using pivot_wider().\nmeta_df_clean <-\n  meta_df %>%\n  # Put the fields with multiple options into one column called variable\n  pivot_longer(cols = c(\"categories\", \"tags\"),\n               names_to = \"variable\",\n               values_to = \"value\") %>%\n  # Split out the fields with multiple options into individual rows\n  unnest(cols = value) %>%\n  # For each blog entry, give each category or tag a number\n  group_by(title, variable) %>%\n  mutate(id = row_number()) %>%\n  # Pivot back to wide format\n  pivot_wider(names_from = c(\"id\", \"variable\"),\n              names_glue = \"{variable}_{id}\",\n              values_from = value)\nAnd that‚Äôs it! Now we have a data frame with each row denoting a blog entry, with its associated metadata.\nThank you so much for reading this article and I look forward to seeing you on the RStudio blog or Twitter.\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nHere's how to pull the YAML for your blogdown posts using #rstats; figuring out how to do this was a great example of why I am excited to join RStudio ü•≥blogpost with code: https://t.co/RdI4zEwfX8h/t to @grrrck 's post on finding tags in blogdown: https://t.co/p85frQbAiG pic.twitter.com/bWwH2rsIYf\n\n‚Äî Isabella Vel√°squez (@ivelasq3) September 13, 2021"
  },
  {
    "objectID": "blog/party-with-R/index.html",
    "href": "blog/party-with-R/index.html",
    "title": "Party with R: How the community enabled us to write a book",
    "section": "",
    "text": "Henri de Toulouse-Lautrec, Marcelle Lender Dancing the Bolero in Chilp√©ric (1895-1896)\n\n\n\nIt is like a party all the time; nobody has to worry about giving one or being invited; it is going on every day in the street and you can go down or be part of it from your window. - Eleanor Clark\n\nWithout the R community, Data Science in Education Using R would never have happened. Most evidently, we wouldn‚Äôt have met each other without the strong R presence on Twitter that sparked a conversation about data use in education. More importantly though, are the aspects of the R community that inspired that initial discussion and enabled us to complete a book for a broad and complex field.\nIn the first chapter of our book, we invite data practitioners in education to the party but the R community invited us to the party first. Like any successful party, certain elements had to exist for us to join in and end up having an amazing time.\nThe Invitation üì©\nAs someone who started following the R community on Twitter after it was already well established and popular, I never felt the apprehension about having to ask how to get involved or interact with others. First, there are so many avenues. The R community offers many ways to let you in, whether it be replying to Tweets, posting a question on community.rstudio.com, or sharing a blog post on R Weekly. Second, the R community welcomes users no matter their level. Whether it is a code snippet with a function you found cool, a blog post, or a personal side project, there are ways to engage that appeal to everybody. Members of the R community can interact how they want and as often as they want.\nFor us, it was exciting to meet on Twitter, talk about collaborating on an education data project, and then just get started on it. We felt welcome and encouraged to do so. Because people in the R community meet other users virtually and begin side projects all the time, we didn‚Äôt have to worry about whether something like this was possible: The invitation was already there.\nAn Open Door üö™\nIn our first blog post, we described what it‚Äôs like to learn by seeing someone do the thing you want to learn. One of the best things about the R community is you constantly get to see this in action. The R community not only holds open principles but actually exhibits them whenever possible. Users post their code, projects, and drafts constantly. Just by scanning the #rstats hashtag, one can discover something new.\nBecause others were open, we knew that we wanted to be open as well. We wrote our book on GitHub, showing all the many changes it went through until its final completion. By having it freely available on a website, we hope that it opens the door to others who‚Äôd like to learn what we did and work on their own open project as well.\nFood (for Thought) üçï\nThe R community offers so many opportunities to get feedback, advice, and information from a wide variety of users. Early on in the book development, we had a lot of questions to nail down: who is the audience? ‚ÄúData is‚Äù or ‚Äúdata are?‚Äù How do we describe ‚Äúpeople who work in the education field and use data and want to get more effective at it?‚Äù Finding a common language was difficult, but we were able to do this by engaging others in the wider R community. We listened to the stories of many data scientists who work in education, then found common experiences we could describe in our writing.\nAs an example, we learned we weren‚Äôt the only ones challenged by learning a programming language while attending to full-time jobs and personal lives. Knowing this, we made sure to discuss these challenges in our book and offer various ways of engaging with the material based on the reader‚Äôs needs.\nSocializing üí¨\nThroughout the process of writing DSIEUR, we asked the R community several times for other types of feedback and suggestions. We also ran into some technical issues, especially when it came to preparing our manuscript with {bookdown} to meet our publisher‚Äôs specifications, and were able to get ideas on how to resolve them. We know that the R community is a safe and encouraging place to ask questions, and this enabled us to write a stronger book.\nThe Next Day ‚òÄÔ∏è\nThe community participation throughout the DSIEUR process helped us define our goals and get feedback. Another wonderful aspect is that the R community engaged us back. Writing this RViews series is an example: Someone reached out to us and wanted us to reflect on what we discovered and share it with all of you. This type of engagement reminds us of what an inclusive and encouraging place the R community is and helps us come up with new ways of making sure others see the invitation as well.\nThank you for reading! üéâ We‚Äôll be back with the fourth post on ‚ÄúOne Writer, Five Authors‚Äù in about two weeks. Until then, we‚Äôd love to know how else the R paRty has encouraged your work, both personally and professionally. You can reach us on Twitter: Emily ebovee09, Jesse kierisi, Joshua jrosenberg6432, Ryan RyanEs, and me ivelasq3.\n\nLiked this post? I‚Äôd love for you to retweet!\n\n\nParty with R: How the Community Enabled Us to Write a Bookhttps://t.co/NnGqM9Ux2X\n\n‚Äî RStudio (@rstudio) August 4, 2020\n\n\n\nThis was originally posted on RViews."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html",
    "href": "blog/understanding-the-r-pipe/index.html",
    "title": "Understanding the native R pipe |>",
    "section": "",
    "text": "Henri-Edmond Cross, Landscape with Stars, (1905‚Äì1908)\nA while back, I wrote this tweet showing many (not all!) of the ways one might search for a particular set of columns in a data frame using R. Several of these approaches used the {magrittr} pipe (%>%) and the native R pipe (|>), the latter of which has been available since R version 4.1.0. The {magrittr} and native R pipes work in different ways and one‚Äôs mental model of each requires some maintenance. This is the story of how I learned to understand the native R pipe."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#how-laziness-sparked-this-post",
    "href": "blog/understanding-the-r-pipe/index.html#how-laziness-sparked-this-post",
    "title": "Understanding the native R pipe |>",
    "section": "How laziness sparked this post",
    "text": "How laziness sparked this post\nWhen I am feeling lazy, I use base R for quick plots:\nplot(mtcars$hp, mtcars$mpg)\nBecause that clearly saves a lot of time compared to the {ggplot2} alternative üòÑ:\nlibrary(ggplot2)\nlibrary(magrittr)\n\nmtcars %>%\n  ggplot(aes(x = hp, y = mpg)) +\n  geom_point()\nOne day, I was feeling extra lazy, so I tried using the new native R pipe |>:\nmtcars |> plot(hp, mpg)\n#> Error in get(as.character(FUN), mode = \"function\", envir = envir) :\n#> object 'mpg' of mode 'function' was not found\nOh no! Why an error?\nIt was a complete misunderstanding of the native R pipe. I thought piping the data with the native R pipe would replace the need for the data$column notation. Turns out, that is not what the native R pipe does at all. So, this attempt to be lazy culminated in an adventuRe to figure out why that doesn‚Äôt work and what works instead."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#how-the-magrittr-pipe-works",
    "href": "blog/understanding-the-r-pipe/index.html#how-the-magrittr-pipe-works",
    "title": "Understanding the native R pipe |>",
    "section": "How the {magrittr} pipe %>% works",
    "text": "How the {magrittr} pipe %>% works\nFirst, let‚Äôs examine the pipe that you might be more familiar with (if you use the {tidyverse} dialect): the %>% forward pipe from the {magrittr} package.\nWithout any dot syntax (.), the {magrittr} pipe %>% is an infix operator that pipes (moves) what is written on the left-hand side (LHS) of the pipe into the first argument of the function on the right-hand side (RHS) of the pipe. Since R is prone to expressions with many nested parentheses, piping allows one to reason about code from left to right (as when writing in English), instead of right to left with many nested parentheses (see example below).\n\n\n\nBefore I learned case_when() üëÄ pic.twitter.com/khJpueqob2\n\n‚Äî Isabella Vel√°squez (@ivelasq3) December 3, 2021\n\n\n\nFrom the {magrittr} documentation, the pipe‚Äôs usage is thus: LHS %>% RHS. It might be easier to think of a ‚Äòpipe‚Äô as one of the famous ‚Äòwarp pipes‚Äô from Super Mario Bros.¬†‚Äî it warps Mario from the LHS into the first argument of the RHS function!\n\n\n\nSo:\nmtcars2 <-\n  mtcars %>%\n  dplyr::mutate(hi_mpg = dplyr::if_else(mpg > 25, \"high\", \"low\"))\nIs equivalent to:\nmtcars2 <-\n  dplyr::mutate(mtcars, hi_mpg = dplyr::if_else(mpg > 25, \"high\", \"low\"))\nAnd,\nmtcars %>% plot(hp, mpg)\nIs equivalent to:\nplot(mtcars, hp, mpg)\n\n# or, more explicitly\nplot(x = mtcars, y = hp, type = mpg)\nWhich does not work and gives us an error message, because the first two arguments of plot() should be objects for the x and y axes (mtcars is an odd x-axis but technically works), and the third argument is for the type of plot (mpg definitely doesn‚Äôt work there).\nIf you want the LHS to be passed somewhere other than the first argument with the {magrittr} pipe, you can use its dot syntax (.): y %>% f(x, .) is equivalent to f(x, y).\nmtcars %>%\n  lm(mpg ~ hp, data = .)\nIs equivalent to:\nlm(mpg ~ hp, data = mtcars)\nSo, how do we get the {magrittr} pipe to work with plot()? We can use the dot syntax as a placeholder for the data frame. However, this also does not work:\nmtcars %>% plot(.$hp, .$mpg)\n#> Error in match.fun(panel) :\n#> '.$mpg' is not a function, character or symbol\nWhy? The error gives a hint that something is wrong with the .$mpg step.1 By default, the {magrittr} pipe passes the LHS into the first argument of the RHS, so the call giving the error above is equivalent to writing:\nplot(mtcars, mtcars$hp, mtcars$mpg)\n\n# or, more explicitly\nplot(x = mtcars, y = mtcars$hp, type = mtcars$mpg)\nWhich does not work nor is what we intended (we want to have mtcars$hp on the x-axis, and mtcars$mpg on the y-axis). The way to get the {magrittr} pipe to do what we want with plot() is to use its curly brace {} syntax. By wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument:\nmtcars %>% {plot(.$hp, .$mpg)}\nThis works! It‚Äôs equivalent to having written:\nplot(mtcars$hp, mtcars$mpg)\nOK! Now, we can apply what we‚Äôve learned to the native R pipe. Right? ‚Ä¶Right?"
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#actually-i-think-we-need-a-detour-to-explain-anonymous-lambda-functions-before-we-can-explain-how-works",
    "href": "blog/understanding-the-r-pipe/index.html#actually-i-think-we-need-a-detour-to-explain-anonymous-lambda-functions-before-we-can-explain-how-works",
    "title": "Understanding the native R pipe |>",
    "section": "ACTUALLY I THINK WE NEED A DETOUR TO EXPLAIN ANONYMOUS (LAMBDA) FUNCTIONS BEFORE WE CAN EXPLAIN HOW |> WORKS",
    "text": "ACTUALLY I THINK WE NEED A DETOUR TO EXPLAIN ANONYMOUS (LAMBDA) FUNCTIONS BEFORE WE CAN EXPLAIN HOW |> WORKS\nWhy anonymous? Because they are not named functions from a package or written by you and stored in a function object. Anonymous functions are created on-the-fly, applied immediately, and don‚Äôt persist after they‚Äôve been used: function(x) {}.\nfunction(x) {\n  x[which.max(x$mpg), ]\n}\nWhat does this do? It creates an anonymous function (also known as a lambda function) if it is not saved to an object.\nIntroduced in R 4.1, the shortcut for anonymous functions \\(x) {} is the same as function(x) {}:\n# equivalent to the above\n\\(x) {\n  x[which.max(x$mpg), ]\n}\nWhat you gain from writing an anonymous function is that you get to direct traffic by explicitly stating the inputs and how they will be used in the function. Going back to our discussion of pipes, you get to direct exactly where the LHS of the pipe goes in the RHS."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#how-the-native-r-pipe-works",
    "href": "blog/understanding-the-r-pipe/index.html#how-the-native-r-pipe-works",
    "title": "Understanding the native R pipe |>",
    "section": "How the native R pipe works",
    "text": "How the native R pipe works\nLike the {magrittr} pipe %>%, the native R pipe |> pipes the LHS into the first argument of the function on the RHS: LHS |> RHS.\nYou can write:\nmtcars |> sqrt() |> head()\nWhich is equivalent to:\nhead(sqrt(mtcars))\n\n\n\n\n\n\nAn important note\n\n\n\nOn the RHS of |>, you need to include the function as a function call, which means appending an () at the end of the function name, rather than just its name. For example, the square root function is called by writing sqrt(). If you try to run mtcars |> sqrt without the () at the end, you will get an error: Error: The pipe operator requires a function call as RHS.\n\n\nSo, the native R pipe pipes the LHS into the first argument of the function on the RHS (with an extra requirement of needing a function call on the RHS). But that‚Äôs all! If you want to do anything beyond piping the LHS into the first argument of the RHS function, then you need the special anonymous function syntax introduced above.\nA gotcha here is that we also needed to write parentheses around the anonymous function, such that a pseudocode version of the above is mtcars |> (anonymous-function-definition)(). The reason for this is so that the second set of () properly points to the complex expression inside the first set of () as the function being called.23\nmtcars |> (\\(x) {\n   x[which.max(x$mpg), ]\n})()\nThinking back to what we learned about the {magrittr} pipe %>%, you might be tempted to use the dot syntax (.). A final important note is that the dot syntax does not work with the native R pipe |> since the dot syntax is a feature of {magrittr} and not of base R. For example, this works:\nmtcars %>% plot(.$hp)\nBut this doesn‚Äôt because there is no support for the dot syntax with the native R pipe:\nmtcars |> plot(.$hp)\n#> Error in pairs.default(data.matrix(x), ...) : object '.' not found\nHowever, if you create an anonymous function, you can decide what the input argument names are, whether ., x, data, anything! So, if you are tied to the dot syntax in {magrittr}, you can ‚Äòbootstrap‚Äô your own dot syntax with \\(.) {}.\nIn sum, the native R pipe does not support the dot syntax unless you explicitly define your own."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#getting-to-the-solution",
    "href": "blog/understanding-the-r-pipe/index.html#getting-to-the-solution",
    "title": "Understanding the native R pipe |>",
    "section": "Getting to the solution",
    "text": "Getting to the solution\nFinally, we get to the solution: to get the native R pipe to do what we wanted to do with plot(), we need to use an anonymous function and bootstrap the dot syntax (or any other argument name) for ourselves:\n# verbosely\nmtcars |> (function(.) plot(.$hp, .$mpg))()\n\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |> (\\(.) plot(.$hp, .$mpg))()\n\n# or if you prefer x to .\nmtcars |> (\\(x) plot(x$hp, x$mpg))()\n\n# or if you prefer to be explicit with argument names\nmtcars |> (\\(data) plot(data$hp, data$mpg))()\nThat‚Äôs it! üéâ It can be done, but the initial attempt to be lazy ended up taking much more time than originally expected. As the saying goes, ‚Äòa pipe in time saves nine.‚Äô\n\n\n\n\n\n\n\n\n\n2022-03-31 Note\n\n\n\nWhile this post is meant to be an exploration of the new native R pipe to understand how it works, a few folks have shared another solution: mtcars |> with(plot(hp, mpg)). This is an (admittedly simpler!) alternative to mtcars |> (\\(data) plot(data$hp, data$mpg))().\nFollowing what we have learned so far:\nmtcars |> with(plot(hp, mpg))\nIs equivalent to:\nwith(mtcars, plot(hp, mpg))\nSince it passes mtcars to the first argument on the RHS. This also creates the original plot we wanted, plot(mtcars$hp, mtcars$mpg)."
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#the-true-lazy-way",
    "href": "blog/understanding-the-r-pipe/index.html#the-true-lazy-way",
    "title": "Understanding the native R pipe |>",
    "section": "The true lazy way",
    "text": "The true lazy way\nThis was an exploration of how the %>% and |> pipes work, but we also have another option! The {magrittr} exposition pipe %$% ‚Äòexposes‚Äô the names in the LHS to the RHS expression.\nSo, the winner for lazy {magrittr} users like myself is:\nmtcars %$% plot(hp, mpg)\nNo dot syntax, no curly braces, no anonymous functions, no terminal function call, just a pipe and column names! It effectively emulates how plot() would work if it were a {tidyverse} function.\nReally though, all pipe users are winners here! As shown in the alignment chart tweet, we have many options of doing what we want to do with R.\n\n\n\nI couldn't resist getting in on the fun, updated for R >= 4.1, created using {gt}! h/t @skyetetra #rstats #tidyverse #magrittr #alignmentchart Repo here: https://t.co/gjGWLKYYG5 pic.twitter.com/1Ez6eQKkGd\n\n‚Äî Isabella Vel√°squez (@ivelasq3) June 15, 2021"
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#more-resources",
    "href": "blog/understanding-the-r-pipe/index.html#more-resources",
    "title": "Understanding the native R pipe |>",
    "section": "More resources",
    "text": "More resources\n\n{magrittr} forward pipe operator help documentation (or ?\"%>%\" in your R console)\nForward pipe operator help documentation (or ?pipeOp in your R console)\nFunctions chapter in Advanced R"
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#extra-credit-for-those-who-made-it-this-far",
    "href": "blog/understanding-the-r-pipe/index.html#extra-credit-for-those-who-made-it-this-far",
    "title": "Understanding the native R pipe |>",
    "section": "Extra credit for those who made it this far",
    "text": "Extra credit for those who made it this far\nHow would you do this with the proposed native R pipe-bind => syntax? Respond to my tweet about this blog post once you have the answer!\n\n\n\nNew blog post üë©üèª‚Äçüíªüìù Understanding the native R pipe |>In this post, I attempted the native R #pipeoperator and was warped into the RHS, and emerged from my misadventuRe having learned all about the #magrittr and #rstats pipes.Post: https://t.co/VOC1C6WGvl pic.twitter.com/IDAHvwyE4K\n\n‚Äî Isabella Vel√°squez (@ivelasq3) January 18, 2022"
  },
  {
    "objectID": "blog/understanding-the-r-pipe/index.html#acknowledgements",
    "href": "blog/understanding-the-r-pipe/index.html#acknowledgements",
    "title": "Understanding the native R pipe |>",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe author thanks @gvelasq for his comments on the first draft of this blog post, and all who replied to the Twitter thread with insights, alternatives, and feedback."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "%>% dreams",
    "section": "",
    "text": "I work at Posit (formerly RStudio). | Resume\nPosit Blog | R Views | Syndicated on R-Bloggers & R Weekly."
  },
  {
    "objectID": "project/package/wizehiver/index.html",
    "href": "project/package/wizehiver/index.html",
    "title": "wizehiver",
    "section": "",
    "text": "Wizehiver Hex"
  },
  {
    "objectID": "til/nzchar/index.html",
    "href": "til/nzchar/index.html",
    "title": "Count the number of characters (or bytes or width)",
    "section": "",
    "text": "From the documentation, nzchar() is a fast way to find out if elements of a character vector are non-empty strings. It returns TRUE for non-empty strings and FALSE for empty strings.\n\nNot Empty VectorEmpty Vector\n\n\n\n# This is not empty\nSys.getenv(\"R_LIBS_USER\")\n\n[1] \"~/Library/R/x86_64/4.1/library\"\n\n\n\n# This returns TRUE\nnzchar(Sys.getenv(\"R_LIBS_USER\"))\n\n[1] TRUE\n\n\n\n\n\n# This is empty\nSys.getenv(\"test\")\n\n[1] \"\"\n\n\n\n# This returns FALSE\nnzchar(Sys.getenv(\"test\"))\n\n[1] FALSE\n\n\n\n\n\n\n\n\nTIL: nzchar(). Super useful when working with environment variables in R.also, #asciicast is amazing! install the GIF converter with remotes::install_github('r-lib/asciicast', ref = remotes::github_pull(24)) #rstats h/t @GaborCsardi pic.twitter.com/pCZQLCNaDl\n\n‚Äî Isabella Vel√°squez (@ivelasq3) May 11, 2022"
  },
  {
    "objectID": "til/gitkeep/Untitled.html",
    "href": "til/gitkeep/Untitled.html",
    "title": "Add .gitkeep to a bunch of folders",
    "section": "",
    "text": "I needed to add a bunch of folders to a GitHub repository. I knew their names but didn‚Äôt have any documents for them yet.\nGitHub only allows you to push folders that contain something. However, you can use .gitkeep files to make ‚Äúempty‚Äù folders.\nFirst, I created all the folders that I needed.\nletters <-\n  c(\"a\", \"b\", \"c\")\n\nfoldernames <-\n  paste0(\"2022/\", letters)\n  \nlapply(foldernames, dir.create, recursive = TRUE)\nNext, I navigated to the new ‚Äú2022‚Äù folder and created the .gitkeep files.\ncd 2022\n\nfind . -type d -empty -not -path \"./.git/*\" -exec touch {}/.gitkeep"
  },
  {
    "objectID": "til/filter-if-any/index.html",
    "href": "til/filter-if-any/index.html",
    "title": "Filter on conditions for more than one variable at the time",
    "section": "",
    "text": "TIL I learned that you can filter on conditions for more than one variable at a time using if_any() or if_all().\n\n\n\nJust to add to the confusion (üòÖ) I think you still do use it for mutate but not for filter?‚ö†Ô∏è Using across() in filter() is deprecated, use if_any() or if_all().\n\n‚Äî Lucy D‚ÄôAgostino McGowan (@LucyStats) March 21, 2022\n\n\n\nTurns out that across() is only for selecting functions (like summarize() and mutate()). This was announced in dplyr 1.0.4.\nYou use if_any() vs.¬†if_all() depending if you need to match some vs.¬†all columns.\nif_any():\n\nmtcars %>%\n  as_tibble() %>%\n  mutate(across(everything(), as.integer)) %>%\n  filter(if_any(contains(\"m\"), ~ . == 0))\n\n# A tibble: 19 √ó 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1    21     6   258   110     3     3    19     1     0     3     1\n 2    18     8   360   175     3     3    17     0     0     3     2\n 3    18     6   225   105     2     3    20     1     0     3     1\n 4    14     8   360   245     3     3    15     0     0     3     4\n 5    24     4   146    62     3     3    20     1     0     4     2\n 6    22     4   140    95     3     3    22     1     0     4     2\n 7    19     6   167   123     3     3    18     1     0     4     4\n 8    17     6   167   123     3     3    18     1     0     4     4\n 9    16     8   275   180     3     4    17     0     0     3     3\n10    17     8   275   180     3     3    17     0     0     3     3\n11    15     8   275   180     3     3    18     0     0     3     3\n12    10     8   472   205     2     5    17     0     0     3     4\n13    10     8   460   215     3     5    17     0     0     3     4\n14    14     8   440   230     3     5    17     0     0     3     4\n15    21     4   120    97     3     2    20     1     0     3     1\n16    15     8   318   150     2     3    16     0     0     3     2\n17    15     8   304   150     3     3    17     0     0     3     2\n18    13     8   350   245     3     3    15     0     0     3     4\n19    19     8   400   175     3     3    17     0     0     3     2\n\n\nif_all():\n\nlarge <- function(x) {\n  x > mean(x, na.rm = TRUE)\n}\n\nmtcars %>%\n  as_tibble() %>%\n  mutate(across(everything(), as.integer)) %>%\n  filter(if_all(contains(\"m\"), large))\n\n# A tibble: 10 √ó 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1    21     6   160   110     3     2    16     0     1     4     4\n 2    21     6   160   110     3     2    17     0     1     4     4\n 3    22     4   108    93     3     2    18     1     1     4     1\n 4    32     4    78    66     4     2    19     1     1     4     1\n 5    30     4    75    52     4     1    18     1     1     4     2\n 6    33     4    71    65     4     1    19     1     1     4     1\n 7    27     4    79    66     4     1    18     1     1     4     1\n 8    26     4   120    91     4     2    16     0     1     5     2\n 9    30     4    95   113     3     1    16     1     1     5     2\n10    21     4   121   109     4     2    18     1     1     4     2\n\n\nAny tidyselect usage is allowable inside if_*() just like inside across(), so they work very similarly.\nThanks to @gvelasq for his explanation."
  },
  {
    "objectID": "til/asciicast/index.html",
    "href": "til/asciicast/index.html",
    "title": "Create a GIF of code and its output",
    "section": "",
    "text": "We can create a GIF of code and its output using asciicast.\nInstall the development version:\n\nremotes::install_github('r-lib/asciicast', ref = remotes::github_pull(24)) \n\nCreate a file called nzchar.R with specifications for the GIF and the code to run.\n\n#' Title: Using nzchar\n#' Columns: 60\n#' Rows: 18\n\n# This is not empty\nSys.getenv(\"R_LIBS_USER\")\n\n# This is empty\nSys.getenv(\"test\")\n\n# This returns TRUE\nnzchar(Sys.getenv(\"R_LIBS_USER\"))\n\n# This returns FALSE\nnzchar(Sys.getenv(\"test\"))\n\nCreate another file that creates the GIF:\n\n#' Title: Using nzchar\n\nsrc <- \"nzchar.R\"\ncast <- asciicast::record(src)\n\n# <<\n# `cast` is an `asciicast` object, which has some metadata and the\n# recording itself:\n# <<\n\ncast\n\n# <<\n# You can write `cast` to a GIF file with the version installed above.\n# <<\n\nsvg <- tempfile(fileext = \"gif\")\nasciicast::write_gif(cast, svg, theme = \"monokai\")\n\nReally fun for adding to tweets!"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "¬© Copyright Isabella Vel√°squez\nThis is my personal website. Nothing here is endorsed by my employer or any organizations of which I am a part. Content on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate my authorship and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "An R package for Data Science in Education Using R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R package for U.S. school district shapefiles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R package for tidying API responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R wrapper for WizeHive‚Äôs Zengine API.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project.html#books",
    "href": "project.html#books",
    "title": "Projects",
    "section": "Books",
    "text": "Books\n\n\n\n\n\n\n\n\n\n\nData Science in Education Using R\n\n\nData Science in Education Using R is the go-to reference for learning data science in the education field.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk/git-through-april/index.html",
    "href": "talk/git-through-april/index.html",
    "title": "Git Through April",
    "section": "",
    "text": "Abstract\nA walkthrough on using Git in the RStudio GUI.\n\n\nDate\nApril 23, 2021\n\n\nTime\n12:00PM PT\n\n\nLocation\nRemote\n\n\nEvent\nR-Ladies Seattle Meetup\n\n\nRepository\nGitHub"
  },
  {
    "objectID": "talk/blogdown-for-what/index.html",
    "href": "talk/blogdown-for-what/index.html",
    "title": "Blogdown for What",
    "section": "",
    "text": "Abstract\nA walkthrough on creating a website using the {blogdown} package.\n\n\nDate\nAugust 23, 2018\n\n\nTime\n6:30PM PT\n\n\nLocation\nSeattle, WA\n\n\nEvent\nR-Ladies Seattle Meetup\n\n\n\n\n\n\n\nHello everybody. My name is Isabella Velasquez. I am very excited to talk to you today about the package blogdown. I‚Äôve decided to call this presentation: BLOGDOWN FOR WHAT?\nA little about me: I‚Äôm Isabella. I‚Äôm a data analyst at the Bill & Melinda Gates Foundation along with R-Ladies Seattle organizer, Chaya. I started learning R in 2015 when attending graduate school at the University of Chicago. We were pretty much thrust into the R world and had to sink or swim, which is how I approach this talk. I try to be as straightforward as possible in my work and hopefully that is reflected in this presentation. This is meant to be a simple introduction on blogdown, why you should use it, and how to get set up.\nLet‚Äôs start at the beginning. Why even have a website at all? It‚Äôs something that I think about a lot because sharing my thoughts and work is not something that comes naturally to me. Although I am not a prolific Twitter user or blogger, I do have accounts and try to keep up with the R world as best I can. Like I mentioned, I started learning R in graduate school. It‚Äôs been three years and I have vague recollections of my R journey. But with a website, I would have been able to clearly see how my analysis of data has evolved and improved. I could see how I moved from the data cleaning steps, summary statistics, visualization of different projects instead of just speculating when I go through all my various files. I also have all my projects in multiple places right now ‚Äì on my laptop, on my work laptop, on my GitHub. If I wanted to find what I‚Äôve done and share it with others, it would be quite a lift. A website can be the holder of all that information. And finally, there is nothing more valuable than sharing what you‚Äôve done with others to get feedback. You get to see any gaps you missed, new ideas you may not have thought of, or how to continue your work and do more interesting, fun things. The R community is so vibrant and collaborative that you get to explore that with many many people around the world.\nEnter blogdown. Blogdown is an R package that makes websites using R Markdown, which is a file format for documents, and Hugo, which is a static website generator. You can create the website entirely in R Studio. If you use GitHub, then you can upload your work to a repository and have version control readily available. And if you use GitHub, you can link your blogdown website with Netlify, which will easily publish your website.\nSo, blogdown for what? There are many options out there for creating websites. And that‚Äôs exactly the issue: there are many options out there for creating websites. There are many factors to consider, like whether your images will load quickly and how easy it is to create a new post. Blogdown considers all these factors and creates a fast, safe, easy to use website.\nBecause we‚Äôre talking about using R, which presumably means there will be data analysis involved. R Markdown is a format that easily details data analysis. It has code chunks so that you can see the actual code that you write and then it also displays the output, whether it be a table or image. But then it‚Äôs also easy to write in your commentary in regular text. Therefore, if you are using your website to show off your analysis and R skills, then R Markdown is a great format to do that in. However, the other very commonly used website creator Jekyll does not display R Markdown files very easily and unfortunately R Markdown isn‚Äôt great for creating websites on its own. Blogdown puts all the necessary steps in place to create a website that easily displays R Markdown pages.\nThe other great thing is that because blogdown also embeds Hugo, there are many themes available that are visually appealing even without customization. And if you do know how to customize websites, then the sky is the limit.\nSo how do you actually make a website with blogdown? The creator of the package, Yihui Xie, and his coauthors Amber Thomas and Alison Presmanes Hill luckily wrote a guide that walks through every step in detail. When creating my website, I just went through each page and followed the instructions. It‚Äôs that easy! But like I mentioned earlier, I want to give you the most straightforward introduction that I can.\nBefore we begin, make sure you already have RStudio and GitHub. Chaya recently gave a talk on GitHub and walked through creating a repository and linking to a project in R Studio. You can create a blogdown website without linking to GitHub, but in addition to all the benefits that GitHub provides, like version control, it will also allow you to host on Netlify easily later on.\nOnce you have a project in R Studio, you want to have blogdown installed!\nAnd because the creators of blogdown have done such a good job making their package user friendly, once you have blogdown installed you actually don‚Äôt have to create a project with code although you can do whatever works for you. I just go into New Project then New Directory then create a Website using Blogdown. I like this add-in because if you use the same directory name as the project you cloned from R Studio, then all the blogdown folders will be created in that directory and then you can commit and push up to GitHub. This add-in also lets you install your theme if you don‚Äôt want the default one.\nOnce you have your new website project, you must run serve site in order to preview it within R Studio. Then you can start customizing within the file config.toml which is created when you create a site. Here, you can put things like your name, your Twitter handle. And once you are ready to create a new post, type new post in the console and run. It will populate an R Markdown file that you can fill in to your liking. And once all that is ready, you can commit and push up to GitHub!\nLike I mentioned before, if you want an actual URL that people can go to, you can use Netlify for hosting. If you have pushed everything up to GitHub, you can connect netlify with your website repository and your webiste will update anytime you push up to GitHub. Netlify gives out a weird URL, which you can change but if you want to do even more things R, you can apply for an rbind.io URL. They have a request template where you list out your Netlify account and GitHub repo and desired URL and they got back to me in a day.\nIn terms of troubleshooting, I ran into an issue where my website wasn‚Äôt loading the way that it did in the R Studio preview window. I ran a quick Google search saying just that and saw that there was already a thread about it. I read through it, followed the instructions, and was able to fix it.\nI mention that because there are so many resources for blogdown even in the few months it has been out. Beyond the official guide, Mara Averick works hard to compile resources and tutorials. StackOverflow has a blogdown tag. And many things are just a search away.\nThanks all for listening. I hope this presentation showed you why you should have a site, in particular a blogdown one, and how to create one. Hopefully I‚Äôve also shown you that there‚Äôs a lot of help out there and they continue to make blogdown more user friendly all the time. And finally here is my blogdown site‚Ä¶with all the neat Hugo functionalities!"
  },
  {
    "objectID": "talk/building-a-blog-with-r/index.html",
    "href": "talk/building-a-blog-with-r/index.html",
    "title": "Building a Blog With R",
    "section": "",
    "text": "Abstract\nA blog is a wonderful opportunity to record your data stories, gain exposure for your expertise, and support others in their R journey. In this talk, I discuss possible reasons for creating a blog and explore tools that make it easy to showcase your R skills.\n\n\nDate\nJanuary 25, 2022\n\n\nTime\n11:00 AM CT\n\n\nLocation\nRemote\n\n\nEvent\nRStudio Enterprise Community Meetup\n\n\nRepository\nGitHub\n\n\nSlides\nPosit Connect\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/whats-new-in-the-tidyverse/index.html",
    "href": "talk/whats-new-in-the-tidyverse/index.html",
    "title": "What‚Äôs new in the tidyverse",
    "section": "",
    "text": "Abstract\nIf you‚Äôre an R user, chances are you‚Äôve heard of the tidyverse - a suite of packages that‚Äôs revolutionized data wrangling, visualization, and analysis. And if you‚Äôve been following the tidyverse blog, you‚Äôll know there‚Äôs been a veritable flood of new functions and feature releases lately.\nIn this talk, we‚Äôll give you the inside scoop on all the latest and greatest updates to the tidyverse. From a new family of string-separating functions to a bunch of new joins, we‚Äôll show you how this cutting-edge universe of packages can make your life as a data scientist easier, more efficient, and more intuitive. So buckle up and get ready to explore what‚Äôs new in the tidyverse!\n\n\n\nDate\nMarch 22, 2023\n\n\n\nTime\n10:00 AM PT\n\n\n\nLocation\nRemote\n\n\n\nEvent\nR-Ladies Rome\n\n\n\nRepository\nGitHub\n\n\n\nWebsite\nSite\n\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/building-a-blog-with-quarto/index.html",
    "href": "talk/building-a-blog-with-quarto/index.html",
    "title": "Building a Blog With Quarto",
    "section": "",
    "text": "Abstract\nA blog is a wonderful opportunity to record your data stories, gain exposure for your expertise, and support others in their data science journey. In this talk, I discuss possible reasons for creating a blog and explore tools that make it easy to showcase your R, Python, Julia, and Observable skills.\n\n\nDate\nAugust 30, 2022\n\n\nTime\n09:00 AM PT\n\n\nLocation\nRemote\n\n\nEvent\nRStudio Enterprise Community Meetup\n\n\nRepository\nGitHub\n\n\nWebsite\nSite\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/hiking-data/index.html",
    "href": "talk/hiking-data/index.html",
    "title": "Taking a Peek at Hiking Data Using R",
    "section": "",
    "text": "Abstract\nAn analysis of hiking routes and metrics.\n\n\nDate\nJuly 16, 2019\n\n\nTime\n6:30PM\n\n\nLocation\nSeattle, WA\n\n\nEvent\nR-Ladies Seattle Meetup\n\n\nBlog Post\nTaking A Peek into My Hiking Data"
  },
  {
    "objectID": "talk/package-mangement-in-r/index.html",
    "href": "talk/package-mangement-in-r/index.html",
    "title": "Package Management for Reproducible Data Science in R",
    "section": "",
    "text": "Abstract\nReproducible data science encompasses a broad range of practices including, but not limited to, version control and literate programming. One practice that is often overlooked is proper management of packages. Code that works with software or package version you have installed may break when run with different software or package version. In this webinar, we will discuss R package management solutions for two scenarios. Isabella will show how to maintain installed packages when upgrading to a new version of R, using the recent release of R 4.0 as an example. Mike will demonstrate how the renv package can be used to maintain packages within individual data science projects.\n\n\nDate\nMay 07, 2020\n\n\nTime\n12:00PM PT\n\n\nLocation\nRemote\n\n\nEvent\nSeattle useR Group\n\n\nRepository\nGitHub\n\n\n\n\n\n\n\n\n\n\n\nUpgrading R in the time of coronavirus\nHi everybody! My name is Isabella Velasquez, I am a data analyst at the Bill & Melinda Gates Foundation.\nI am really excited to talk to you today about updating R in the time of coronavirus!\nR 4.0.0, with the release name Arbor Day, was released in late April. It moved R from a 3.x version to 4.0, which is called a major upgrade. This is pretty exciting - as with all major upgrades of R, there‚Äôs a lot of new features in 4.0. There‚Äôs a function that will convert your lists to a data frame, sort your non-atomic objects, there are new color palettes, etc. ‚Ä¶ And probably what most were excited about was that the default for stringsAsFactors is now FALSE, which is great for those of us who suffered a lot when the default was TRUE, and it‚Äôs probably the most talked about detail about this upgrade.\nI invite you to think now about this new version of R, and ask: If I were to request that you upgrade R from your current version to 4.0 right now, how would you feel? You can write how worried you‚Äôd be and something you may be worried about in the chat now.\nFor some, upgrading R immediately sounds great. It means you have the newest stuff! For others, there is a lot of hesitation. But regardless of how you feel, when you upgrade R, you will have to reinstall your packages.\nOn most single-user systems (Mac, Windows, and Linux), when you upgrade to a new major version of R (like 3.3.0 to 4.0.0), R will no longer find your previously installed packages because now there is a new package library. When this happens, you may see that when R code calls the library() function, it throws an error and reports that the package is missing.\nThere are many options to deal with reinstalling packages in a new version of R and we will walk through some of them. I want to emphasize that I‚Äôll be focusing on single-user systems like Mac, Windows, and Linux - I know there are other ways that people use R, but that will be out of scope for my talk.\nThe first option we‚Äôll talk about is upgrading R, which will create a new package library for 4.0.0, and then installing packages as you need them for your code. Some people prefer this option because it feels like you have a clean start, since your package library will be completely clear. You may know how refreshing this is if you‚Äôve downloaded a bunch of packages to try out and then never ended up using them. If you use the RStudio IDE, it will let you know if you have a package that is missing so that you can install it before you run your script. The downside of this option is that you won‚Äôt be up and ready to go with your scripts because the packages will first need to be installed, which can be a hassle. So, this leads into another option, where you can‚Ä¶.\nUpgrade R, and then reinstall all your packages at once. This will enable you to get up and going with your scripts shortly after upgrading R. There are many ways of doing this. I‚Äôll walk through an example script that I posted on Twitter that was created by my brother. This is something you‚Äôd run after you‚Äôve upgraded to R 4.0.0.\nFirst, remember that all your packages need to be reinstalled because there is a new package library. So, you reinstall tidyverse and fs and then call them. Then, you find which versions of R you have installed on your machine. The directories on Windows, Linux, and Mac slightly vary, that is why you have different options depending on your operating system.\nThen, the script determines which of your versions is the newest, current version of R and which was the second newest version of R and calls them new_r and old_r. Then, it finds the library paths, or .libPaths(), for new_r and old_r and extracts the list of packages that are available in old_r.\nThen, it creates a function called install_all that installs packages. Finally, it installs all the packages in the list of packages, purring all the way\nAgain, that is just one of the many options available to you. Here is another script that garnered a lot of attention on Twitter. There are also functions in formal packages that will reinstall your packages for you.\nThere are a few things to note. You have to make sure that whatever script or function works for you - for example, {installr} only works on Windows. If you use Homebrew, the default Homebrew behavior is to delete the old R .libPaths() folder when upgrading, so then there won‚Äôt be a 3.x folder from which to build a list of pkgs to install in 4.0.\nAlso, some of these options will not redownload your Bioconductor or GitHub dev packages. You may have to find other ways of getting these packages in R 4.0.0.\nFinally, when you reinstall packages, they will download the latest version of the package. If your scripts rely on an older version of the package, your code may no longer run. So, if having old packages is important to you, you can‚Ä¶\nUpgrade R but use your old packages. So full disclosure, I have not done this before. As far as I can tell, it‚Äôs not very encouraged. The idea behind this option is to upgrade R, keep the old library folders, change .libPaths() to the old R version library, and try to call the library() function from there. Another way is to try to copy the folders from the old package library to the new library and that is not a good plan. If the packages were built on 3.x, then there‚Äôs no guarantee they‚Äôd work in 4.x. This is because R and packages have system dependencies, and some of the packages may break if installed on R 4.0.0 and maintainers haven‚Äôt upgraded their code yet.\nSo, if you‚Äôve ever tried to download RJava even in the best of times, you may know what dependency hell is - so try to avoid that by not creating too much chaos with your packages.\nSo, we‚Äôve talked about upgrading R and installing your packages one at a time, upgrading R and installing all your packages at once, and upgrading R and keeping your old packages. But, if you want to avoid all of this, you can always‚Ä¶\nNot upgrade R! This is very common, especially as people wait for kinks to be sorted out. For example, there are a lot of issues right now with R 4.0.0 users not being able to upload Shiny apps to shinyapps.io. If your organization is dependent on this app, it might be better to wait until a more stable time to try out the new major version.\nStill, even if you‚Äôre not ready to jump into R 4.0.0 yet, you can do a few things. Users have provided ways to test out your packages and scripts before you fully upgrade, and there are more formal ways of switching between R versions available as well.\nSo, how do you decide what to do? It very much depends on you! For example, if you worry about being able to refind packages that aren‚Äôt on CRAN, you may consider porting it over to your new R version somehow. If you live in a place with very slow internet, it might make sense to not have to reinstall all your packages all at once. If your code is very organized, maybe you won‚Äôt have issues knowing which packages to install or not install. Maybe you want to be able to hit the ground running with all the packages you‚Äôve used in the past, so may as well install them all now.\nI want to take a quick poll - how many of you have already upgraded to 4.0.0?\nFun fact: as we were developing this talk, we ran into an issue where one of us (Mike) had upgraded to R 4.0.0, and another one of us (me) had not, and so I wasn‚Äôt able to knit our slides. So I provide this as an example of how knowing your team and workflow is crucial to decide what to do! When you weigh these different considerations, you can figure out the path forward for R and your packages. And, I‚Äôve now updated R, and am installing my packages one by one.\nSo, all of these different options assume that you are not using virtual environments, which create isolated environments for projects and provide another option for package management. To discuss this, I will now turn it over to Mike!"
  },
  {
    "objectID": "talk/rladies-allyship/index.html",
    "href": "talk/rladies-allyship/index.html",
    "title": "What do penguins have to do with being anti-racist?",
    "section": "",
    "text": "Abstract\nA talk on why being anti-racist matters in data science.\n\n\nDate\nAugust 05, 2020\n\n\nTime\n12:00PM PT\n\n\nLocation\nRemote\n\n\nEvent\nR-Ladies Seattle Meetup"
  },
  {
    "objectID": "talk/intro-to-quarto/index.html",
    "href": "talk/intro-to-quarto/index.html",
    "title": "Intro to Quarto",
    "section": "",
    "text": "Date\nOctober 28, 2022\n\n\nTime\n04:00PM PT\n\n\nLocation\nRemote\n\n\nEvent\nR-Ladies St.¬†Louis Meetup\n\n\nRepository\nGitHub\n\n\nSlides\nQuartoPub\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/longitudinal-analysis/index.html",
    "href": "talk/longitudinal-analysis/index.html",
    "title": "Longitudinal Analysis with Federal Students with Disabilities Data",
    "section": "",
    "text": "Abstract\nA Data Science in Education Using R bookclub session on longitudinal data analysis.\n\n\nDate\nMarch 31, 2021\n\n\nTime\n4:00PM CT\n\n\nLocation\nRemote\n\n\nEvent\nR4DS\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/gt-talk/index.html",
    "href": "talk/gt-talk/index.html",
    "title": "Around the World in a Single Table (gt Walkthrough)",
    "section": "",
    "text": "Abstract\nTables are a format for communicating information. In this talk, I walk through creating a table using the {gt} package.\n\n\nDate\nOctober 27, 2021\n\n\nTime\n11:00AM CT\n\n\nLocation\nRemote\n\n\nEvent\nR-Ladies Johannesburg Meetup\n\n\nRepository\nGitHub\n\n\nSlides\nGitHub\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/using-r-and-other-tools/index.html",
    "href": "talk/using-r-and-other-tools/index.html",
    "title": "Packages for Using R With Python, Tableau, and Other Tools",
    "section": "",
    "text": "Abstract\nR is a powerful programming language for statistics and data science with an accompanying ecosystem that supports its use. Interoperability is part of what makes the R ecosystem so rich, enabling users to apply R in a variety of contexts. In this talk, I will discuss packages that allow you to integrate R together with other tools, including Python and Tableau.\n\n\nDate\nDecember 10, 2021\n\n\nTime\n10:00AM CT\n\n\nLocation\nRemote\n\n\nEvent\nWhy R? 2021 Conference\n\n\nRepository\nGitHub\n\n\nSlides\nGoogle Slides\n\n\nRecording\nYouTube"
  },
  {
    "objectID": "talk/excel-data-viz-tips-in-r/index.html",
    "href": "talk/excel-data-viz-tips-in-r/index.html",
    "title": "Seven Essential Excel Data Visualization Tips (in R!)",
    "section": "",
    "text": "Abstract\nHave you ever seen a nifty Excel data visualization and wondered, ‚ÄúCan I do that with R?‚Äù The answer is likely yes! With R, you can create stunning, customizable visuals that elevate your data analysis. In this talk, we will cover seven data visualization tips that are achievable in Excel, but also demonstrate how (and why) to create them in R. From sparklines to heat maps, if Excel can do it, R can do it, too! Best of all, your work will be reproducible and repeatable. This means that you can share your code with others and remake your charts anytime, making your work more transparent and streamlined. Join us to learn the tools for making impactful and reproducible dataviz in R!\n\n\n\n\n\n\nDate\nJune 07, 2023\n\n\n\n\nTime\n12:00 PM MT\n\n\n\n\nLocation\nRemote\n\n\n\n\nEvent\nSalt Lake City R Users Group\n\n\n\n\nRepository\nGitHub\n\n\n\n\nSlides\nQuarto Pub\n\n\n\n\nRecording\nYouTube"
  }
]